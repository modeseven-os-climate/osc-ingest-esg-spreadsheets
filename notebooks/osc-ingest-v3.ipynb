{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0ebfc5-d93a-4731-a1a5-319b73accd2a",
   "metadata": {},
   "source": [
    "## Load 2020 WIDE-formatted ESG data (Generic)\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Initially developed using the Royal Dutch Shell plc Sustainability Report 2020 report (Many Sheets)\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691f183-a756-45e1-9348-1156044dee52",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74dff22-a393-47e4-bcaa-3d6c992ab083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee67533-80c2-4594-8e78-c864adddf1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CO2e\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from itertools import islice\n",
    "\n",
    "import pint\n",
    "import pint_pandas\n",
    "import iam_units\n",
    "from openscm_units import unit_registry\n",
    "pint_pandas.PintType.ureg = unit_registry\n",
    "ureg = unit_registry\n",
    "ureg.define('fraction = [] = frac')\n",
    "ureg.define('percent = 1e-2 frac = pct = percentage')\n",
    "ureg.define('ppm = 1e-6 fraction')\n",
    "\n",
    "ureg.define(\"USD = [currency]\")\n",
    "ureg.define(\"EUR = nan USD\")\n",
    "ureg.define(\"JPY = nan USD\")\n",
    "ureg.define(\"MM_USD = 1000000 USD\")\n",
    "ureg.define(\"revenue = USD\")\n",
    "\n",
    "ureg.define(\"btu = Btu\")\n",
    "ureg.define(\"tBtu = T Btu\")\n",
    "ureg.define(\"boe = 5.712 GJ\")\n",
    "ureg.define(\"UEDCTM = [shell_index]\")\n",
    "\n",
    "ureg.define(\"CO2e = CO2 = CO2eq = CO2_eq\")\n",
    "ureg.define(\"HFC = [ HFC_emissions ]\")\n",
    "ureg.define(\"PFC = [ PFC_emissions ]\")\n",
    "\n",
    "ureg.define(\"production = [ output ]\")\n",
    "ureg.define(\"Index = pct\")\n",
    "\n",
    "one_co2 = ureg(\"CO2e\")\n",
    "print(one_co2)\n",
    "\n",
    "from osc_ingest_trino import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893d0110-c719-4ae8-9fe0-248a3a4256e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.0 CO2e metric_ton/revenue"
      ],
      "text/latex": [
       "$1.0\\ \\frac{\\mathrm{CO2e} \\cdot \\mathrm{metric\\_ton}}{\\mathrm{revenue}}$"
      ],
      "text/plain": [
       "1.0 <Unit('CO2e * metric_ton / revenue')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ureg(\"tonnes CO2e/revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59df89e-d31d-4f2e-a5f2-e0d696f9e7c3",
   "metadata": {},
   "source": [
    "For spreadsheets in WIDE format, pre-process the spreadsheet as a workbook, cascading label data into 3rd-normal form row and column metadata\n",
    "\n",
    "* var_col is the label of the variable being measured (whose specificity (like CO2, CH4, NOx, etc) often affects units)\n",
    "* units_col is the column where units are stated\n",
    "* val_col:last_val_col are the column where the values are quantitatively reported\n",
    "* last_val_col+1:last_col are additional columns that are presumed to be metadata labels (such as GRI or SASB labels)\n",
    "\n",
    "We add:\n",
    "* notes_col (source worksheet-specific; could act as a kind of source table metadata)\n",
    "* topic_col (sheet-level category; if we wanted large tables, they could be named by topic)\n",
    "* category_col (to which row-level data rolls up; if we wanted small tables, they could be named by topic:category)\n",
    "* segment_col (the dimension by which row-level data is segmented)\n",
    "* units_col (if not already existing in input)\n",
    "\n",
    "Some spreadsheets use color to express a multi-level category hierarchy (such as Energy Consumption>>Business Use>>Fuel Type).  We concatenate the categories from left to right as the category for our purposes, except we split off the rightmost subcategory as the segmentation.\n",
    "\n",
    "Based on all of the above, we don't really have table-level metadata other than notes attached to sheets and generic column information.  An argument could be made that we need to allocate specifier columns for additional data we want to split out from our variables.  That could look like:\n",
    "\n",
    "* spec1_col\n",
    "* spec2_col\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a09246-078b-4ede-b891-e79fe89c8036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# var_col = 1\n",
    "\n",
    "# Magic knowledge\n",
    "# last_col = 4\n",
    "# max_hidden_col = 5\n",
    "# year_regex = r'^(20\\d\\d) Data$'\n",
    "\n",
    "ingest_columns = [ 'Variable', 'Notes', 'Topic', 'Category', 'Segmentation', 'Unit' ]\n",
    "ingest_col_offsets = dict((j,i) for i,j in enumerate(ingest_columns[1:], start=1))\n",
    "\n",
    "# In this case, Value columns are named like 2020, 2019, 2018, ... .  It is the pd.melt function that gives us an actual Value column.\n",
    "# the val_col index merely refers to the first such value row (which hopefully has tasty data)\n",
    "\n",
    "# Magic knowledge\n",
    "# val_col = units_col+1     # units_col starts as var_col+1, val_col starts as var_col+2 which is also units_col+1\n",
    "\n",
    "# If topic_row is None, set topic based on name of sheet\n",
    "# topic = topic_row = None\n",
    "# header_row = None\n",
    "\n",
    "# If init_header_row is None, find header row based on color scheme\n",
    "# init_header_row = 1\n",
    "\n",
    "class corp_report_magic:\n",
    "    def __init__(self, shortname, input_filename, ws_start, ws_end, var_col=None, units_col=None, \n",
    "                 notes_col=None, topic_row=None, topic_col=None, category_col=None, init_header_row=None, header_row_list=None,\n",
    "                 header_color=None, cat_color_dict={ None:0 }, year_regex=None, max_hidden_col=None,\n",
    "                 val_col=None, last_val_col=None):\n",
    "        self.shortname = shortname\n",
    "        self.input_filename = '/'.join([os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src')),\n",
    "                                        'osc-ingest-shell/data/external', input_filename])\n",
    "        self.ws_start = ws_start,\n",
    "        self.ws_end = ws_end,\n",
    "        self.init_topic_row = topic_row    # If topic_row is None, use the worksheet name as the topic\n",
    "        self.var_col = var_col or 1\n",
    "        self.init_units_col = units_col            # If units_col is none, we have to allocate it\n",
    "        self.init_topic_col = topic_col    # If topic_col is non-null, we get topics from this row\n",
    "        self.init_category_col = category_col\n",
    "        self.init_notes_col = notes_col\n",
    "        self.init_val_col = val_col\n",
    "        self.init_last_val_col = last_val_col\n",
    "        # val_col, last_val_col, and last_col can be derived from the spreadsheet\n",
    "        self.units_row = -1 if units_col==None else 0   # -1: Carry across only; 0: no units seen yet; > 0 row of prevailing unit\n",
    "        self.init_header_row = init_header_row\n",
    "        self.header_row_list = header_row_list if header_row_list else ([-1] * ws_start) + ([init_header_row] * (ws_end-ws_start+1))\n",
    "        self.header_row = None\n",
    "        self.header_color = header_color\n",
    "        self.cat_color_dict = cat_color_dict\n",
    "        self.year_regex = year_regex\n",
    "        # For AEP, there are several hidden columns on the first sheet we must delete\n",
    "        # to make that sheet line up with other sheets\n",
    "        self.max_hidden_col = max_hidden_col\n",
    "        \n",
    "        self.units_col = units_col\n",
    "        self.topic_col = topic_col\n",
    "        self.category_col = category_col\n",
    "        self.notes_col = notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = val_col or units_col+1 if units_col else var_col+1 if var_col else 2\n",
    "        self.last_val_col = last_val_col\n",
    "        self.last_val_row = None              # Set by preprocess (after we've identified our value columns)\n",
    "        self.last_col = None                  # Set by crop_sheet\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.topic_row = self.init_topic_row\n",
    "        self.units_col = self.init_units_col\n",
    "        self.topic_col = self.init_topic_col\n",
    "        self.category_col = self.init_category_col\n",
    "        self.notes_col = self.init_notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = self.init_val_col or self.init_units_col+1 if self.init_units_col else self.var_col+1\n",
    "        self.last_val_col = self.init_last_val_col\n",
    "\n",
    "Shell_magic = corp_report_magic(\"Shell\", r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\", 1, 10,\n",
    "                                init_header_row=5, units_col=2)\n",
    "DPDHL_magic = corp_report_magic(\"DPDHL\", r\"DPDHL-ESG-Statbook-2020-en.xlsx\", 2, 4,\n",
    "                                topic_row=1, header_row_list=[ -1, -1, 8, 5, 4], header_color='FFBF00',\n",
    "                                cat_color_dict={ 'FF00B050':0, 'E2F0D9':1, 'D0CECE':0, 'E7E6E6':0 },\n",
    "                                units_col=2)\n",
    "Unilever_magic = corp_report_magic(\"Unilever\", r\"Unilever sustainability performance data_Climate FINAL.xlsx\", 0, 0,\n",
    "                                   topic_row=9, init_header_row=10,\n",
    "                                   cat_color_dict={'FFEBF1DE':0, 'E2F0D9':1})\n",
    "AEP_magic = corp_report_magic(\"AEP\", r\"2021-Data-Centerv1.xlsx\", 0, 3,\n",
    "                              init_header_row=1,\n",
    "                              cat_color_dict={'FF237F2E':0, 'FF40B14B':1, 'FFC6E7C8':2,\n",
    "                                              'FF757575':0, 'FFBDBDBD':1, \n",
    "                                              'FF5FB3F9':0, 'FFB9DDFC':1, \n",
    "                                              'FFD0AF8F':0, 'FFEEDCCA':1},\n",
    "                              year_regex=r'^(20\\d\\d) Data$', max_hidden_col=5)\n",
    "Altria_magic = corp_report_magic(\"Altra\", r\"esg-tables.xlsx\", 1, 1,\n",
    "                                 init_header_row=2,\n",
    "                                 cat_color_dict={'FF9BDA44':0, 'FF92D050':1},\n",
    "                                 units_col=2)\n",
    "\n",
    "SUEZ_magic = corp_report_magic(\"SUEZ\", r\"SUEZ-FY-2020-ESG-dataset-xls-may2020.xlsx\", 1, 1,\n",
    "                               init_header_row=3,\n",
    "                               topic_col=1, category_col=2, var_col=3, units_col=4,\n",
    "                               val_col=9, last_val_col=10)\n",
    "\n",
    "filename_magic = {\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "}\n",
    "# A storage area in case we delete items from the above.\n",
    "foo = {\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "}\n",
    "\n",
    "crm = None\n",
    "value_vars = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da79214-e1e1-4fef-9222-fde04ec5e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_regex = re.compile(r'^((mi|bi|tri|quadri)llion|thousand|hundred)(s of)? ', re.I)\n",
    "sc_xlate = {'hun':1e2, 'tho':1e3, 'mil':1e6, 'bil':1e9, 'tri':1e12, 'qua':1e15}\n",
    "\n",
    "def find_units(var):\n",
    "    scale = 1.0\n",
    "    if var in ['%', 'pct', 'percent']:\n",
    "        return 'percent'\n",
    "    if '-based' in var:\n",
    "        return None\n",
    "    var = var.replace ('trillion (10^12)', 'trillion')\n",
    "    if var in ureg:\n",
    "        return f'{ureg(var).u:~}'\n",
    "    m = re.search(scale_regex, var)\n",
    "    if m:\n",
    "        var = ' '.join([var[0:m.start(0)],var[m.end(1)+1:]]).strip()\n",
    "        if var in ureg:\n",
    "            units = sc_xlate[m.group(1)[0:3]] * ureg(var)\n",
    "            units = units.to_compact()\n",
    "            if units.m - 1.0 < 0.00001:\n",
    "                # Address roundoff problems such as giga = 1.00000000000002 x 10^9\n",
    "                return f'{units.u:~}'\n",
    "            error(f'units do not reduce: {units}')\n",
    "    print(f'find units: nothing found for {var}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a4127b-27de-4eaf-b776-edec0e224eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_keywords = { 'footprint':['intensity'],\n",
    "                   'emissions':['scope 1', 'scope 2', 'scope 3', 'ghg'],\n",
    "                   'energy':['consum', 'generat', 'renewable'],\n",
    "                   'water':['consum', 'discharge', 'withdraw'],\n",
    "                   'waste':['landfill', 'incinerate', 'compost', 'recycle', 'reuse'],\n",
    "                   'other':[]}\n",
    "\n",
    "topic_cell = None\n",
    "category_cell = None\n",
    "segmentation_stack = []\n",
    "\n",
    "def process_topic(ws, row):\n",
    "    \"\"\"\n",
    "    If we find a topic (or a sub-topic), set the parse state accordingly.\n",
    "    If we find a category, segmentation, or variable, process that.  In either case, return the value of the next row to process, or -1 if none.\n",
    "    \"\"\"\n",
    "    global topic_cell, category_cell\n",
    "    \n",
    "    if row==None:\n",
    "        # We have to put the topic in header_row+1 because header_row is column info, not data, for the dataframe\n",
    "        row = crm.header_row+1\n",
    "        topic_cell = ws.cell(row, crm.topic_col)\n",
    "        topic_cell.value = ws.title\n",
    "        print(f'process_topic {row}: setting topic from title {ws.title}')\n",
    "\n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    var_text = cell.value\n",
    "        \n",
    "    if var_text:\n",
    "        if topic_cell==None or topic_cell.value!=ws.title:\n",
    "            # Let's assume topic text is not parenthetical, but titular\n",
    "            var_text = re.sub(r'\\(.+\\)', '', var_text)\n",
    "            var_words = var_text.split(' ')\n",
    "            for word in var_words:\n",
    "                if topic_cell and word.lower() == topic_cell.value:\n",
    "                    # Not a new topic\n",
    "                    break\n",
    "                if word.lower() in topic_keywords:\n",
    "                    print(f'process_topic {row}: setting topic {word}')\n",
    "                    topic_cell = ws.cell(row, crm.topic_col)\n",
    "                    topic_cell.value = word.lower()\n",
    "\n",
    "            if topic_cell==None:\n",
    "                print(f'worksheet {ws.title}: unknown topic {var_text}')\n",
    "                topic_cell = ws.cell(row, crm.topic_col)\n",
    "                topic_cell.value = ws.title\n",
    "                topic_keywords[ws.title] = []\n",
    "\n",
    "        # Try to extract units from Variable description\n",
    "        if ws.cell(row, crm.units_col).value==None:\n",
    "            p_exprs = re.findall(r'\\((.+)\\)', ws.cell(row, crm.var_col).value)\n",
    "            for p in p_exprs:\n",
    "                if find_units(p):\n",
    "                    print(f'process_topic {row}: setting units from var: {p}')\n",
    "                    ws.cell(row, crm.units_col).value = p\n",
    "                    break\n",
    "\n",
    "        # If we definitely have units, set the category\n",
    "        if ws.cell(row, crm.units_col).value:\n",
    "            print(f'process_topic {row}: setting category {var_text}')\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            ws.cell(row, crm.category_col).value = var_text\n",
    "\n",
    "            print(f'process_topic {row}: setting units {ws.cell(row, crm.units_col).value}')\n",
    "            units = find_units (ws.cell(row, crm.units_col).value)\n",
    "            if units == None:\n",
    "                error(f'unknown units {ws.cell(row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = units\n",
    "            \n",
    "            row = process_categories (ws, row)\n",
    "    else:\n",
    "        print(f'process_topic {row}: no var text')\n",
    "    if row < 0 or row >= crm.last_val_row:\n",
    "        return crm.last_val_row\n",
    "    return row+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659ec878-2e22-4330-b1da-e6c0d8fee782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_as_sub(cell1, cell2):\n",
    "    sub_score = 0\n",
    "    if cell1.font.b and cell2.font.b==False:\n",
    "        print('+bold')\n",
    "        sub_score += 1\n",
    "    if cell1.font.b==False and cell2.font.b:\n",
    "        print('-bold')\n",
    "        sub_score -= 1\n",
    "    if cell1.font.u and cell2.font.u==False:\n",
    "        print('+underline')\n",
    "        sub_score += 1\n",
    "    if cell1.font.u==False and cell2.font.u:\n",
    "        print('-underline')\n",
    "        sub_score += 1\n",
    "    if cell1.alignment.indent < cell2.alignment.indent:\n",
    "        print('+indent')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.indent > cell2.alignment.indent:\n",
    "        sub_score -= 1\n",
    "    if cell1.font.sz < cell2.font.sz:\n",
    "        print('+size')\n",
    "        sub_score += 1\n",
    "    elif cell1.font.sz > cell2.font.sz:\n",
    "        sub_score -= 1\n",
    "    if cell1.alignment.horizontal == 'left' and cell2.alignment.horizontal == 'right':\n",
    "        print('+halign')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.horizontal == 'right' and cell2.alignment.horizontal == 'left':\n",
    "        sub_score -= 1\n",
    "    print(f'sub_score = {sub_score}')\n",
    "    if sub_score > 0:\n",
    "        return True\n",
    "    if sub_score < 0:\n",
    "        return False\n",
    "    if sub_score == 0:\n",
    "        return None\n",
    "\n",
    "def process_categories(ws, row):\n",
    "    \"\"\"\n",
    "    Categories have units, which are now in the parse_context\n",
    "    \"\"\"\n",
    "    global category_cell, segmentation_stack\n",
    "    \n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        var_text = cell.value\n",
    "        segment_by = ''\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            if var_text and x in var_text:\n",
    "                segment_by = x\n",
    "                break\n",
    "        if formatted_as_sub(ws.cell(row, crm.var_col), ws.cell(row+1, crm.var_col)):\n",
    "            if segment_by:\n",
    "                c1, c2 = var_text.split(segment_by, 1)\n",
    "            else:\n",
    "                c1 = var_text\n",
    "                c2 = '(anon)'\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            category_cell.value = c1\n",
    "            segmentation_stack = [ ws.cell(row, crm.segmentation_col) ]\n",
    "            segmentation_stack[-1].value = c2\n",
    "            print(f'process_categories {row}: segmenting {c1}:{c2}')\n",
    "            row = process_segmentation (ws, row+1)\n",
    "            if segmentation_stack != []:\n",
    "                print(f'process_categories: segmentation_stack = {segmentation_stack}')\n",
    "            if row < 0:\n",
    "                return row\n",
    "            continue\n",
    "        var_units = ws.cell(row, crm.units_col).value\n",
    "        var_species = ''\n",
    "        if var_units:\n",
    "            var_units = find_units (var_units)\n",
    "            m = re.search('r\\((.+)\\)', var_text)\n",
    "            if m:\n",
    "                var_species = m.group(1)\n",
    "                species_units = find_units(' '.join([var_units, var_species]))\n",
    "                if species_units:\n",
    "                    units = species_units\n",
    "                    var_text = ' '.join([var_text[0:m.start(1)], var_text[m.end(1)+1:]]).replace('  ', ' ')\n",
    "            else:\n",
    "                units = var_units\n",
    "        else:\n",
    "            units = ws.cell(category_cell.row, crm.units_col).value\n",
    "        ws.cell(row, crm.units_col).value = units\n",
    "        category_cell = ws.cell(row, crm.category_col)\n",
    "        category_cell.value = var_text\n",
    "        segmentation_stack = []\n",
    "        print(f'process_category {row}: processing variable')\n",
    "        row = process_var(ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c35c9d9-d4ed-4b71-a5cb-95a49ce2ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segmentation(ws, row):\n",
    "    global segmentation_stack\n",
    "    \n",
    "    seg_start_cell = ws.cell(row, crm.var_col)\n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        if formatted_as_sub(seg_start_cell, cell):\n",
    "            segmentation_stack.append(ws.cell(row, crm.var_col))\n",
    "            row = process_segmentation(ws, row)\n",
    "        elif formatted_as_sub(seg_start_cell, cell)==False:\n",
    "            segmentation_stack.pop()\n",
    "            return row\n",
    "        else:\n",
    "            row = process_var (ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "        if segmentation_stack:\n",
    "            print(f'process_segmentation: stack at end = {segmentation_stack}')\n",
    "            segmentation_stack = []\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fef3e2c-2cbb-4351-baac-8ef069c34441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_var(ws, row):\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    \n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    var_text = cell.value\n",
    "    m = re.search(r'^(.*) \\((.*?)\\)', var_text)\n",
    "    if m and '-based' not in m.group(2) and 'scope' not in m.group(2).lower():\n",
    "        if m.group(2) in ureg:\n",
    "            print(f'process_var {row}: found species or units in {var_text}')\n",
    "            units = ws.cell(row, crm.units_col).value\n",
    "            if units:\n",
    "                species_units = find_units(' '.join([units, m.group(2)]))\n",
    "                if species_units:\n",
    "                    var_text = m.group(1).rstrip()\n",
    "                    units = species_units\n",
    "                else:\n",
    "                    print(f'??? Not overriding {units} with {m.group(2)}')\n",
    "                    # units = ws.cell(row, crm.units_col).value\n",
    "            else:\n",
    "                print(f'??? Overriding {units} with {m.group(2)}')\n",
    "                units = m.group(2)\n",
    "            if units != ws.cell(category_cell.row, crm.units_col).value:\n",
    "                print(f'changing units from category: {ws.cell(category_cell.row, crm.units_col).value} to {units}')\n",
    "                ws.cell(row, crm.units_col).value = units\n",
    "        elif m.group(2).lower() in topic_keywords[topic_cell.value]:\n",
    "            # Scope 1 is actually a sneaky segmentation\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            category_cell.value = m.group(2)\n",
    "        else:\n",
    "            print(f'process_var {row}: unhandled ( {m.group(2)} )')\n",
    "    else:\n",
    "        if ws.cell(row, crm.units_col).value == None:\n",
    "            print(f'process_var {row}: propagating units {ws.cell(category_cell.row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = ws.cell(category_cell.row, crm.units_col).value\n",
    "        else:\n",
    "            print(f'process_var {row}: using units {ws.cell(row, crm.units_col).value}')\n",
    "    ws.cell(row, crm.topic_col).value = topic_cell.value\n",
    "    ws.cell(row, crm.category_col).value = category_cell.value\n",
    "    if segmentation_stack != []:\n",
    "        ws.cell(row, crm.segmentation_col).value = '::'.join(s.value for s in segmentation_stack)\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11df632b-8a86-4b42-b0bf-19549e6ff623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? The header row color is going to be spreadsheet-specific.  This is what DPDHL gives us.\n",
    "\n",
    "import cell2rgb\n",
    "from cell2rgb import cell2rgb\n",
    "\n",
    "def find_header_row(wb, ws):\n",
    "    # If we haven't found the header by max_row-1, we'll never find it...\n",
    "    for row in range(1, ws.max_row):\n",
    "        color = cell2rgb(wb, ws.cell(row,1))\n",
    "        if color == crm.header_color:\n",
    "            return row\n",
    "        print(f'color = {color}')\n",
    "    error('No header found')\n",
    "    return -1\n",
    "\n",
    "# The role of this function is to capture and distrbute data attributes that can be inferred/applied to subsequent rows\n",
    "def split_cell(c):\n",
    "    notes = ''\n",
    "\n",
    "    # Deal with None\n",
    "    if c.value:\n",
    "        h = str(c.value)\n",
    "    else:\n",
    "        h = ''\n",
    "\n",
    "    # Don't let 'Scope 1' look like a note\n",
    "    m = re.search(r'[^ ](\\d+)$', h)\n",
    "    if m:\n",
    "        notes = m.group(1)\n",
    "        h = h[0:m.start(1)]\n",
    "    else:\n",
    "        m = re.search(r'\\s*\\[[A-Z]\\](\\s?\\[[A-Z]\\])*', h)\n",
    "        if m:\n",
    "            notes = m.group(0)\n",
    "            h = h[0:m.start(0)].strip()\n",
    "\n",
    "    # If we have a ':' in the cell, treat that as a major connector that overrides splitting on per/by/of\n",
    "    if ':' not in h:\n",
    "        # If the variable expresses a segmentation, pass that back accordingly\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            sub_h_arr = h.split(x, 1)\n",
    "            if len(sub_h_arr)>1:\n",
    "                return notes, sub_h_arr[0], sub_h_arr[1]\n",
    "\n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    m = re.search(r'^(.*) \\((.*?)\\)', h)\n",
    "    if m:\n",
    "        return notes, m.group(1), m.group(2)\n",
    "    return notes, h, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7606077b-0034-4e48-8d29-7cddfa2380d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pre-process the structure of the worksheet so that it can be trivially loaded into a dataframe for further reshaping.\n",
    "\n",
    "# Stash notes for each worksheet here.  These are *per worksheet*\n",
    "# ??? In the case of DPDHL, there's a Comment field we don't track, which means we miss a stated target\n",
    "ws_notes = {}\n",
    "\n",
    "def preprocess(wb, ws):\n",
    "    global crm, value_vars\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    def crop_sheet(ws):\n",
    "        global crm\n",
    "        # Frist, set max_row/max_column based on actually active cells, not cells with random spaces or empty strings\n",
    "        this_max_row = 1\n",
    "        this_max_col = 1\n",
    "        for row in range(1,ws.max_row+1):\n",
    "            row_max_col = None\n",
    "            for col in range(1,ws.max_column+1):\n",
    "                cell = ws.cell(row,col)\n",
    "                if cell.value==None:\n",
    "                    continue\n",
    "                if type(cell.value)==str and cell.value.strip()=='':\n",
    "                    cell.value = None\n",
    "                    continue\n",
    "                if col > this_max_col:\n",
    "                    this_max_col = col\n",
    "                row_max_col = col\n",
    "            if row_max_col:\n",
    "                this_max_row = row\n",
    "        print('crop_sheet')\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        ws.delete_rows(this_max_row+1,ws.max_row)\n",
    "        ws.delete_cols(this_max_col+1,ws.max_column)\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        crm.last_col = ws.max_column\n",
    "    \n",
    "    crm.preprocess()\n",
    "    \n",
    "    topic_cell = None\n",
    "    category_cell = None\n",
    "    segmentation_stack = []\n",
    "\n",
    "    # Remove merged cells\n",
    "    mergedRanges=ws.merged_cells.ranges\n",
    "    while mergedRanges:\n",
    "        for entry in mergedRanges:\n",
    "            ws.unmerge_cells(str(entry))\n",
    "\n",
    "    if crm.max_hidden_col and wb.worksheets[0]==ws:\n",
    "        ws.delete_cols(1,crm.max_hidden_col)\n",
    "    crop_sheet(ws)\n",
    "\n",
    "    if crm.init_header_row:\n",
    "        crm.header_row = crm.init_header_row\n",
    "    else:\n",
    "        crm.header_row = find_header_row (wb, ws)\n",
    "    \n",
    "    # Reset this for each worksheet\n",
    "    if crm.units_row >= 0:\n",
    "        crm.units_row = 0\n",
    "\n",
    "    col = crm.val_col\n",
    "    last_val_col = crm.last_val_col or col\n",
    "    while crm.last_val_col==None or col<=crm.last_val_col:\n",
    "        # ??? Deal with note in header value (such as '2019(b)' or, God forbit '20197' where the superscripted 7 just sits like it's part of the number)\n",
    "        if crm.year_regex:\n",
    "            maybe_year = re.sub(crm.year_regex, r'\\1', str(ws.cell(crm.header_row, col).value))\n",
    "        else:\n",
    "            maybe_year = str(ws.cell(crm.header_row, col).value)\n",
    "        if len(maybe_year)>=4 and maybe_year[0:2]=='20' and maybe_year[2].isdigit() and maybe_year[3].isdigit():\n",
    "            ws.cell(crm.header_row, col).value = maybe_year[0:4]\n",
    "            last_val_col = col\n",
    "        elif crm.last_val_col==None:\n",
    "            crm.last_val_col = last_val_col\n",
    "            break\n",
    "        col = col+1\n",
    "    value_vars = [ None ] * (crm.last_val_col-crm.val_col+1)\n",
    "    for col in range(crm.val_col, crm.last_val_col+1):\n",
    "        value_vars[col-crm.val_col] = ws.cell(crm.header_row, col).value\n",
    "    print(value_vars)\n",
    "    \n",
    "    # Make space for TOPIC : CATEGORY : SEGMENTATION triple.\n",
    "    # This triple could very well become an index into a data framework (such as SASB, TCFD, etc)\n",
    "    new_column_count = (len(ingest_columns)-1\n",
    "                        -int(crm.notes_col!=None)\n",
    "                        -int(crm.topic_col!=None)\n",
    "                        -int(crm.category_col!=None)\n",
    "                        -int(crm.units_col!=None))\n",
    "    ws.insert_cols(crm.last_val_col+1,amount=new_column_count)\n",
    "    if crm.notes_col==None:\n",
    "        crm.notes_col = crm.last_val_col+ingest_col_offsets['Notes']\n",
    "    ws.cell(crm.header_row,crm.notes_col).value = 'Notes'\n",
    "    if crm.topic_col==None:\n",
    "        crm.topic_col = crm.last_val_col+ingest_col_offsets['Topic']\n",
    "    ws.cell(crm.header_row,crm.topic_col).value = 'Topic'\n",
    "    if crm.category_col==None:\n",
    "        crm.category_col = crm.last_val_col+ingest_col_offsets['Category']\n",
    "    ws.cell(crm.header_row,crm.category_col).value = 'Category'\n",
    "    crm.segmentation_col = crm.last_val_col+ingest_col_offsets['Segmentation']\n",
    "    ws.cell(crm.header_row,crm.segmentation_col).value = 'Segmentation'\n",
    "    if crm.units_col==None:\n",
    "        crm.units_col = crm.last_val_col+ingest_col_offsets['Unit']\n",
    "    ws.cell(crm.header_row,crm.units_col).value = 'Unit'\n",
    "    ws.cell(crm.header_row,crm.var_col).value = 'Variable'\n",
    "        \n",
    "    crm.last_col = crm.last_col + new_column_count\n",
    "\n",
    "    # Find notes in header line (such as a callout that measurement systems changed in a particular year)\n",
    "    for col in range(crm.units_col, crm.val_col):\n",
    "        notes, main_text, segmentation = split_cell(ws.cell(crm.header_row, col))\n",
    "        if notes:\n",
    "            # main_text has the note removed from it\n",
    "            ws.cell(crm.header_row, col).value = main_text\n",
    "            if segmentation:\n",
    "                error('found note, but lost this: ' + segmentation)\n",
    "    # Find notes in value fields.  Need to remove so values can function like numbers, not text\n",
    "    for col in range(crm.last_val_col+1, crm.last_col+1):\n",
    "        notes, main_text, segmentation = split_cell(ws.cell(crm.header_row, col))\n",
    "        if notes:\n",
    "            # main_text has the note removed from it\n",
    "            ws.cell(crm.header_row, col).value = main_text\n",
    "            if segmentation:\n",
    "                error('found note, but lost this: ' + segmentation)\n",
    "    # Find last row of actual values so we can process notes at the end\n",
    "    for row in range(ws.max_row, 0, -1):\n",
    "        if any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "            crm.last_val_row = row\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34019c7e-8467-4b37-b0e2-96b13bacd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(wb, ws):\n",
    "    \n",
    "    # Intended for Shell notes\n",
    "    def save_ws_notes(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        note_label, note_text = note.split(' ', 1)\n",
    "        ws_notes[ws.title][note_label] = note_text.strip()\n",
    "    \n",
    "    # Intended for DPDHL notes\n",
    "    def save_ws_notes2(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        notes = re.split(r' (\\d+)\\)\\s+', note)\n",
    "        print('NOTES')\n",
    "        print(notes)\n",
    "        print('END NOTES')\n",
    "        ws_notes[ws.title]['0'] = notes[0]\n",
    "        for i in range(int(len(notes)/2)):\n",
    "            ws_notes[ws.title][notes[1+2*i]] = notes[2+2*i].strip()\n",
    "    \n",
    "    for row in range(crm.last_val_row+1, ws.max_row+1):\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        # Find either bracketed note or note that begins with possible superscript\n",
    "        if cell.value[0]=='[':\n",
    "            save_ws_notes(ws, cell.value)\n",
    "            continue\n",
    "        elif re.search(r'^[^(]*\\d[)]', str(cell.value)):\n",
    "            save_ws_notes2(ws, cell.value)\n",
    "        if re.search(r'notes', str(cell.value), re.I):\n",
    "            finish_notes(row)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7aa4190-7ea4-4f5b-afab-cc055c0d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a nicely formatted workbook, do the rest of our work (including writing to Trino) using dataframes\n",
    "\n",
    "# IPIECA, SASB, and GRI columns all feed metadata\n",
    "\n",
    "def ws_to_df(wb, i):\n",
    "    data = islice(wb.worksheets[i].values, crm.header_row_list[i]-1, None)\n",
    "    cols = list(next(data))\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 0, None) for r in data)\n",
    "    cols[crm.units_col-1] = 'Unit'            # Already set by Shell; DF indexes are XLSX-1\n",
    "    df = pd.DataFrame(data, columns=cols) # we don't pass in an index here\n",
    "\n",
    "    # Remove null columns\n",
    "    df = df[[c for c in df.columns if c!= None]]\n",
    "    \n",
    "    # For now, do not remove rows lacking units.  Those are basically where Notes are stored (for better or worse).\n",
    "    # print('rows lacking proper Units')\n",
    "    # display(df[df['Unit'].isnull()])\n",
    "    df = df.loc[df.Unit.notna() | df.Category.isna()]\n",
    "\n",
    "    # Clear out data that is n/a, n/c (not collected), n/d (not disclosed)\n",
    "    df[df['Unit'].notna()].replace(to_replace='^n/[acd]$', value='', regex=True, inplace=True)\n",
    "    \n",
    "    # Change numerical years to strings to make pandas indexing behave\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    # Drop completely empty rows\n",
    "    df.dropna(how='all', axis=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63095f7-55e2-4eef-b1d4-379fcf9ebaba",
   "metadata": {},
   "source": [
    "Write out polymorphic dataframe in LONG format.  This follows tidy data model, with one variable observation per row.  \n",
    "Polymorphic means that Units/dimensions of each row are specified, but not necessarily the same row to row.  \n",
    "Aggregation functions must be careful that selection criteria does not mix up incompatible unit types and/or observation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ffed37-1d28-4038-8390-d2f5ec91f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = None\n",
    "ws = None\n",
    "\n",
    "def ingest_filename(filename):\n",
    "    global crm, wb, ws\n",
    "    \n",
    "    crm = filename_magic[filename]\n",
    "\n",
    "    wb = load_workbook(crm.input_filename, data_only=True)\n",
    "\n",
    "    # For a label like \"Scope 1 emissions by country\" return ['', 'Scope 1 emissions', 'country']\n",
    "    # For a label like \"Direct GHG emissions (Scope 1) [A] [B] [C] [D]\" return ['[A] [B] [C] [D]', 'Direct GHG emissions (Scope 1) ', '']\n",
    "\n",
    "    long_fmt_filename = ''\n",
    "    wide_fmt_filename = ''\n",
    "\n",
    "    for i in range(crm.ws_start[0], crm.ws_end[0]+1):\n",
    "        ws = wb.worksheets[i]\n",
    "        ws_notes = {}\n",
    "        preprocess(wb, ws)\n",
    "        \n",
    "        row = process_topic(ws, crm.topic_row)\n",
    "\n",
    "        while row < crm.last_val_row:\n",
    "            if row == crm.header_row_list[i]:\n",
    "                row = row+1\n",
    "                continue\n",
    "            print(f'ingest_file: processing {row}')\n",
    "            row = process_topic(ws, row)\n",
    "    \n",
    "        # process_topic (wb, ws, header_row_list[i])\n",
    "        # preprocess2(wb, ws)\n",
    "        postprocess(wb, ws)\n",
    "        # What to do with ws_notes???\n",
    "        df = ws_to_df(wb, i)\n",
    "        df.replace('',pd.NA,inplace=True)\n",
    "        print(f'wb({i}) dataframe')\n",
    "        display(df.loc[0:min(len(df),30)])\n",
    "        melted_df = pd.melt(df, id_vars=ingest_columns, var_name='Year', value_name='Value', value_vars=value_vars)\n",
    "        melted_df.dropna(subset=['Value'],inplace=True)\n",
    "        melted_df = melted_df.astype({'Year': 'int'})\n",
    "\n",
    "        if i==crm.ws_start[0]:\n",
    "            report_year = max(df.columns[crm.val_col-1:crm.last_val_col])\n",
    "            long_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'LONG.xlsx'])\n",
    "            writer_long = pd.ExcelWriter(long_fmt_filename)\n",
    "            wide_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'WIDE.xlsx'])\n",
    "            writer_wide = pd.ExcelWriter(wide_fmt_filename)\n",
    "\n",
    "        # This writes out LONG data with TOPIC as SHEET_NAME.  Later we'll create a truly long table with TOPIC restored as a column\n",
    "        melted_df.loc[:, melted_df.columns != 'Topic'].to_excel(writer_long, index=False, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "        print(ws.title)\n",
    "        columns = ['Variable', 'Unit']\n",
    "        # We need these columns to reshape our data\n",
    "        for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "            if df[extra_col].notna().any():\n",
    "                columns.append(extra_col)\n",
    "        # In the case of Shell, we have only one topic per sheet, so can transform melted_df directly\n",
    "        pf = melted_df.pivot(index=['Year', 'Topic'], columns=columns, values=['Value'])\n",
    "        pf = pf.droplevel('Topic')\n",
    "        # Once reshaped, the extra columns actually appear as multi-level indexes.  Drop them from also behaving like values\n",
    "        pf[[c for c in columns if c not in ['Variable', 'Unit']]] = pd.NA\n",
    "        pf.dropna(how='all', axis=1, inplace=True)\n",
    "        pf.to_excel(writer_wide, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "    writer_long.close()\n",
    "    writer_wide.close()\n",
    "    \n",
    "    # We are now working with our own workbook, which doesn't have a zero-index sheet to ignore\n",
    "    # Make the workbook more legible to those reading it\n",
    "    long_wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "    for ws in long_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            if get_column_letter(col)=='A':\n",
    "                width = 40\n",
    "            elif get_column_letter(col) in ['B', 'E']:\n",
    "                width = 15\n",
    "            elif get_column_letter(col) in ['C', 'D']:\n",
    "                width = 25\n",
    "            else:\n",
    "                width = 10\n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=width)\n",
    "        ws.column_dimensions = dim_holder\n",
    "    \n",
    "    long_wb.save(long_fmt_filename)\n",
    "    long_wb.close()\n",
    "    \n",
    "    def as_text(value):\n",
    "        if value is None:\n",
    "            return \"\"\n",
    "        return str(value)\n",
    "    \n",
    "    # Write out dataframe in WIDE format.  This data is technically tidy, with one multi-dimensional observation per row.\n",
    "    # Units/dimensions are consistent on a per-column basis, making it easy to aggregate column-based data.\n",
    "    wide_wb = load_workbook(wide_fmt_filename, data_only=True)\n",
    "    # Make the workbook more legible to those reading it\n",
    "    for ws in wide_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            cell = ws.cell(2, col)\n",
    "            cell.alignment = Alignment(wrap_text=True,vertical='top') \n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=max(10,1+len(as_text(cell.value))/3))\n",
    "        ws.column_dimensions = dim_holder\n",
    "\n",
    "    wide_wb.save(wide_fmt_filename)\n",
    "    wide_wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09fedea-25f5-4559-b8e6-b7b12ec6a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPDHL-ESG-Statbook-2020-en.xlsx\n",
      "crop_sheet\n",
      "90 x 13\n",
      "80 x 10\n",
      "color = FF00B050\n",
      "color = 00000000\n",
      "color = 00000000\n",
      "color = 00000000\n",
      "color = 00000000\n",
      "color = 00000000\n",
      "color = 00000000\n",
      "['2016', '2017', '2018', '2019', '2020']\n",
      "worksheet Environmental Group Overview: unknown topic Environmental Data at Group levels\n",
      "ingest_file: processing 2\n",
      "ingest_file: processing 3\n",
      "find units: nothing found for KPI\n",
      "ingest_file: processing 4\n",
      "ingest_file: processing 5\n",
      "ingest_file: processing 6\n",
      "ingest_file: processing 7\n",
      "process_topic 7: no var text\n",
      "ingest_file: processing 9\n",
      "ingest_file: processing 10\n",
      "process_topic 10: setting category KPI: Carbon Efficiency Index (CEX)1\n",
      "process_topic 10: setting units Index points\n",
      "sub_score = 0\n",
      "process_category 10: processing variable\n",
      "process_var 10: unhandled ( CEX )\n",
      "+bold\n",
      "sub_score = 1\n",
      "process_categories 11: segmenting CO2e emissions total1:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 12: propagating units m t CO2e\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 13: propagating units m t CO2e\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 14: propagating units m t CO2e\n",
      "-bold\n",
      "sub_score = -1\n",
      "-bold\n",
      "sub_score = -1\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 15: segmenting CO2e emissions:modes1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 16: propagating units Share\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 17: propagating units Share\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 18: propagating units Share\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 19: propagating units Share\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "+bold\n",
      "sub_score = 1\n",
      "process_categories 20: segmenting CO2e intensity total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 21: propagating units Grams per € revenue\n",
      "-bold\n",
      "sub_score = -1\n",
      "-bold\n",
      "sub_score = -1\n",
      "sub_score = 0\n",
      "process_category 22: processing variable\n",
      "process_var 22: using units kWh * m\n",
      "+indent\n",
      "sub_score = 1\n",
      "process_categories 23: segmenting Fleet consumption total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 24: propagating units None\n",
      "+bold\n",
      "sub_score = 1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 25: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 26: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 27: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 28: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 29: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 30: unhandled ( LPG )\n",
      "-bold\n",
      "sub_score = -1\n",
      "-bold\n",
      "sub_score = -1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 31: propagating units None\n",
      "+bold\n",
      "sub_score = 1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 32: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 33: unhandled ( CNG )\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 34: unhandled ( LNG )\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "sub_score = -1\n",
      "sub_score = -1\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 35: segmenting Buildings consumption total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 36: propagating units None\n",
      "+indent\n",
      "sub_score = 1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 37: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 38: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 39: propagating units None\n",
      "sub_score = -1\n",
      "sub_score = -1\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 40: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 41: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 42: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 43: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 44: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 45: unhandled ( LPG )\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "sub_score = 0\n",
      "process_category 46: processing variable\n",
      "process_var 46: using units t\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 47: segmenting Mono-nitrogen oxides (NOx) total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 48: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 49: propagating units None\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 50: segmenting Sulfur dioxide (SO2) total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 51: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 52: propagating units None\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 53: segmenting Particulate matter (PM10) total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 54: propagating units None\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 55: propagating units None\n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "sub_score = 0\n",
      "find units: nothing found for Share\n",
      "process_category 56: processing variable\n",
      "process_var 56: propagating units None\n",
      "sub_score = 0\n",
      "find units: nothing found for No. \n",
      "process_category 57: processing variable\n",
      "process_var 57: propagating units None\n",
      "+bold\n",
      "+indent\n",
      "sub_score = 2\n",
      "process_categories 58: segmenting Certified sites total:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 59: propagating units No. \n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 60: propagating units No. \n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 61: propagating units No. \n",
      "-bold\n",
      "sub_score = -2\n",
      "-bold\n",
      "sub_score = -2\n",
      "sub_score = 0\n",
      "process_category 62: processing variable\n",
      "process_var 62: using units No. \n",
      "+bold\n",
      "sub_score = 1\n",
      "process_categories 63: segmenting Water consumption:(anon)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 64: using units m liter\n",
      "-bold\n",
      "sub_score = -1\n",
      "-bold\n",
      "sub_score = -1\n",
      "+bold\n",
      "sub_score = 1\n",
      "process_categories 65: segmenting Scope 2 CO2e emissions calculated:location-based method (GHG)\n",
      "sub_score = 0\n",
      "sub_score = 0\n",
      "process_var 66: using units m t CO2e\n",
      "-bold\n",
      "sub_score = -1\n",
      "-bold\n",
      "sub_score = -1\n",
      "sub_score = 0\n",
      "process_category 67: processing variable\n",
      "process_var 67: propagating units None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-96a16add3a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename_magic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mingest_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3a8d85c765f9>\u001b[0m in \u001b[0;36mingest_filename\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ingest_file: processing {row}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# process_topic (wb, ws, header_row_list[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bddc3cb0d5dc>\u001b[0m in \u001b[0;36mprocess_topic\u001b[0;34m(ws, row)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_categories\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'process_topic {row}: no var text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2cfbba1cd2d3>\u001b[0m in \u001b[0;36mprocess_categories\u001b[0;34m(ws, row)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msegment_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m' per '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' by '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' of '\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0msegment_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "for filename in filename_magic:\n",
    "    print(filename)\n",
    "    crm = filename_magic[filename]\n",
    "    ingest_filename(filename)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2b98b-4100-4300-b818-fac283172197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg('m kWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ada4c-9a2a-4b39-87bc-859c4f094c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm.topic_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeed95-70f4-4fb4-bf0c-66bb23abdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_units('trillion (10^12) MJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a8bc7-b1ea-4a7a-937b-d62b821209e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell2rgb import cell2rgb\n",
    "cell2rgb(ws.cell(8, crm.units_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef452a-5172-48b8-8463-0f64f92667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gj = f\"{ureg('1e9 J').to_compact():P}\".split(' ', 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83df93b-1772-4b08-b0e9-77f59e82cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg(gj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbab97-6b10-436d-9157-b09f62913ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg('1e9 J').to_compact().u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2828c7f-7b70-44bb-96d2-7348a97ea763",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_regex = re.compile(r'^(((mi|bi|tri|quadri)llion)|(thousand)|(hundred))(s of)? ', re.I)\n",
    "scale_regex = re.compile(r'^((mi|bi|tri|quadri)llion|thousand|hundred)(s of)? ', re.I)\n",
    "sc_xlate[re.search(scale_regex, 'thousand ').group(1)[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a405a-8d21-42f7-ac61-9b6c5f9e2011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bletch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11503d-24dd-4d87-a57c-ca0355715485",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm = filename_magic['greenhouse-gas-and-energy-data-shell-sr20.xlsx']\n",
    "wb = load_workbook(crm.input_filename, data_only=True)\n",
    "ws = wb.worksheets[2]\n",
    "preprocess(wb, ws)\n",
    "crm.topic_row=3\n",
    "cell = ws.cell(crm.topic_row, crm.var_col)\n",
    "pc = parse_context(cell, 'topic', cell.value)\n",
    "parse_context.wb = wb\n",
    "parse_context.ws = ws\n",
    "\n",
    "row = process_topic(pc, crm.topic_row)\n",
    "\n",
    "while row < ws.max_row:\n",
    "    if row == crm.init_header_row:\n",
    "        row = row+1\n",
    "        continue\n",
    "    new_row = process_topic(pc, row)\n",
    "    if new_row == -1:\n",
    "        break\n",
    "    if new_row == row:\n",
    "        error('lost increment')\n",
    "    row = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d4733-e25f-4746-9fcf-270483363f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess2(wb, ws):\n",
    "    global crm\n",
    "    \n",
    "    scope1_gases = ['CO2', 'CH4', 'N2O', 'HFC', 'SF6', 'PFC', 'NF3', 'CO2e', 'NOx', 'SO2', 'PM10']\n",
    "    scope1_regex = re.compile('(' + ')|('.join(scope1_gases) + ')', flags=re.I)\n",
    "    \n",
    "    scope3_dict = { 'Purchased Goods and Services':1,\n",
    "                    'Capital Goods':2,\n",
    "                    'Fuel and Energy Related Activities':3,\n",
    "                    'Fuel and Energy Related Activities (Market-Based)':3,\n",
    "                    'Fuel and Energy Related Activities (Location-Based)':3,\n",
    "                    'Upstream Transportation and Distribution':4,\n",
    "                    'Transportation services':4,                # DPDHL\n",
    "                    'Fuel- and energy-related activities':4,    # DPDHL\n",
    "                    'Waste Generated in Operations (Large office campuses)':5,\n",
    "                    'Business Travel':6,\n",
    "                    'Employee Commuting':7,\n",
    "                    'Upstream Leased Assets':8,\n",
    "                    'Downstream Transportation and Distribution':9,\n",
    "                    'Processing of Sold Products':10,\n",
    "                    'Use of Sold Products':11,\n",
    "                    'End of Life Treatment of Sold Products':12,\n",
    "                    'Downstream Leads Assets':13,\n",
    "                    'Franchises':14,\n",
    "                    'Investments':15 }\n",
    "\n",
    "    def normalize_scope3(s3):\n",
    "        # Later we should normalize against the scope3_dict\n",
    "        return s3\n",
    "    \n",
    "    # This only returns notes and a value, and it presumes the cell is None, Numeric, or would be Numeric, but for the note.\n",
    "    def split_value_cell(c):\n",
    "        if c.value==None:\n",
    "            return '', ''\n",
    "        \n",
    "        notes = ''\n",
    "        v = str(c.value)\n",
    "        m = re.search(r'(\\([a-z]\\))+', v)\n",
    "        if m:\n",
    "            notes = m.group(0)\n",
    "            v = v.replace(notes,'').strip().replace(',','')\n",
    "            if '.' in v:\n",
    "                v = float(v)\n",
    "            else:\n",
    "                v = int(v)\n",
    "            return notes, v\n",
    "        return '', c.value\n",
    "    \n",
    "    # Convert reported units to things standard in `pint`\n",
    "    unit_dict = { 'trillion (10^12) MJ':'PJ', 'million MWh':'TWh', 'MW':'MW', \n",
    "                  'million tonnes CO2e':'Mt CO2e', 'tonnes CO2e':'t CO2e', 'm t CO2e':'Mt CO2e', 'MT CO2e':'Mt CO2e',\n",
    "                  'million tonnes':'Mt', 'thousand tonnes':'kilot', 'tonnes':'t', 'kg':'kg', 'MT':'Mt', 'Lbs':'lbs', 'Metric Tons':'t', \n",
    "                  'tBtu':'TBtu',\n",
    "                  'm liter':'M liter', 'Grams per € revenue':'Grams / EUR',\n",
    "                  'Millions of m3':'1000 dam', 'm3':'m3', 'Gallons':'gal',\n",
    "                  'Million Gallons':'M gallons', 'Billions of Liters':'10^9 l', 'billions of Liters':'10^9 l',}\n",
    "    u2u_dict = { '%':'pct', 'Grams per € revenue':'Grams / EUR', 'revenue':'EUR', 'MM$ revenue':'1000000 revenue',\n",
    "                 'short ton':'short_ton', 'No.':'[]', 'Nb':'[]', }\n",
    "    scale_regex = re.compile(r'^(((mi|bi|tri|quadri)llion)|(thousand)|(hundred))(s of)? ', re.I)\n",
    "    sc_xlate = {'hun':1e2, 'tho':1e3, 'mil':1e6, 'bil':1e9, 'tri':1e12, 'qua':1e15}\n",
    "    def normalize_units(u, g):\n",
    "        scale = 1.0\n",
    "        m = re.search(scale_regex, u)\n",
    "        if m:\n",
    "            u = u[m.end(0):].strip()\n",
    "            scale = sc_xlate[m.group(0)[0:3].lower()]\n",
    "        m = re.search(r'((short)|(long)|(metric))( )ton', u, re.I)\n",
    "        if m:\n",
    "            u = '_'.join([u[0:m.start(5)], u[m.start(5)+1:]])\n",
    "            print(u)\n",
    "        if u in u2u_dict:\n",
    "            u = u2u_dict[u]\n",
    "        if g and g not in u:\n",
    "            u = ' '.join([u, g])\n",
    "        return ureg(u) * scale\n",
    "        \n",
    "        if g in u:\n",
    "            g = ''\n",
    "        if '/' in u:\n",
    "            u1, u2 = u.split('/', 1)\n",
    "            if g in u2:\n",
    "                g1 = ''\n",
    "                g2 = g\n",
    "            else:\n",
    "                g1 = g\n",
    "                g2 = ''\n",
    "            return ' / '.join([normalize_units(u1, g1), normalize_units(u2, g2)])\n",
    "        u = u.strip()\n",
    "        if u in unit_dict and u!=unit_dict[u]:\n",
    "            return normalize_units(unit_dict[u], g)\n",
    "        if g:\n",
    "            return ' '.join([u, g])\n",
    "        return u\n",
    "    \n",
    "    def finish_notes(row):\n",
    "        print('finish_notes @ {}'.format(row))\n",
    "    \n",
    "    if crm.topic_row==None:\n",
    "        topic = ws.title\n",
    "    else:\n",
    "        topic = ws.cell(crm.topic_row,crm.var_col).value\n",
    "    \n",
    "    notes = ''\n",
    "    category = ''\n",
    "    categories = ['', '', '']\n",
    "    segmentation = ''\n",
    "    scope1_gas = ''\n",
    "    units = ''\n",
    "    \n",
    "    # Make the inferences, filling out TOPIC : CATEGORY : SEGMENTATION, as well as inferring/adjusting UNITS\n",
    "    # If we start with no Units column, then all units can be carried across from parenthetical expressions in Variable\n",
    "    # If we do have a Units column, either it's fully expressed (like Shell),\n",
    "    # or a prevailing unit can be carried down (and cross-combined with parenthetical expressions in Variable)\n",
    "    \n",
    "    for row in range(crm.header_row+1, crm.last_val_row+1):\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        \n",
    "        # Needed to put dataframe together later\n",
    "        if (crm.topic_col < crm.var_col):\n",
    "            topic = ws.cell(row, crm.topic_col).value\n",
    "        else:\n",
    "            ws.cell(row, crm.topic_col).value = topic\n",
    "        \n",
    "        # Carry-forward comes from state variables: category, segmentation, units\n",
    "        if cell.value==None:\n",
    "            continue\n",
    "        \n",
    "        # *BOLD* text indicates we have a header to parse, as do particular colors\n",
    "        if cell.fill.fgColor.type == 'rgb':\n",
    "            cat_color = format(cell.fill.fgColor.rgb)\n",
    "        else:\n",
    "            theme = cell.fill.start_color.theme\n",
    "            tint = cell.fill.start_color.tint\n",
    "            cat_color = theme_and_tint_to_rgb(wb, theme, tint)\n",
    "        if cat_color not in crm.cat_color_dict:\n",
    "            cat_color = None\n",
    "\n",
    "        if cell.font.b or cat_color:\n",
    "            notes, category, segmentation = split_cell(cell)\n",
    "\n",
    "            if cat_color==None:\n",
    "                # Shell doesn't use colors\n",
    "                categories[0] = re.sub(r' total\\s?', '', category)\n",
    "            else:\n",
    "                categories[crm.cat_color_dict[cat_color]] = re.sub(r' total\\s?', '', category)\n",
    "                for i in range(crm.cat_color_dict[cat_color]+1, len(categories)):\n",
    "                    categories[i] = ''\n",
    "                category = ':'.join([c for c in categories[0:2] if c])\n",
    "            if re.search(r'Scope\\s*3', category, flags=re.I):\n",
    "                m = re.search(r'Scope\\s+3 (CO2e?\\s*)?(emissions\\s+)(by.*categor((y)|(ies)))?', category, flags=re.I)\n",
    "                category = 'Scope 3 emissions'\n",
    "                segmentation = 'GHG Categories'\n",
    "            else:\n",
    "                if categories[2]:\n",
    "                    if segmentation:\n",
    "                        print('cat[2] = {}; segmentation = {}'.format(categories[2], segmentation))\n",
    "                    segmentation = categories[2]\n",
    "\n",
    "        if crm.units_row >= 0 and ws.cell(row, crm.units_col).value:\n",
    "            crm.units_row = row\n",
    "            # We might refine this as \"tonnes of WHAT\", depending on Variable\n",
    "            units = normalize_units (ws.cell(row, crm.units_col).value, '')\n",
    "\n",
    "        if any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "            # Has a variable observation.  Need to set/use units\n",
    "            if crm.units_row < 0 and any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "                # We have to dig out units for each and every variable row\n",
    "                notes, var_text, unit_text = split_cell(cell)\n",
    "                s1_gases = [ g for g in var_text.split(' ') if re.search(scope1_regex, g) ]\n",
    "                s1_gas = '' if s1_gases==[] else s1_gases[0]\n",
    "                if unit_text not in ['market-based', 'location_based']:\n",
    "                    units = normalize_units (unit_text, s1_gas)\n",
    "                # Could also check for fuel types, water types, and other things\n",
    "                ws.cell(row, crm.units_col).value = format(units.u, '~')\n",
    "            elif crm.units_row >= 0:\n",
    "                # If we have no units, borrow from following row (see 'opd-scope-1-2-ghg-emissions')\n",
    "                # Theory: if border line heavier above, borrow from below; if heavier below, borrow from above\n",
    "\n",
    "                # from openpyxl.styles.borders import Border, Side, BORDER_THIN\n",
    "                # thin_border = Border(\n",
    "                #     top=Side(border_style=BORDER_THIN, color='00000000'),\n",
    "                #     bottom=Side(border_style=BORDER_THIN, color='00000000')\n",
    "                # )\n",
    "                # ws.cell(row=3, column=2).border = thin_border\n",
    "                \n",
    "                if ws.cell(row, crm.units_col).value==None and ws.cell(row+1, crm.units_col).value!=None:\n",
    "                    ws.cell(row, crm.units_col).value = normalize_units(ws.cell(row+1, crm.units_col).value, '')\n",
    "\n",
    "                # If there is no disclosure here, move on with the notes/category/segmentation we've captured\n",
    "                if not any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "                    continue\n",
    "\n",
    "                # ??? Should correctly compute segmentation here and pass as argument to normalize_scope3\n",
    "                if category == 'Scope 3 emissions' and segmentation == 'GHG Categories':\n",
    "                    ws.cell(row, crm.var_col).value = normalize_scope3 (ws.cell(row, crm.var_col).value)\n",
    "                \n",
    "                # Try to get units and category from variable description by using a found unit, inferring from previous rows,\n",
    "                # and possibly combining with other info in the variable (such as gas species)\n",
    "                if ws.cell(row, crm.units_col).value==None:\n",
    "                    ws.cell(row, crm.units_col).value = units\n",
    "                units = ureg(ws.cell(row, crm.units_col).value)\n",
    "                \n",
    "                maybe_notes, maybe_category, xyzzy = split_cell(cell)\n",
    "                if maybe_var_units:\n",
    "                    maybe_var_units = re.sub(r'\\((.*)\\)', r'\\1', maybe_var_units)\n",
    "                    print('maybe_var_units: {}'.format(maybe_var_units))\n",
    "                    if ' per ' in maybe_var_units:\n",
    "                        u1, u2 = maybe_var_units.split(' per ', 1)\n",
    "                        units = normalize_units(u1, '') / normalize_units(u2, '')\n",
    "                    elif maybe_var_units in ureg:\n",
    "                        units = normalize_units (maybe_var_units, '')\n",
    "                    else:\n",
    "                        error('maybe_var_units: {}'.format(maybe_var_units))\n",
    "                    # We carry down units, whether we changed them this row or not\n",
    "                    ws.cell(row, crm.units_col).value = format(units.u, '~')\n",
    "                notes, category = maybe_notes, maybe_category\n",
    "\n",
    "        # Now fill the empty columns we created with the metadata we have inferred\n",
    "        ws.cell(row, crm.category_col).value = category\n",
    "        ws.cell(row, crm.segmentation_col).value = segmentation\n",
    "        if 'emissions' in category.lower() and not re.search('CO2e', ws.cell(row, crm.units_col).value, re.I):\n",
    "            m = re.search(scope1_regex, str(ws.cell(row, crm.var_col).value))\n",
    "            if m:\n",
    "                scope1_gas = m.group(0)\n",
    "            # else it carries forward\n",
    "        else:\n",
    "            scope1_gas = ''\n",
    "        if ws.cell(row, crm.units_col).value!=None:\n",
    "            units = normalize_units(ws.cell(row, crm.units_col).value, scope1_gas)\n",
    "            ws.cell(row, crm.units_col).value = format(units.u, '~')\n",
    "        # Find notes hiding in values \n",
    "        for col in range(crm.val_col, crm.last_val_col+1):\n",
    "            # print('cell({},{}) = {}'.format(row,col,ws.cell(row,col).value))\n",
    "            maybe_notes, value = split_value_cell(ws.cell(row, col))\n",
    "            if maybe_notes:\n",
    "                if maybe_notes not in notes:\n",
    "                    notes = notes + maybe_notes\n",
    "                ws.cell(row,col).value = value\n",
    "        # Scan for notes in remaining columns, but don't scan again the columns we ourselves created\n",
    "        # (namely notes, topic, category, segmentation, and possibly units)\n",
    "        for col in range(crm.last_val_col+1, crm.notes_col):\n",
    "            # print('cell({},{}) = {}'.format(row,col,ws.cell(row,col).value))\n",
    "            maybe_notes, main_text, error_if_nonempty = split_cell(ws.cell(row, col))\n",
    "            if maybe_notes:\n",
    "                if maybe_notes not in notes:\n",
    "                    notes = notes + maybe_notes\n",
    "                if error_if_nonempty:\n",
    "                    error('error_if_nonempty={}; cell({},{}) = {}'.format(error_if_nonempty,row,col,ws.cell(row, col)))\n",
    "                ws.cell(row,col).value = main_text\n",
    "        ws.cell(row, crm.notes_col).value = notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d95790-9edb-48ff-b5c3-fe4b2e37ca03",
   "metadata": {},
   "source": [
    "### Time for a Pint!\n",
    "\n",
    "See https://github.com/IAMconsortium/units/issues/9https://github.com/IAMconsortium/units/issues/9\n",
    "and https://github.com/openscm/openscm-units/issues/31https://github.com/openscm/openscm-units/issues/31\n",
    "and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9369e4-b14f-42cd-9b28-2f906092b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pint_pandas\n",
    "from openscm_units import unit_registry\n",
    "\n",
    "pint_pandas.PintType.ureg = u = unit_registry\n",
    "\n",
    "one_co2 = unit_registry(\"CO2\")\n",
    "print(one_co2)\n",
    "\n",
    "x = pd.DataFrame([[2.0,'Mt CO2']], columns=['Value', 'Unit'])\n",
    "print(x)\n",
    "x = x.astype({'Value': 'pint[Mt CO2]'})\n",
    "print(x.Value.pint.to('t CO2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39945448-35db-4179-ba8c-0d2cddb0e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u('Mt/1000000').to_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524e262-0708-41c9-9348-8815dbe0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = unit_registry\n",
    "Q_ = ureg.Quantity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31d04f51-689a-4022-aaa7-77206d273458",
   "metadata": {},
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = pint.UnitRegistry()\n",
    "Q_ = ureg.Quantity\n",
    "\n",
    "pint_pandas.PintType.ureg = ureg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bf3a7-bfca-4329-b503-0961841a52a8",
   "metadata": {},
   "source": [
    "Note that pint[unit] must be used for the Series constuctor, whereas the PintArray constructor allows the unit string or object.\n",
    "\n",
    "```\n",
    "    df = pd.DataFrame({\n",
    "        \"length\" : pd.Series([1.,2.], dtype=\"pint[m]\"),\n",
    "        \"width\" : PA_([2.,3.], dtype=\"pint[m]\"),\n",
    "        \"distance\" : PA_([2.,3.], dtype=\"m\"),\n",
    "        \"height\" : PA_([2.,3.], dtype=ureg.m),\n",
    "        \"depth\" : PA_.from_1darray_quantity(Q_([2,3],ureg.m)),\n",
    "    })\n",
    "```\n",
    "\n",
    "See https://pint.readthedocs.io/en/0.18/pint-pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07843c-4fb7-4fd1-b124-43acffea39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def long_ws_to_df(ws):\n",
    "    data = ws.values\n",
    "    cols = next(data)\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 1, None) for r in data)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "    # The original data has topic we construct.  It is removed when writing LONG data but can be restored from SHEET_NAME\n",
    "    if 'Topic' not in df.columns:\n",
    "        print('Restoring Topic ' + ws.title)\n",
    "        df.insert(crm.topic_col-1, 'Topic', ws.title)\n",
    "    \n",
    "    return df\n",
    "\n",
    "trino_df = pd.concat([long_ws_to_df(ws) for ws in wb.worksheets])\n",
    "    \n",
    "len(trino_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2879-1246-4faf-b229-b102dd971acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trino_df['Unit'].value_counts())\n",
    "trino_df.Unit.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075088-fa8f-446c-a377-1dc65cf34290",
   "metadata": {},
   "source": [
    "Now create data in Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8bb0c-a4b4-4028-9d27-8008582bbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937708e8-adcf-459e-8aec-d855c53a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956592b-0ce9-4716-963e-4e3823b17fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.now()\n",
    "# For now we used a fixed date so we don't fill things up needlessly\n",
    "timestamp = \"2008-09-03T20:56:35.450686Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1c7de-7caf-4772-a497-610394df13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'osc_corp_data'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447cdbd-4b5b-42b5-9386-bac8f52c8981",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75766c69-167c-4100-8811-ce8c49870759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(enforce_sql_column_names(df))\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}/{timestamp}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid,timestamp=timestamp)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5e83d-618d-43c0-aeb1-d0bc0706476d",
   "metadata": {},
   "source": [
    "### Write out Report with metadata\n",
    "\n",
    "Create the actual metadata for the source.  In this case, it is osc_corp_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a9c61-9180-446e-80b3-0cc95d34c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {}\n",
    "metadata_text = \"\"\"Title: AEP GHG and Energy Report, 2020\n",
    "Description: \n",
    "Version: 2020\n",
    "Release Date: \n",
    "URI: https://reports.shell.com/sustainability-report/2020/our-performance-data/greenhouse-gas-and-energy-data.html\n",
    "Copyright: \n",
    "License: \n",
    "Contact: \n",
    "Citation: \"\"\"\n",
    "\n",
    "for line in metadata_text.split('\\n'):\n",
    "    k, v = line.split(':', 1)\n",
    "    k = sql_compliant_name(k)\n",
    "    custom_meta_content[k] = v\n",
    "\n",
    "custom_meta_content['abstract'] = \"\"\"Abstract text\"\"\"\n",
    "custom_meta_content['name'] = 'osc_corp_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a3fe-4149-45e4-88b9-9bd3a7bd009d",
   "metadata": {},
   "source": [
    "Create the metadata for all the fields in all the tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd5046d-5237-42b9-9d40-508bfd177bcf",
   "metadata": {},
   "source": [
    "field_text = \"\"\"`country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [https://www.iso.org/iso-3166-country-codes.html]\n",
    "`country_long` (text): longer form of the country designation\n",
    "`name` (text): name or title of the power plant, generally in Romanized form\n",
    "`gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "`capacity_mw` (number): electrical generating capacity in megawatts\n",
    "`latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "`other_fuel1` (text): energy source used in electricity generation or export\n",
    "`other_fuel2` (text): energy source used in electricity generation or export\n",
    "`other_fuel3` (text): energy source used in electricity generation or export\n",
    "`commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "`owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "`source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "`url` (text): web document corresponding to the `source` field\n",
    "`geolocation_source` (text): attribution for geolocation information\n",
    "`wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "`year_of_capacity_data` (number): year the capacity information was reported\n",
    "`generation_data_source` (text): attribution for the reported generation information\"\"\"\n",
    "\n",
    "field_descs = [line.split(': ')[1] for line in field_text.split('\\n')]\n",
    "field_keys = [line.split(': ')[0].split(' ')[0][1:-1] for line in field_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566b9fd-7cfa-42da-a5f5-b0be69d0e85e",
   "metadata": {},
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a27bcc-cb92-481a-bf99-ba9ac9090a9c",
   "metadata": {},
   "source": [
    "custom_meta_fields = {}\n",
    "for k, v in zip(field_keys, field_descs):\n",
    "    custom_meta_fields[k] = { 'description': v }\n",
    "\n",
    "custom_meta_fields['capacity_mw']['dimension'] = 'MW'\n",
    "custom_meta_fields['latitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['longitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['commissioning_year']['dimension'] = 'year'\n",
    "custom_meta_fields['year_of_capacity_data']['dimension'] = 'year'\n",
    "custom_meta_fields['year'] = { 'description': 'year of report', 'dimension': 'year'}\n",
    "custom_meta_fields['gppd_idnr'] = { 'description': 'unique index into plants table', 'dimension': None}\n",
    "custom_meta_fields['generation_gwh'] = { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_gwh'] = { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_note'] = { 'description': 'label of the model/method used to estimate generation for the year', 'dimension': None }\n",
    "custom_meta_key_fields = 'metafields'\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'Global Power Plant Database',\n",
    "    'description': 'A comprehensive, global, open source database of power plants',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "}\n",
    "custom_meta_key = 'metaset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e1321-bfa5-49a8-975e-d88042985316",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722f0bd-eca3-4bbc-9406-26d6b09d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'aep_2020'\n",
    "custom_meta_fields = {}\n",
    "create_trino_pipeline (s3, schemaname, tablename, timestamp, shell_df, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b29ea-3636-4af6-8108-49f4d3c80cf4",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26c30d50-88f0-4f41-b1f8-1dffc87c21b1",
   "metadata": {},
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd177b-4035-43ee-a8bf-6b8500be0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6e28-686b-4540-bc9f-83b44cba3d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c7bbc-c00f-41e8-85b0-6b553e73a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'aep_2020',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b86eb-aebb-445d-b125-197b7f73a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(iam_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e01667-a1a2-4752-b756-dad449a8853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb585901-7a49-4bf2-aa6e-732ba4a8e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
