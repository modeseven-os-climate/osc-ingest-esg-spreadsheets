{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0ebfc5-d93a-4731-a1a5-319b73accd2a",
   "metadata": {},
   "source": [
    "## Load 2020 WIDE-formatted ESG data (Generic)\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Initially developed using the Royal Dutch Shell plc Sustainability Report 2020 report (Many Sheets)\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691f183-a756-45e1-9348-1156044dee52",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74dff22-a393-47e4-bcaa-3d6c992ab083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee67533-80c2-4594-8e78-c864adddf1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CO2e\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from itertools import islice\n",
    "\n",
    "import pint\n",
    "import pint_pandas\n",
    "import iam_units\n",
    "from openscm_units import unit_registry\n",
    "pint_pandas.PintType.ureg = unit_registry\n",
    "ureg = unit_registry\n",
    "ureg.define('fraction = [] = frac')\n",
    "ureg.define('percent = 1e-2 frac = pct = percentage')\n",
    "ureg.define('ppm = 1e-6 fraction')\n",
    "\n",
    "ureg.define(\"USD = [currency]\")\n",
    "ureg.define(\"EUR = nan USD\")\n",
    "ureg.define(\"JPY = nan USD\")\n",
    "ureg.define(\"MM_USD = 1000000 USD\")\n",
    "ureg.define(\"revenue = USD\")\n",
    "\n",
    "ureg.define(\"btu = Btu\")\n",
    "ureg.define(\"tBtu = T Btu\")\n",
    "ureg.define(\"boe = 5.712 GJ\")\n",
    "ureg.define(\"UEDCTM = [shell_index]\")\n",
    "\n",
    "ureg.define(\"CO2e = CO2 = CO2eq = CO2_eq\")\n",
    "ureg.define(\"HFC = [ HFC_emissions ]\")\n",
    "ureg.define(\"PFC = [ PFC_emissions ]\")\n",
    "ureg.define(\"mercury = Hg = Mercury\")\n",
    "ureg.define(\"PM10 = [ PM10_emissions ]\")\n",
    "\n",
    "ureg.define(\"production = [ output ]\")\n",
    "ureg.define(\"Index = pct = Share\")\n",
    "\n",
    "ureg.define(\"Number = dimensionless\")\n",
    "\n",
    "one_co2 = ureg(\"CO2e\")\n",
    "print(one_co2)\n",
    "\n",
    "from osc_ingest_trino import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893d0110-c719-4ae8-9fe0-248a3a4256e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.0 CO2e metric_ton/revenue"
      ],
      "text/latex": [
       "$1.0\\ \\frac{\\mathrm{CO2e} \\cdot \\mathrm{metric\\_ton}}{\\mathrm{revenue}}$"
      ],
      "text/plain": [
       "1.0 <Unit('CO2e * metric_ton / revenue')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ureg(\"tonnes CO2e/revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59df89e-d31d-4f2e-a5f2-e0d696f9e7c3",
   "metadata": {},
   "source": [
    "For spreadsheets in WIDE format, pre-process the spreadsheet as a workbook, cascading label data into 3rd-normal form row and column metadata\n",
    "\n",
    "* var_col is the label of the variable being measured (whose specificity (like CO2, CH4, NOx, etc) often affects units)\n",
    "* units_col is the column where units are stated\n",
    "* val_col:last_val_col are the column where the values are quantitatively reported\n",
    "* last_val_col+1:last_col are additional columns that are presumed to be metadata labels (such as GRI or SASB labels)\n",
    "\n",
    "We add:\n",
    "* notes_col (source worksheet-specific; could act as a kind of source table metadata)\n",
    "* topic_col (sheet-level category; if we wanted large tables, they could be named by topic)\n",
    "* category_col (to which row-level data rolls up; if we wanted small tables, they could be named by topic:category)\n",
    "* segment_col (the dimension by which row-level data is segmented)\n",
    "* units_col (if not already existing in input)\n",
    "\n",
    "Some spreadsheets use color to express a multi-level category hierarchy (such as Energy Consumption>>Business Use>>Fuel Type).  We concatenate the categories from left to right as the category for our purposes, except we split off the rightmost subcategory as the segmentation.\n",
    "\n",
    "Based on all of the above, we don't really have table-level metadata other than notes attached to sheets and generic column information.  An argument could be made that we need to allocate specifier columns for additional data we want to split out from our variables.  That could look like:\n",
    "\n",
    "* spec1_col\n",
    "* spec2_col\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a09246-078b-4ede-b891-e79fe89c8036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# var_col = 1\n",
    "\n",
    "# Magic knowledge\n",
    "# last_col = 4\n",
    "# max_hidden_col = 5\n",
    "# year_regex = r'^(20\\d\\d) Data$'\n",
    "\n",
    "ingest_columns = [ 'Variable', 'Notes', 'Topic', 'Category', 'Segmentation', 'Unit' ]\n",
    "ingest_col_offsets = dict((j,i) for i,j in enumerate(ingest_columns[1:], start=1))\n",
    "\n",
    "# In this case, Value columns are named like 2020, 2019, 2018, ... .  It is the pd.melt function that gives us an actual Value column.\n",
    "# the val_col index merely refers to the first such value row (which hopefully has tasty data)\n",
    "\n",
    "# Magic knowledge\n",
    "# val_col = units_col+1     # units_col starts as var_col+1, val_col starts as var_col+2 which is also units_col+1\n",
    "\n",
    "# If topic_row is None, set topic based on name of sheet\n",
    "# topic = topic_row = None\n",
    "# header_row = None\n",
    "\n",
    "# If init_header_row is None, find header row based on color scheme\n",
    "# init_header_row = 1\n",
    "\n",
    "class corp_report_magic:\n",
    "    def __init__(self, shortname, input_filename, ws_start, ws_end, var_col=None, units_col=None, \n",
    "                 notes_col=None, topic_row=None, topic_col=None, category_col=None, init_header_row=None, header_row_list=None,\n",
    "                 header_color=None, cat_color_dict={ None:0 }, year_regex=None, max_hidden_col=None,\n",
    "                 val_col=None, last_val_col=None):\n",
    "        self.shortname = shortname\n",
    "        self.input_filename = '/'.join([os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src')),\n",
    "                                        'osc-ingest-shell/data/external', input_filename])\n",
    "        self.ws_start = ws_start,\n",
    "        self.ws_end = ws_end,\n",
    "        self.init_topic_row = topic_row    # If topic_row is None, use the worksheet name as the topic\n",
    "        self.var_col = var_col or 1\n",
    "        self.init_units_col = units_col            # If units_col is none, we have to allocate it\n",
    "        self.init_topic_col = topic_col    # If topic_col is non-null, we get topics from this row\n",
    "        self.init_category_col = category_col\n",
    "        self.init_notes_col = notes_col\n",
    "        self.init_val_col = val_col\n",
    "        self.init_last_val_col = last_val_col\n",
    "        # val_col, last_val_col, and last_col can be derived from the spreadsheet\n",
    "        self.units_row = -1 if units_col==None else 0   # -1: Carry across only; 0: no units seen yet; > 0 row of prevailing unit\n",
    "        self.init_header_row = init_header_row\n",
    "        self.header_row_list = header_row_list if header_row_list else ([-1] * ws_start) + ([init_header_row] * (ws_end-ws_start+1))\n",
    "        self.header_row = None\n",
    "        self.header_color = header_color\n",
    "        self.cat_color_dict = cat_color_dict\n",
    "        self.year_regex = year_regex\n",
    "        # For AEP, there are several hidden columns on the first sheet we must delete\n",
    "        # to make that sheet line up with other sheets\n",
    "        self.max_hidden_col = max_hidden_col\n",
    "        \n",
    "        self.units_col = units_col\n",
    "        self.topic_col = topic_col\n",
    "        self.category_col = category_col\n",
    "        self.notes_col = notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = val_col or units_col+1 if units_col else var_col+1 if var_col else 2\n",
    "        self.last_val_col = last_val_col\n",
    "        self.last_val_row = None              # Set by preprocess (after we've identified our value columns)\n",
    "        self.last_col = None                  # Set by crop_sheet\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.wb_superscripts = None\n",
    "        self.topic_row = self.init_topic_row\n",
    "        self.units_col = self.init_units_col\n",
    "        self.topic_col = self.init_topic_col\n",
    "        self.category_col = self.init_category_col\n",
    "        self.notes_col = self.init_notes_col\n",
    "        self.segmentation_col = None\n",
    "        self.val_col = self.init_val_col or self.init_units_col+1 if self.init_units_col else self.var_col+1\n",
    "        self.last_val_col = self.init_last_val_col\n",
    "\n",
    "Shell_magic = corp_report_magic(\"Shell\", r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\", 1, 10,\n",
    "                                init_header_row=5, units_col=2)\n",
    "DPDHL_magic = corp_report_magic(\"DPDHL\", r\"DPDHL-ESG-Statbook-2020-en.xlsx\", 2, 4,\n",
    "                                topic_row=1, header_row_list=[ -1, -1, 8, 5, 4], header_color='FFBF00',\n",
    "                                cat_color_dict={ 'FF00B050':0, 'E2F0D9':1, 'D0CECE':0, 'E7E6E6':0 },\n",
    "                                units_col=2)\n",
    "Unilever_magic = corp_report_magic(\"Unilever\", r\"Unilever sustainability performance data_Climate FINAL.xlsx\", 0, 0,\n",
    "                                   topic_row=9, init_header_row=10,\n",
    "                                   cat_color_dict={'FFEBF1DE':0, 'E2F0D9':0})\n",
    "AEP_magic = corp_report_magic(\"AEP\", r\"2021-Data-Centerv1.xlsx\", 0, 3,\n",
    "                              init_header_row=1,\n",
    "                              cat_color_dict={'FF237F2E':0, 'FF40B14B':1, 'FFC6E7C8':2,\n",
    "                                              'FF757575':0, 'FFBDBDBD':1, 'FFE5E5E5':2,\n",
    "                                              'FF5FB3F9':0, 'FFB9DDFC':1, \n",
    "                                              'FFD0AF8F':0, 'FFEEDCCA':1},\n",
    "                              year_regex=r'^(20\\d\\d) Data$', max_hidden_col=5)\n",
    "Altria_magic = corp_report_magic(\"Altra\", r\"esg-tables.xlsx\", 1, 1,\n",
    "                                 init_header_row=2,\n",
    "                                 cat_color_dict={'FF9BDA44':0, 'FF92D050':1},\n",
    "                                 units_col=2)\n",
    "\n",
    "SUEZ_magic = corp_report_magic(\"SUEZ\", r\"SUEZ-FY-2020-ESG-dataset-xls-may2020.xlsx\", 1, 1,\n",
    "                               init_header_row=3,\n",
    "                               topic_col=1, category_col=2, var_col=3, units_col=4,\n",
    "                               val_col=9, last_val_col=10)\n",
    "\n",
    "filename_magic = {\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "}\n",
    "# A storage area in case we delete items from the above.\n",
    "foo = {\n",
    "    r\"esg-tables.xlsx\": Altria_magic,\n",
    "    r\"greenhouse-gas-and-energy-data-shell-sr20.xlsx\": Shell_magic,\n",
    "    r\"DPDHL-ESG-Statbook-2020-en.xlsx\": DPDHL_magic,\n",
    "    r\"Unilever sustainability performance data_Climate FINAL.xlsx\": Unilever_magic,\n",
    "    r\"2021-Data-Centerv1.xlsx\": AEP_magic,\n",
    "}\n",
    "\n",
    "crm = None\n",
    "value_vars = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da79214-e1e1-4fef-9222-fde04ec5e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_regex = re.compile(r'^((mi|bi|tri|quadri)llion|thousand|hundred)(s of)? ', flags=re.I)\n",
    "sc_xlate = {'hun':1e2, 'tho':1e3, 'mil':1e6, 'bil':1e9, 'tri':1e12, 'qua':1e15}\n",
    "\n",
    "def find_units(var):\n",
    "    scale = 1.0\n",
    "    if var in ['%', 'pct', 'percent']:\n",
    "        return 'percent'\n",
    "    if '-based' in var or 'KPI' in var:\n",
    "        return None\n",
    "    if 'Total no.' in var:\n",
    "        var = var.replace('Total no.', 'Number')\n",
    "    elif 'No.' in var:\n",
    "        var = var.replace('No.', 'Number')\n",
    "    if ' of production' in var:\n",
    "        var = var.replace(' of production', '')\n",
    "    var = var.replace ('Net MWh', 'MWh')\n",
    "    var = var.replace ('trillion (10^12)', 'trillion')\n",
    "    var = var.replace ('m3', 'kl')\n",
    "    var = var.replace ('KWh', 'kWh')\n",
    "    var = var.replace ('Index points', 'Number')\n",
    "    if var in ureg:\n",
    "        return f'{ureg(var).u:~}'\n",
    "    if var.lower() in ureg:\n",
    "        return f'{ureg(var.lower()).u:~}'\n",
    "    m = re.search(r'(((metric)|(short)) t)on', var, flags=re.I)\n",
    "    if m:\n",
    "        var = '_'.join(var.lower().split(' '))\n",
    "        if var in ureg:\n",
    "            return f'{ureg(var).u:~}'\n",
    "    m = re.search(scale_regex, var)\n",
    "    if m:\n",
    "        var = ' '.join([var[0:m.start(0)],var[m.end(0):]]).strip()\n",
    "        if var in ureg:\n",
    "            units = sc_xlate[m.group(1)[0:3].lower()] * ureg(var)\n",
    "            units = units.to_compact()\n",
    "            if units.m - 1.0 < 0.00001:\n",
    "                # Address roundoff problems such as giga = 1.00000000000002 x 10^9\n",
    "                return f'{units.u:~}'\n",
    "            print(f'units do not reduce: {units}')\n",
    "    print(f'find units: nothing found for {var}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007e3a1b-a0a6-410a-8b6b-f921c5be2261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_keywords = { 'footprint':['intensity'],\n",
    "                   'emissions':['scope 1', 'scope 2', 'scope 3', 'ghg', 'intensity'],\n",
    "                   'energy':['consum', 'generat', 'renewable', 'intensity'],\n",
    "                   'water':['consum', 'discharge', 'withdraw', 'intensity'],\n",
    "                   'waste':['landfill', 'incinerate', 'compost', 'recycle', 'reuse', 'intensity'],\n",
    "                   'other':[]}\n",
    "\n",
    "topic_cell = None\n",
    "category_cell = None\n",
    "segmentation_stack = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9da422c-ea94-4e24-87d3-2a00d7fd8b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_topic(ws, row):\n",
    "    \"\"\"\n",
    "    Topics are major headers.  If such headers also have units defined, they are also categories.\n",
    "    If such headers also have values defined, they are also processed as a variable.\n",
    "    \"\"\"\n",
    "    global topic_cell, category_cell\n",
    "    \n",
    "    if row==None:\n",
    "        # We have to put the topic in header_row+1 because header_row is column info, not data, for the dataframe\n",
    "        row = crm.header_row+1\n",
    "        topic_cell = ws.cell(row, crm.topic_col)\n",
    "        topic_cell.value = ws.title.lower()\n",
    "        print(f'process_topic {row}: setting topic from title {topic_cell.value}')\n",
    "\n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    if cell.value:\n",
    "        var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "    else:\n",
    "        var_text = None\n",
    "    \n",
    "    if var_text:\n",
    "        topic_cell = ws.cell(row, crm.topic_col)\n",
    "        if topic_cell.value!=ws.title.lower():\n",
    "            # Let's assume topic text is not parenthetical, but titular\n",
    "            var_text = re.sub(r'\\(.+\\)', '', var_text)\n",
    "            var_words = var_text.split(' ')\n",
    "            for word in var_words:\n",
    "                if topic_cell.value and word.lower() == topic_cell.value:\n",
    "                    # Not a new topic\n",
    "                    break\n",
    "                if word.lower() in topic_keywords.keys():\n",
    "                    print(f'process_topic {row}: setting topic {word}')\n",
    "                    topic_cell.value = word.lower()\n",
    "\n",
    "            if topic_cell.value==None:\n",
    "                print(f'worksheet {ws.title}: unknown topic {var_text}')\n",
    "                topic_cell.value = ws.title.lower()\n",
    "                # topic_keywords[ws.title.lower()] = []\n",
    "\n",
    "        # Try to extract units from Variable description\n",
    "        if ws.cell(row, crm.units_col).value==None:\n",
    "            p_exprs = re.findall(r'\\((.+)\\)', ws.cell(row, crm.var_col).value)\n",
    "            for p in p_exprs:\n",
    "                if find_units(p):\n",
    "                    print(f'process_topic {row}: setting units from var: {p}')\n",
    "                    ws.cell(row, crm.units_col).value = p\n",
    "                    break\n",
    "        # Here we don't look for species_unit; mistake???\n",
    "        \n",
    "        # If we definitely have units, set the category, which will also process the variable (if needed)\n",
    "        if ws.cell(row, crm.units_col).value:\n",
    "            print(f'process_topic {row}: setting category {var_text}')\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            ws.cell(row, crm.category_col).value = var_text\n",
    "\n",
    "            print(f'process_topic {row}: setting units {ws.cell(row, crm.units_col).value}')\n",
    "            units = find_units (ws.cell(row, crm.units_col).value)\n",
    "            if units == None:\n",
    "                error(f'unknown units {ws.cell(row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = units\n",
    "            \n",
    "            row = process_categories (ws, row)\n",
    "    else:\n",
    "        print(f'process_topic {row}: no var text')\n",
    "    if row < 0 or row >= crm.last_val_row:\n",
    "        return crm.last_val_row\n",
    "    return row+1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e918cc0-f5ee-47da-af22-9f17a403c266",
   "metadata": {},
   "source": [
    "BORDER_NONE = None\n",
    "BORDER_DASHDOT = 'dashDot'\n",
    "BORDER_DASHDOTDOT = 'dashDotDot'\n",
    "BORDER_DASHED = 'dashed'\n",
    "BORDER_DOTTED = 'dotted'\n",
    "BORDER_DOUBLE = 'double'\n",
    "BORDER_HAIR = 'hair'\n",
    "BORDER_MEDIUM = 'medium'\n",
    "BORDER_MEDIUMDASHDOT = 'mediumDashDot'\n",
    "BORDER_MEDIUMDASHDOTDOT = 'mediumDashDotDot'\n",
    "BORDER_MEDIUMDASHED = 'mediumDashed'\n",
    "BORDER_SLANTDASHDOT = 'slantDashDot'\n",
    "BORDER_THICK = 'thick'\n",
    "BORDER_THIN = 'thin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659ec878-2e22-4330-b1da-e6c0d8fee782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_as_sub(cell1, cell2):\n",
    "    color1 = cell2rgb(cell1)\n",
    "    color2 = cell2rgb(cell2)\n",
    "    if color1 in crm.cat_color_dict:\n",
    "        if color2 in crm.cat_color_dict:\n",
    "            if crm.cat_color_dict[color1] < crm.cat_color_dict[color2]:\n",
    "                return True\n",
    "            if crm.cat_color_dict[color1] > crm.cat_color_dict[color2]:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    elif color2 in crm.cat_color_dict:\n",
    "        return False\n",
    "    \n",
    "    sub_score = 0\n",
    "    if cell1.font.b and cell2.font.b==False:\n",
    "        # print('+bold')\n",
    "        sub_score += 1\n",
    "    if cell1.font.b==False and cell2.font.b:\n",
    "        # print('-bold')\n",
    "        sub_score -= 1\n",
    "    if cell1.font.u and cell2.font.u==False:\n",
    "        # print('+underline')\n",
    "        sub_score += 1\n",
    "    if cell1.font.u==False and cell2.font.u:\n",
    "        # print('-underline')\n",
    "        sub_score -= 1\n",
    "    if cell1.alignment.indent < cell2.alignment.indent:\n",
    "        # print('+indent')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.indent > cell2.alignment.indent:\n",
    "        # print('-indent')\n",
    "        sub_score -= 1\n",
    "    if cell1.font.sz < cell2.font.sz:\n",
    "        # print('+size')\n",
    "        sub_score += 1\n",
    "    elif cell1.font.sz > cell2.font.sz:\n",
    "        # print('-size')\n",
    "        sub_score -= 1\n",
    "    if cell1.alignment.horizontal == 'left' and cell2.alignment.horizontal == 'right':\n",
    "        # print('+halign')\n",
    "        sub_score += 1\n",
    "    elif cell1.alignment.horizontal == 'right' and cell2.alignment.horizontal == 'left':\n",
    "        # print('-halign')\n",
    "        sub_score -= 1\n",
    "    # print(f'sub_score = {sub_score}')\n",
    "    if sub_score > 0:\n",
    "        return True\n",
    "    if sub_score < 0:\n",
    "        return False\n",
    "    if sub_score == 0:\n",
    "        return None\n",
    "\n",
    "def process_categories(ws, row):\n",
    "    \"\"\"\n",
    "    Categories have units\n",
    "    \"\"\"\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        if cell.value:\n",
    "            var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "        else:\n",
    "            # Some categories are declared across multiple lines, with units by themselves\n",
    "            var_text = None\n",
    "        \n",
    "        # If we're already processing this row as a topic (and being called from that context), don't recurse\n",
    "        if var_text and topic_cell.row < row:\n",
    "            color = cell2rgb(ws.cell(row,1))\n",
    "            if color and color in crm.cat_color_dict:\n",
    "                # Hack because Unilever uses colors wrongly, but punctuation saves the day\n",
    "                if crm.cat_color_dict[color]==0 and var_text[-1]!=':':\n",
    "                    # Register that we have a new topic\n",
    "                    topic_cell = ws.cell(row, crm.topic_col)\n",
    "                    topic_cell.value = var_text\n",
    "                    print(f'process_categories: new topic set at row {row}: {var_text} (color = {color})')\n",
    "                    return process_topic(ws, row)\n",
    "                if crm.cat_color_dict[color]==1:\n",
    "                    category_cell = ws.cell(row, crm.category_col)\n",
    "                    category_cell.value = var_text\n",
    "                    print(f'process_categories: new category set at row {row}: {var_text} (color = {color})')\n",
    "            elif color:\n",
    "                if color!='00000000':\n",
    "                    print(f'process_categories: unknown color {color} at row {row}: {var_text}')\n",
    "            elif None in crm.cat_color_dict:\n",
    "                category_cell = ws.cell(row, crm.category_col)\n",
    "                category_cell.value = var_text\n",
    "                print(f'process_categories: new category set at row {row}: {var_text}')\n",
    "\n",
    "        # Try to extract units from Variable description\n",
    "        if ws.cell(row, crm.units_col).value==None and var_text:\n",
    "            p_exprs = re.findall(r'\\((.+)\\)', ws.cell(row, crm.var_col).value)\n",
    "            for p in p_exprs:\n",
    "                if 'scope' in p.lower() or 'category' in p.lower():\n",
    "                    continue\n",
    "                if find_units(p):\n",
    "                    print(f'process_categories {row}: setting units from var: {p}')\n",
    "                    ws.cell(row, crm.units_col).value = p\n",
    "                    break\n",
    "        \n",
    "        # Apply our best guess for units in case we need to propagate in segmentation\n",
    "        var_units = ws.cell(row, crm.units_col).value\n",
    "        var_species = ''\n",
    "        if var_units:\n",
    "            var_units = find_units (var_units)\n",
    "            m = re.search('r\\((.+)\\)', var_text)\n",
    "            if m:\n",
    "                var_species = m.group(1)\n",
    "                species_units = find_units(' '.join([var_units, var_species]))\n",
    "                if species_units:\n",
    "                    units = species_units\n",
    "                    var_text = ' '.join([var_text[0:m.start(1)], var_text[m.end(1)+1:]]).replace('  ', ' ')\n",
    "            else:\n",
    "                units = var_units\n",
    "        elif category_cell:\n",
    "            units = ws.cell(category_cell.row, crm.units_col).value\n",
    "        else:\n",
    "            units = None\n",
    "        ws.cell(row, crm.units_col).value = units\n",
    "        \n",
    "        total_of = ''\n",
    "        if var_text and 'total' in var_text.lower():\n",
    "            c1, c2 = re.split(r'\\s*totals?\\s*', var_text, flags=re.I)\n",
    "            if c2.strip()=='':\n",
    "                total_of = c2 = c1\n",
    "                c1 = var_text\n",
    "            elif c1.strip()=='':\n",
    "                c1 = var_text\n",
    "                total_of = c2\n",
    "            else:\n",
    "                total_of = var_text\n",
    "                # we have c1:c2\n",
    "        \n",
    "        segment_by = ''\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            if var_text and x in var_text:\n",
    "                segment_by = x\n",
    "                break\n",
    "\n",
    "        if formatted_as_sub(ws.cell(row, crm.var_col), ws.cell(row+1, crm.var_col)):\n",
    "            if segment_by:\n",
    "                c1, c2 = var_text.split(segment_by, 1)\n",
    "            elif not total_of:\n",
    "                c1 = var_text\n",
    "                c2 = '(anon)'\n",
    "            category_cell.value = c1\n",
    "            if segment_by or total_of:\n",
    "                print(f'process_categories {row}: segmenting {c1}::{c2}')\n",
    "                segmentation_stack = [ ws.cell(row, crm.segmentation_col) ]\n",
    "                segmentation_stack[-1].value = c2\n",
    "                row = process_var(ws, row)\n",
    "                row = process_segmentation (ws, row)\n",
    "                if segmentation_stack != []:\n",
    "                    print(f'process_categories: segmentation_stack = {segmentation_stack}')\n",
    "                if row < 0:\n",
    "                    return row\n",
    "                continue\n",
    "        segmentation_stack = []\n",
    "        # print(f'process_category {row}: processing variable')\n",
    "        row = process_var(ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c35c9d9-d4ed-4b71-a5cb-95a49ce2ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segmentation(ws, row):\n",
    "    \"\"\"Process rows starting at ROW as a part of a segmentation.  We push and recurse if we see\n",
    "    a new level of indentation.  We pop and return if we see an outdent.\n",
    "    \"\"\"\n",
    "    global segmentation_stack\n",
    "    \n",
    "    seg_start_cell = ws.cell(row, crm.var_col)\n",
    "    seg_start_units = ws.cell(row, crm.units_col).value\n",
    "    while row < crm.last_val_row:\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        if cell.value==None:\n",
    "            if ws.cell(row, crm.units_col).value and ws.cell(row+1, crm.units_col).value==None:\n",
    "                ws.cell(row+1, crm.units_col).value = ws.cell(row, crm.units_col).value\n",
    "                row = row+1\n",
    "                # Don't start a segment with an unlabeled cell\n",
    "                seg_start_cell = ws.cell(row, crm.var_col)\n",
    "                continue\n",
    "            else:\n",
    "                print(f'unhandled units at row {row}: {ws.cell(row+1, crm.units_col).value}; {ws.cell(row, crm.units_col).value}')\n",
    "\n",
    "        if cell==seg_start_cell:\n",
    "            # Handle easy case\n",
    "            row = process_var (ws, row)\n",
    "        else:\n",
    "            new_seg_start = formatted_as_sub(seg_start_cell, cell)\n",
    "            if new_seg_start:\n",
    "                if ws.cell(row, crm.units_col).value==None:\n",
    "                    ws.cell(row, crm.units_col).value = seg_start_units\n",
    "                # There could be many rows at the same level as SEG_START_CELL before a new segmentation is seen\n",
    "                # The label we care about is the one immediately preceding, not the first one with that indentation\n",
    "                segmentation_stack.append(ws.cell(row-1, crm.var_col))\n",
    "                row = process_segmentation(ws, row)\n",
    "            elif new_seg_start==False:\n",
    "                segmentation_stack.pop()\n",
    "                print(f'pop at row {row}: segmentation_stack now {segmentation_stack}')\n",
    "                return row\n",
    "            else:\n",
    "                if ws.cell(row, crm.units_col).value==None and seg_start_units:\n",
    "                    ws.cell(row, crm.units_col).value = seg_start_units\n",
    "                row = process_var (ws, row)\n",
    "        if row < 0:\n",
    "            return row\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    if row == crm.last_val_row:\n",
    "        process_var (ws, row)\n",
    "        if segmentation_stack:\n",
    "            print(f'process_segmentation: stack at end = {segmentation_stack}')\n",
    "            segmentation_stack = []\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66de52b-8aae-443a-938a-21f1f34987ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_var(ws, row):\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    cell = ws.cell(row, crm.var_col)\n",
    "    \n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    if cell.value:\n",
    "        var_text = cell.value.split('\\n')[0]    # notes have been stripped out\n",
    "        var_text = var_text.replace('(%)', '(pct)')\n",
    "        m = re.search(r'^(.*) \\((.*?)\\)', var_text)\n",
    "    else:\n",
    "        var_text = None\n",
    "        m = None\n",
    "    if m and '-based' not in m.group(2) and 'scope' not in m.group(2).lower() and 'category' not in m.group(2).lower():\n",
    "        species_units = find_units (m.group(2))\n",
    "        if species_units:\n",
    "            # print(f'process_var {row}: found species or units in {var_text}')\n",
    "            units = ws.cell(row, crm.units_col).value\n",
    "            if units and units != species_units:\n",
    "                species_units = find_units(' '.join([units, m.group(2)]))\n",
    "                if species_units:\n",
    "                    var_text = m.group(1).rstrip()\n",
    "                    units = species_units\n",
    "                else:\n",
    "                    print(f'??? Not overriding {units} with {m.group(2)}')\n",
    "                    # units = ws.cell(row, crm.units_col).value\n",
    "            else:\n",
    "                units = species_units\n",
    "                if ' ' not in m.group(1) and m.group(1) in ureg:\n",
    "                    species_units = find_units(' '.join([units, m.group(1)]))\n",
    "                    if species_units:\n",
    "                        var_text = m.group(1).rstrip()\n",
    "                        units = species_units\n",
    "                # print(f'Inferring/composing units: {units}')\n",
    "            if units != ws.cell(category_cell.row, crm.units_col).value:\n",
    "                # print(f'changing units from category: {ws.cell(category_cell.row, crm.units_col).value} to {units}')\n",
    "                ws.cell(row, crm.units_col).value = units\n",
    "        elif m.group(2).lower() and topic_cell.value.lower() in topic_keywords and m.group(2).lower() in topic_keywords[topic_cell.value.lower()]:\n",
    "            # Scope 1 is actually a sneaky segmentation\n",
    "            category_cell = ws.cell(row, crm.category_col)\n",
    "            category_cell.value = m.group(2)\n",
    "        else:\n",
    "            print(f'process_var {row}: unhandled ( {m.group(2)} )')\n",
    "    else:\n",
    "        if ws.cell(row, crm.units_col).value == None:\n",
    "            # print(f'process_var {row}: propagating units {ws.cell(category_cell.row, crm.units_col).value}')\n",
    "            ws.cell(row, crm.units_col).value = ws.cell(category_cell.row, crm.units_col).value\n",
    "        else:\n",
    "            # print(f'process_var {row}: using units {ws.cell(row, crm.units_col).value}')\n",
    "            pass\n",
    "    ws.cell(row, crm.topic_col).value = topic_cell.value\n",
    "    ws.cell(row, crm.category_col).value = category_cell.value\n",
    "    if segmentation_stack != []:\n",
    "        ws.cell(row, crm.segmentation_col).value = '::'.join(s.value for s in segmentation_stack)\n",
    "    if row < crm.last_val_row:\n",
    "        return row+1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4b839c-1347-4459-ba03-64510dc67ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? The header row color is going to be spreadsheet-specific.  This is what DPDHL gives us.\n",
    "\n",
    "import cell2rgb\n",
    "from cell2rgb import cell2rgb\n",
    "\n",
    "def find_header_row(wb, ws):\n",
    "    # If we haven't found the header by max_row-1, we'll never find it...\n",
    "    for row in range(1, ws.max_row):\n",
    "        color = cell2rgb(ws.cell(row,1))\n",
    "        if color == crm.header_color:\n",
    "            return row\n",
    "        print(f'find_header_row: color = {color}')\n",
    "    error('No header found')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2927f878-eade-47b7-81b7-dea1b10de622",
   "metadata": {},
   "source": [
    "# The role of this function is to capture and distrbute data attributes that can be inferred/applied to subsequent rows\n",
    "def split_cell(c):\n",
    "    notes = ''\n",
    "\n",
    "    # Deal with None\n",
    "    if c.value:\n",
    "        h = str(c.value)\n",
    "    else:\n",
    "        h = ''\n",
    "\n",
    "    # Don't let 'Scope 1' look like a note\n",
    "    m = re.search(r'[^ ](\\d+)$', h)\n",
    "    if m:\n",
    "        notes = m.group(1)\n",
    "        h = h[0:m.start(1)]\n",
    "    else:\n",
    "        m = re.search(r'\\s*\\[[A-Z]\\](\\s?\\[[A-Z]\\])*', h)\n",
    "        if m:\n",
    "            notes = m.group(0)\n",
    "            h = h[0:m.start(0)].strip()\n",
    "\n",
    "    # If we have a ':' in the cell, treat that as a major connector that overrides splitting on per/by/of\n",
    "    if ':' not in h:\n",
    "        # If the variable expresses a segmentation, pass that back accordingly\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            sub_h_arr = h.split(x, 1)\n",
    "            if len(sub_h_arr)>1:\n",
    "                return notes, sub_h_arr[0], sub_h_arr[1]\n",
    "\n",
    "    # Treat X (Y) as 'Category X Segmentation Y'\n",
    "    m = re.search(r'^(.*) \\((.*?)\\)', h)\n",
    "    if m:\n",
    "        return notes, m.group(1), m.group(2)\n",
    "    return notes, h, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43609bc3-b405-417c-95fd-c07652e290f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as eTree\n",
    "\n",
    "# We pre-process the structure of the worksheet so that it can be trivially loaded into a dataframe for further reshaping.\n",
    "\n",
    "# Stash notes for each worksheet here.  These are *per worksheet*\n",
    "# ??? In the case of DPDHL, there's a Comment field we don't track, which means we miss a stated target\n",
    "ws_notes = {}\n",
    "\n",
    "def preprocess(wb, ws):\n",
    "    global crm, value_vars\n",
    "    global topic_cell, category_cell, segmentation_stack\n",
    "    \n",
    "    cell_notes_text = []\n",
    "    cell_notes_cells = []\n",
    "    \n",
    "    def crop_sheet(ws):\n",
    "        global crm\n",
    "        # Frist, set max_row/max_column based on actually active cells, not cells with random spaces or empty strings\n",
    "        this_max_row = 1\n",
    "        this_max_col = 1\n",
    "        for row in range(1,ws.max_row+1):\n",
    "            row_max_col = None\n",
    "            for col in range(1,ws.max_column+1):\n",
    "                cell = ws.cell(row,col)\n",
    "                if cell.value==None:\n",
    "                    continue\n",
    "                if type(cell.value)==str and cell.value.strip()=='':\n",
    "                    cell.value = None\n",
    "                    continue\n",
    "                if col > this_max_col:\n",
    "                    this_max_col = col\n",
    "                row_max_col = col\n",
    "            if row_max_col:\n",
    "                this_max_row = row\n",
    "        print('crop_sheet')\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        ws.delete_rows(this_max_row+1,ws.max_row)\n",
    "        ws.delete_cols(this_max_col+1,ws.max_column)\n",
    "        print('{} x {}'.format(ws.max_row, ws.max_column))\n",
    "        crm.last_col = ws.max_column\n",
    "        \n",
    "    def preprocess_notes():\n",
    "        global crm\n",
    "        z = zipfile.ZipFile(crm.input_filename)\n",
    "\n",
    "        if crm.wb_superscripts==None:\n",
    "            with z.open('xl/sharedStrings.xml') as fp:\n",
    "                ss_xml = etree.fromstring(fp.read())\n",
    "            # get the namespaces                                                                                                                                                                                                                                             \n",
    "            ssns = ss_xml.nsmap\n",
    "            if None in ssns:\n",
    "                ssns['none'] = ssns.pop(None)\n",
    "            crm.text_list = ss_xml.xpath('//none:si', namespaces=ssns)\n",
    "            # All shared strings across all sheets with superscripts                                                                                                                                                                                                                           \n",
    "            crm.wb_superscripts = [s for s in range(len(crm.text_list))\n",
    "                                   if 'superscript' in eTree.tostring(crm.text_list[s], encoding='unicode')]\n",
    "        \n",
    "        with z.open(f'xl/worksheets/sheet{wb.worksheets.index(ws)+1}.xml') as fp:\n",
    "            ws_xml = etree.fromstring(fp.read())\n",
    "        z.close()\n",
    "\n",
    "        # get the namespaces                                                                                                                                                                                                                                             \n",
    "        wsns = ws_xml.nsmap\n",
    "        if None in wsns:\n",
    "            wsns['none'] = wsns.pop(None)\n",
    "        cell_list = ws_xml.xpath('//none:c', namespaces=wsns)\n",
    "\n",
    "        # Dictionary of cells:shared strings with superscripts within this sheet's cells                                                                                                                                                                                                          \n",
    "        sheet_ss_dict = {c:s for c in range(len(cell_list))\n",
    "                         for s in crm.wb_superscripts if f' t=\"s\"><ns0:v>{s}</ns0:v></ns0:c>' in eTree.tostring(cell_list[c],\n",
    "                                                                                                                 encoding='unicode')}\n",
    "\n",
    "        cell_notes_text = []\n",
    "        cell_notes_cells = []\n",
    "        for c, s in sheet_ss_dict.items():\n",
    "            cell_name = eTree.tostring(cell_list[c], encoding='unicode').split(' ')[2].split('=')[1][1:-1]\n",
    "            cell_text_xml = eTree.tostring(crm.text_list[s],encoding='unicode')\n",
    "            ss_bool = ['superscript' in x for x in cell_text_xml.split('<ns0:r>')[1:]]\n",
    "            cell_text_parts = [re.split('<ns0:t.*?>', x)[-1].split('</ns0:t>')[0]\n",
    "                  for x in cell_text_xml.split('<ns0:r>')[1:]]\n",
    "            for x in range(len(ss_bool)-1):\n",
    "                if ss_bool[x]==False and ss_bool[x+1]==True:\n",
    "                    cell_notes_text.append(cell_text_parts[x+1])\n",
    "                    cell_notes_cells.append(cell_name)\n",
    "                    ws[cell_name].value = ws[cell_name].value.replace(cell_text_parts[x]+cell_text_parts[x+1],\n",
    "                                                                      cell_text_parts[x])\n",
    "    \n",
    "    crm.preprocess()\n",
    "    preprocess_notes()\n",
    "    print(cell_notes_text)\n",
    "    print(cell_notes_cells)\n",
    "    \n",
    "    topic_cell = None\n",
    "    category_cell = None\n",
    "    segmentation_stack = []\n",
    "\n",
    "    # Remove merged cells\n",
    "    mergedRanges=ws.merged_cells.ranges\n",
    "    while mergedRanges:\n",
    "        for entry in mergedRanges:\n",
    "            ws.unmerge_cells(str(entry))\n",
    "\n",
    "    if crm.max_hidden_col and wb.worksheets[0]==ws:\n",
    "        ws.delete_cols(1,crm.max_hidden_col)\n",
    "    crop_sheet(ws)\n",
    "\n",
    "    if crm.init_header_row:\n",
    "        crm.header_row = crm.init_header_row\n",
    "    else:\n",
    "        crm.header_row = find_header_row (wb, ws)\n",
    "    \n",
    "    # Reset this for each worksheet\n",
    "    if crm.units_row >= 0:\n",
    "        crm.units_row = 0\n",
    "\n",
    "    col = crm.val_col\n",
    "    last_val_col = crm.last_val_col or col\n",
    "    while crm.last_val_col==None or col<=crm.last_val_col:\n",
    "        # ??? Deal with note in header value (such as '2019(b)' or, God forbit '20197' where the superscripted 7 just sits like it's part of the number)\n",
    "        if crm.year_regex:\n",
    "            maybe_year = re.sub(crm.year_regex, r'\\1', str(ws.cell(crm.header_row, col).value))\n",
    "        else:\n",
    "            maybe_year = str(ws.cell(crm.header_row, col).value)\n",
    "        if len(maybe_year)>=4 and maybe_year[0:2]=='20' and maybe_year[2].isdigit() and maybe_year[3].isdigit():\n",
    "            ws.cell(crm.header_row, col).value = maybe_year[0:4]\n",
    "            last_val_col = col\n",
    "        elif crm.last_val_col==None:\n",
    "            crm.last_val_col = last_val_col\n",
    "            break\n",
    "        col = col+1\n",
    "    value_vars = [ None ] * (crm.last_val_col-crm.val_col+1)\n",
    "    for col in range(crm.val_col, crm.last_val_col+1):\n",
    "        value_vars[col-crm.val_col] = ws.cell(crm.header_row, col).value\n",
    "    print(value_vars)\n",
    "    \n",
    "    # Make space for TOPIC : CATEGORY : SEGMENTATION triple.\n",
    "    # This triple could very well become an index into a data framework (such as SASB, TCFD, etc)\n",
    "    new_column_count = (len(ingest_columns)-1\n",
    "                        -int(crm.notes_col!=None)\n",
    "                        -int(crm.topic_col!=None)\n",
    "                        -int(crm.category_col!=None)\n",
    "                        -int(crm.units_col!=None))\n",
    "    ws.insert_cols(crm.last_val_col+1,amount=new_column_count)\n",
    "    if crm.notes_col==None:\n",
    "        crm.notes_col = crm.last_val_col+ingest_col_offsets['Notes']\n",
    "    ws.cell(crm.header_row,crm.notes_col).value = 'Notes'\n",
    "    if crm.topic_col==None:\n",
    "        crm.topic_col = crm.last_val_col+ingest_col_offsets['Topic']\n",
    "    ws.cell(crm.header_row,crm.topic_col).value = 'Topic'\n",
    "    if crm.category_col==None:\n",
    "        crm.category_col = crm.last_val_col+ingest_col_offsets['Category']\n",
    "    ws.cell(crm.header_row,crm.category_col).value = 'Category'\n",
    "    crm.segmentation_col = crm.last_val_col+ingest_col_offsets['Segmentation']\n",
    "    ws.cell(crm.header_row,crm.segmentation_col).value = 'Segmentation'\n",
    "    if crm.units_col==None:\n",
    "        crm.units_col = crm.last_val_col+ingest_col_offsets['Unit']\n",
    "    ws.cell(crm.header_row,crm.units_col).value = 'Unit'\n",
    "    ws.cell(crm.header_row,crm.var_col).value = 'Variable'\n",
    "        \n",
    "    crm.last_col = crm.last_col + new_column_count\n",
    "\n",
    "    # Find last row of actual values so we can process notes at the end\n",
    "    for row in range(ws.max_row, 0, -1):\n",
    "        if any([True for col in range(crm.val_col, crm.last_val_col+1) if ws.cell(row, col).value]):\n",
    "            crm.last_val_row = row\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34019c7e-8467-4b37-b0e2-96b13bacd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(wb, ws):\n",
    "    \n",
    "    # Intended for Shell notes\n",
    "    def save_ws_notes(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        note_label, note_text = note.split(' ', 1)\n",
    "        ws_notes[ws.title][note_label] = note_text.strip()\n",
    "    \n",
    "    # Intended for DPDHL notes\n",
    "    def save_ws_notes2(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        notes = re.split(r' (\\d+)\\)\\s+', note)\n",
    "        print('NOTES')\n",
    "        print(notes)\n",
    "        print('END NOTES')\n",
    "        ws_notes[ws.title]['0'] = notes[0]\n",
    "        for i in range(int(len(notes)/2)):\n",
    "            ws_notes[ws.title][notes[1+2*i]] = notes[2+2*i].strip()\n",
    "    \n",
    "    # Used for Unilever\n",
    "    def finish_notes(row):\n",
    "        print('finish_notes @ {}'.format(row))\n",
    "\n",
    "    for row in range(crm.last_val_row+1, ws.max_row+1):\n",
    "        cell = ws.cell(row, crm.var_col)\n",
    "        # Find either bracketed note or note that begins with possible superscript\n",
    "        if cell.value==None:\n",
    "            continue\n",
    "        if cell.value[0]=='[':\n",
    "            save_ws_notes(ws, cell.value)\n",
    "            continue\n",
    "        if re.search(r'^[^(]*\\d[)]', str(cell.value)):\n",
    "            save_ws_notes2(ws, cell.value)\n",
    "        if re.search(r'notes', str(cell.value), flags=re.I):\n",
    "            finish_notes(row)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7aa4190-7ea4-4f5b-afab-cc055c0d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a nicely formatted workbook, do the rest of our work (including writing to Trino) using dataframes\n",
    "\n",
    "# IPIECA, SASB, and GRI columns all feed metadata\n",
    "\n",
    "def ws_to_df(wb, i):\n",
    "    data = islice(wb.worksheets[i].values, crm.header_row_list[i]-1, None)\n",
    "    cols = list(next(data))\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 0, None) for r in data)\n",
    "    cols[crm.units_col-1] = 'Unit'            # Already set by Shell; DF indexes are XLSX-1\n",
    "    df = pd.DataFrame(data, columns=cols) # we don't pass in an index here\n",
    "\n",
    "    # Remove null columns\n",
    "    df = df[[c for c in df.columns if c!= None]]\n",
    "    \n",
    "    # For now, do not remove rows lacking units.  Those are basically where Notes are stored (for better or worse).\n",
    "    # print('rows lacking proper Units')\n",
    "    # display(df[df['Unit'].isnull()])\n",
    "    df = df.loc[df.Unit.notna() | df.Category.isna()]\n",
    "\n",
    "    # Clear out data that is n/a, n/c (not collected), n/d (not disclosed)\n",
    "    df[df['Unit'].notna()].replace(to_replace='^n/[acd]$', value='', regex=True, inplace=True)\n",
    "    \n",
    "    # Change numerical years to strings to make pandas indexing behave\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    # Drop completely empty rows\n",
    "    # df.dropna(how='all', axis=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63095f7-55e2-4eef-b1d4-379fcf9ebaba",
   "metadata": {},
   "source": [
    "Write out polymorphic dataframe in LONG format.  This follows tidy data model, with one variable observation per row.  \n",
    "Polymorphic means that Units/dimensions of each row are specified, but not necessarily the same row to row.  \n",
    "Aggregation functions must be careful that selection criteria does not mix up incompatible unit types and/or observation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ffed37-1d28-4038-8390-d2f5ec91f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = None\n",
    "ws = None\n",
    "melted_df = None\n",
    "\n",
    "def ingest_filename(filename):\n",
    "    global melted_df\n",
    "    global topic_cell\n",
    "    global crm, wb, ws\n",
    "    \n",
    "    crm = filename_magic[filename]\n",
    "\n",
    "    wb = load_workbook(crm.input_filename, data_only=True)\n",
    "\n",
    "    # For a label like \"Scope 1 emissions by country\" return ['', 'Scope 1 emissions', 'country']\n",
    "    # For a label like \"Direct GHG emissions (Scope 1) [A] [B] [C] [D]\" return ['[A] [B] [C] [D]', 'Direct GHG emissions (Scope 1) ', '']\n",
    "\n",
    "    long_fmt_filename = ''\n",
    "    wide_fmt_filename = ''\n",
    "\n",
    "    for i in range(crm.ws_start[0], crm.ws_end[0]+1):\n",
    "        ws = wb.worksheets[i]\n",
    "        ws_notes = {}\n",
    "        preprocess(wb, ws)\n",
    "        \n",
    "        row = process_topic(ws, crm.topic_row)\n",
    "\n",
    "        while row < crm.last_val_row:\n",
    "            if row == crm.header_row_list[i]:\n",
    "                row = row+1\n",
    "                continue\n",
    "            color = cell2rgb(ws.cell(row,1))\n",
    "            if color and color in crm.cat_color_dict:\n",
    "                if crm.cat_color_dict[color]==0:\n",
    "                    # Register that we have a new topic\n",
    "                    topic_cell = ws.cell(row, crm.topic_col)\n",
    "                    topic_cell.value = ws.cell(row, crm.var_col).value\n",
    "                    print(f'new topic set at row {row}: {topic_cell.value} (color = {color})')\n",
    "                    row = process_topic(ws, row)\n",
    "                else:\n",
    "                    row = process_categories(ws, row)\n",
    "                    if row < 0:\n",
    "                        break\n",
    "            else:\n",
    "                print(f'ingest_file: processing {row}')\n",
    "                row = process_topic(ws, row)\n",
    "    \n",
    "        # process_topic (wb, ws, header_row_list[i])\n",
    "        # preprocess2(wb, ws)\n",
    "        postprocess(wb, ws)\n",
    "        # What to do with ws_notes???\n",
    "        df = ws_to_df(wb, i)\n",
    "        df.replace('',pd.NA,inplace=True)\n",
    "        print(f'wb({i}) dataframe')\n",
    "        # display(df.loc[0:min(len(df),45)])\n",
    "        melted_df = pd.melt(df, id_vars=ingest_columns, var_name='Year', value_name='Value', value_vars=value_vars)\n",
    "        melted_df.dropna(subset=['Value'],inplace=True)\n",
    "        melted_df = melted_df.astype({'Year': 'int'})\n",
    "\n",
    "        if i==crm.ws_start[0]:\n",
    "            report_year = max(df.columns[crm.val_col-1:crm.last_val_col])\n",
    "            long_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'LONG.xlsx'])\n",
    "            writer_long = pd.ExcelWriter(long_fmt_filename)\n",
    "            wide_fmt_filename = ''.join([os.environ.get('PWD', '/opt/app-root/src'), '/osc-ingest-shell/data/interim/',\n",
    "                                         crm.shortname, '_', report_year, '_', 'WIDE.xlsx'])\n",
    "            writer_wide = pd.ExcelWriter(wide_fmt_filename)\n",
    "\n",
    "        # This writes out LONG data with TOPIC as SHEET_NAME.  Later we'll create a truly long table with TOPIC restored as a column\n",
    "        melted_df.loc[:, melted_df.columns != 'Topic'].to_excel(writer_long, index=False, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "        print(ws.title)\n",
    "        columns = ['Variable', 'Unit']\n",
    "        # We need these columns to reshape our data\n",
    "        for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "            if df[extra_col].notna().any():\n",
    "                columns.append(extra_col)\n",
    "        # In the case of Shell, we have only one topic per sheet, so can transform melted_df directly\n",
    "        pf = melted_df.pivot(index=['Year', 'Topic'], columns=columns, values=['Value'])\n",
    "        pf = pf.droplevel('Topic')\n",
    "        # Once reshaped, the extra columns actually appear as multi-level indexes.  Drop them from also behaving like values\n",
    "        pf[[c for c in columns if c not in ['Variable', 'Unit']]] = pd.NA\n",
    "        pf.dropna(how='all', axis=1, inplace=True)\n",
    "        pf.to_excel(writer_wide, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "    writer_long.close()\n",
    "    writer_wide.close()\n",
    "    \n",
    "    # We are now working with our own workbook, which doesn't have a zero-index sheet to ignore\n",
    "    # Make the workbook more legible to those reading it\n",
    "    long_wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "    for ws in long_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            if get_column_letter(col)=='A':\n",
    "                width = 40\n",
    "            elif get_column_letter(col) in ['B', 'E']:\n",
    "                width = 15\n",
    "            elif get_column_letter(col) in ['C', 'D']:\n",
    "                width = 25\n",
    "            else:\n",
    "                width = 10\n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=width)\n",
    "        ws.column_dimensions = dim_holder\n",
    "    \n",
    "    long_wb.save(long_fmt_filename)\n",
    "    long_wb.close()\n",
    "    \n",
    "    def as_text(value):\n",
    "        if value is None:\n",
    "            return \"\"\n",
    "        return str(value)\n",
    "    \n",
    "    # Write out dataframe in WIDE format.  This data is technically tidy, with one multi-dimensional observation per row.\n",
    "    # Units/dimensions are consistent on a per-column basis, making it easy to aggregate column-based data.\n",
    "    wide_wb = load_workbook(wide_fmt_filename, data_only=True)\n",
    "    # Make the workbook more legible to those reading it\n",
    "    for ws in wide_wb.worksheets:\n",
    "        dim_holder = DimensionHolder(worksheet=ws)\n",
    "        for col in range(ws.min_column, ws.max_column + 1):\n",
    "            cell = ws.cell(2, col)\n",
    "            cell.alignment = Alignment(wrap_text=True,vertical='top') \n",
    "            dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=max(10,1+len(as_text(cell.value))/3))\n",
    "        ws.column_dimensions = dim_holder\n",
    "\n",
    "    wide_wb.save(wide_fmt_filename)\n",
    "    wide_wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09fedea-25f5-4559-b8e6-b7b12ec6a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenhouse-gas-and-energy-data-shell-sr20.xlsx\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "77 x 11\n",
      "25 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-net-carbon-footprint\n",
      "ingest_file: processing 7\n",
      "process_topic 7: setting topic Footprint\n",
      "process_topic 7: setting category Net Carbon Footprint\n",
      "process_topic 7: setting units g CO2e/ MJ\n",
      "process_categories 9: segmenting Share of energy delivered::energy product type [C]\n",
      "pop at row 15: segmentation_stack now []\n",
      "process_categories 16: segmenting Carbon intensity::energy products type \n",
      "process_segmentation: stack at end = [<Cell 'opd-net-carbon-footprint'.K16>]\n",
      "wb(1) dataframe\n",
      "opd-net-carbon-footprint\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "111 x 12\n",
      "60 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-scope-1-ghg-emissions\n",
      "process_topic 6: setting category Direct GHG emissions (Scope 1) [A] [B] [C] [D]\n",
      "process_topic 6: setting units million tonnes CO2e\n",
      "process_categories 14: segmenting Scope 1 emissions::business\n",
      "pop at row 19: segmentation_stack now []\n",
      "process_categories 19: segmenting Scope 1 emissions::country\n",
      "pop at row 32: segmentation_stack now []\n",
      "process_categories 32: segmenting Scope 1 emissions::source\n",
      "pop at row 38: segmentation_stack now [<Cell 'opd-scope-1-ghg-emissions'.K32>]\n",
      "pop at row 43: segmentation_stack now [<Cell 'opd-scope-1-ghg-emissions'.K32>]\n",
      "pop at row 44: segmentation_stack now []\n",
      "process_categories 44: setting units from var: CH4\n",
      "process_categories 50: segmenting Total hydrocarbons flared::hydrocarbons flared\n",
      "process_segmentation: stack at end = [<Cell 'opd-scope-1-ghg-emissions'.K50>]\n",
      "wb(2) dataframe\n",
      "opd-scope-1-ghg-emissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/pandas/core/frame.py:5238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "91 x 12\n",
      "33 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-scope-2-ghg-emissions\n",
      "process_topic 6: setting category Scope 2 emissions - market-based method \n",
      "process_topic 6: setting units million tonnes CO2e\n",
      "process_categories 8: segmenting Scope 2 emissions::business (market-based method)\n",
      "pop at row 13: segmentation_stack now []\n",
      "process_categories 13: segmenting Scope 2 emissions::country (market-based method)\n",
      "pop at row 21: segmentation_stack now []\n",
      "process_categories 21: segmenting Scope 2 emissions::business (location-based method)\n",
      "pop at row 26: segmentation_stack now []\n",
      "process_categories 26: segmenting Scope 2 emissions::country (location-based method)\n",
      "process_segmentation: stack at end = [<Cell 'opd-scope-2-ghg-emissions'.K26>]\n",
      "wb(3) dataframe\n",
      "opd-scope-2-ghg-emissions\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "67 x 12\n",
      "13 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-greenhouse-gas-intensities\n",
      "process_topic 6: setting category Upstream and Integrated Gas GHG intensity [A]\n",
      "process_topic 6: setting units tonne CO2e/tonne production\n",
      "find units: nothing found for tonne CO2e/UEDC\n",
      "wb(4) dataframe\n",
      "opd-greenhouse-gas-intensities\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "79 x 15\n",
      "21 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-scope-1-2-ghg-emissions\n",
      "process_topic 6: setting category Direct GHG emissions (Scope 1)\n",
      "process_topic 6: setting units million tonnes CO2e\n",
      "wb(5) dataframe\n",
      "opd-scope-1-2-ghg-emissions\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "71 x 12\n",
      "21 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-scope-3-greenhosue-gas-em\n",
      "find units: nothing found for Category 1\n",
      "ingest_file: processing 7\n",
      "worksheet opd-scope-3-greenhosue-gas-em: unknown topic Third-party products [C]\n",
      "process_topic 7: setting category Third-party products [C]\n",
      "process_topic 7: setting units million tonnes CO2e\n",
      "process_categories 10: segmenting Use::sold products (Category 11) \n",
      "process_segmentation: stack at end = [<Cell 'opd-scope-3-greenhosue-gas-em'.K10>, <Cell 'opd-scope-3-greenhosue-gas-em'.A11>]\n",
      "wb(6) dataframe\n",
      "opd-scope-3-greenhosue-gas-em\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "66 x 12\n",
      "9 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-other-greenhouse-gas-data\n",
      "ingest_file: processing 7\n",
      "worksheet opd-other-greenhouse-gas-data: unknown topic CO2 captured and stored\n",
      "process_topic 7: setting category CO2 captured and stored\n",
      "process_topic 7: setting units million tonnes\n",
      "wb(7) dataframe\n",
      "opd-other-greenhouse-gas-data\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "66 x 12\n",
      "9 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-carbon-offsets\n",
      "process_topic 6: setting category Total carbon offsets retired\n",
      "process_topic 6: setting units million tonnes\n",
      "process_categories 6: segmenting Total carbon offsets retired::carbon offsets retired\n",
      "process_segmentation: stack at end = [<Cell 'opd-carbon-offsets'.K6>]\n",
      "wb(8) dataframe\n",
      "opd-carbon-offsets\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "78 x 12\n",
      "22 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title opd-energy-use\n",
      "process_topic 6: setting category Total energy use\n",
      "process_topic 6: setting units million MWh\n",
      "process_categories 6: segmenting Total energy use::energy use\n",
      "pop at row 12: segmentation_stack now []\n",
      "process_categories 12: segmenting Consumption::energy from renewable sources\n",
      "pop at row 17: segmentation_stack now []\n",
      "wb(9) dataframe\n",
      "opd-energy-use\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "84 x 11\n",
      "9 x 11\n",
      "['2020', '2019', '2018', '2017', '2016']\n",
      "process_topic 6: setting topic from title sef-sales-gas-power\n",
      "process_topic 6: setting units from var: tBtu\n",
      "process_topic 6: setting category Gas (tBtu)\n",
      "process_topic 6: setting units tBtu\n",
      "wb(10) dataframe\n",
      "sef-sales-gas-power\n",
      "2021-Data-Centerv1.xlsx\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "306 x 4\n",
      "44 x 4\n",
      "['2018', '2019', '2020']\n",
      "process_topic 2: setting topic from title energy\n",
      "process_categories: new category set at row 3: Owned Generation Capacity (MW) (color = FF40B14B)\n",
      "process_categories 3: setting units from var: MW\n",
      "process_categories 4: segmenting Total Owned Nameplate Generation Capacity::Owned Nameplate Generation Capacity\n",
      "pop at row 13: segmentation_stack now []\n",
      "process_categories: new category set at row 13: Owned Net Generation (MWh) (color = FF40B14B)\n",
      "process_categories 13: setting units from var: MWh\n",
      "process_categories 14: segmenting Total Owned Net Generation::Owned Net Generation\n",
      "pop at row 23: segmentation_stack now []\n",
      "process_categories: new category set at row 23: Purchased Net Generation (MWh) (color = FF40B14B)\n",
      "process_categories 23: setting units from var: MWh\n",
      "process_categories 24: segmenting Total Purchased Generation ::Purchased Generation \n",
      "pop at row 33: segmentation_stack now []\n",
      "process_categories: new category set at row 33: Total Net Generation (MWh) (color = FF40B14B)\n",
      "process_categories 33: setting units from var: MWh\n",
      "process_categories 33: segmenting Total Net Generation (MWh)::Net Generation (MWh)\n",
      "pop at row 43: segmentation_stack now [<Cell 'Energy'.H33>]\n",
      "pop at row 43: segmentation_stack now []\n",
      "process_categories: new category set at row 43: Facility Energy Performance (color = FF40B14B)\n",
      "wb(0) dataframe\n",
      "Energy\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "16 x 4\n",
      "16 x 4\n",
      "['2018', '2019', '2020']\n",
      "process_topic 2: setting topic from title emissions\n",
      "find units: nothing found for From AEP owned facilities only\n",
      "process_categories: new category set at row 3: Scope 1 emissions breakdown (color = FFBDBDBD)\n",
      "process_categories 4: setting units from var: Metric Tons\n",
      "process_categories 5: setting units from var: Lbs\n",
      "process_categories 6: setting units from var: MT\n",
      "process_categories 7: setting units from var: Lbs\n",
      "process_categories 8: setting units from var: MT\n",
      "process_categories 9: setting units from var: Lbs\n",
      "process_categories 10: setting units from var: kg\n",
      "process_categories: new category set at row 11: Scope 1 Emissions GHG CO2e (color = FFBDBDBD)\n",
      "process_categories 12: setting units from var: Metric Tons\n",
      "process_categories 13: setting units from var: MT\n",
      "process_categories 14: setting units from var: MT CO2e\n",
      "process_categories 15: setting units from var: MT CO2e\n",
      "wb(1) dataframe\n",
      "Emissions\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "28 x 4\n",
      "28 x 4\n",
      "['2018', '2019', '2020']\n",
      "process_topic 2: setting topic from title water\n",
      "process_categories: new category set at row 3: Total Water Withdrawal (color = FFB9DDFC)\n",
      "find units: nothing found for Comanche Plant\n",
      "process_categories 3: segmenting Total Water Withdrawal::Water Withdrawal\n",
      "find units: nothing found for Gallons/day\n",
      "process_var 4: unhandled ( Million Gallons/day )\n",
      "find units: nothing found for Gallons/ year\n",
      "process_var 5: unhandled ( Million Gallons/ year )\n",
      "find units: nothing found for Liters/year\n",
      "process_var 6: unhandled ( Billions of Liters/year )\n",
      "pop at row 8: segmentation_stack now []\n",
      "process_categories: new category set at row 8: Water Withdrawal by Source Breakdown (color = FFB9DDFC)\n",
      "process_categories 8: segmenting Water Withdrawal::Source Breakdown\n",
      "find units: nothing found for Gallons per day\n",
      "process_var 9: unhandled ( Million Gallons per day )\n",
      "find units: nothing found for Gallons/ year\n",
      "process_var 10: unhandled ( Million Gallons/ year )\n",
      "pop at row 14: segmentation_stack now []\n",
      "process_categories: new category set at row 14: Water Discharge (color = FFB9DDFC)\n",
      "process_categories: unknown color FFE2F2FE at row 15: Total Water Discharge (Million Gallons per day)\n",
      "find units: nothing found for Gallons per day\n",
      "find units: nothing found for Gallons per day\n",
      "process_var 15: unhandled ( Million Gallons per day )\n",
      "process_categories: unknown color FFE2F2FE at row 16: Total Water Discharge (Million Gallons/ year)\n",
      "find units: nothing found for Gallons/ year\n",
      "find units: nothing found for Gallons/ year\n",
      "process_var 16: unhandled ( Million Gallons/ year )\n",
      "process_categories: unknown color FFE2F2FE at row 17: Total Water Discharge (Billions of Liters/year)\n",
      "find units: nothing found for Liters/year\n",
      "find units: nothing found for Liters/year\n",
      "process_var 17: unhandled ( Billions of Liters/year )\n",
      "process_categories: unknown color FFE2F2FE at row 18: Total Water Discharge (m3/year)\n",
      "process_categories 18: setting units from var: m3/year\n",
      "process_categories: new category set at row 19: Water Consumption (color = FFB9DDFC)\n",
      "process_categories: unknown color FFE2F2FE at row 20: Total Water Consumption (Million Gallons per day)\n",
      "find units: nothing found for Gallons per day\n",
      "find units: nothing found for Gallons per day\n",
      "process_var 20: unhandled ( Million Gallons per day )\n",
      "process_categories: unknown color FFE2F2FE at row 21: Total Water Consumption (Million Gallons/ year)\n",
      "find units: nothing found for Gallons/ year\n",
      "find units: nothing found for Gallons/ year\n",
      "process_var 21: unhandled ( Million Gallons/ year )\n",
      "process_categories: unknown color FFE2F2FE at row 22: Total Water Consumption (billions of Liters/year)\n",
      "find units: nothing found for Liters/year\n",
      "find units: nothing found for Liters/year\n",
      "process_var 22: unhandled ( billions of Liters/year )\n",
      "process_categories: unknown color FFE2F2FE at row 23: Total Water Consumption (m3/year)\n",
      "process_categories 23: setting units from var: m3/year\n",
      "process_categories: new category set at row 24: Water Intensity (color = FFB9DDFC)\n",
      "process_categories: unknown color FFE2F2FE at row 25: Consumptive Water use intensity (gallons/Net MWh)\n",
      "process_categories 25: setting units from var: gallons/Net MWh\n",
      "process_categories: unknown color FFE2F2FE at row 26: Consumptive Water use intensity (Billions of liters/Net MWh)\n",
      "process_categories 26: setting units from var: Billions of liters/Net MWh\n",
      "process_categories: unknown color FFE2F2FE at row 27: Non- Consumptive Water use intensity (gallons/Net MWh)\n",
      "process_categories 27: setting units from var: gallons/Net MWh\n",
      "wb(2) dataframe\n",
      "Water\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "13 x 4\n",
      "13 x 4\n",
      "['2018', '2019', '2020']\n",
      "process_topic 2: setting topic from title waste\n",
      "process_categories: new category set at row 3: Facility Waste Generation (color = FFEEDCCA)\n",
      "find units: nothing found for Waste data does not include waste streams from competitive portion of business\n",
      "process_categories: unknown color FFF2E8DE at row 4: Recycled Paper and Office Waste (Lbs)\n",
      "process_categories 4: setting units from var: Lbs\n",
      "process_categories: unknown color FFF2E8DE at row 5: Recycled Scrap Metal Waste (Lbs)\n",
      "process_categories 5: setting units from var: Lbs\n",
      "process_categories: unknown color FFF2E8DE at row 6: Batteries Recycled (Lbs)\n",
      "process_categories 6: setting units from var: Lbs\n",
      "process_categories: unknown color FFF2E8DE at row 7: Electronic Waste Recycled( Lbs)\n",
      "process_categories 7: setting units from var:  Lbs\n",
      "process_categories: unknown color FFF2E8DE at row 8: Light Bulbs Recycled (Lbs)\n",
      "process_categories 8: setting units from var: Lbs\n",
      "process_categories: unknown color FFF2E8DE at row 9: Recycled used Oil (Gallons)\n",
      "process_categories 9: setting units from var: Gallons\n",
      "process_categories: new category set at row 10: Coal Combustion Products (color = FFEEDCCA)\n",
      "process_categories: unknown color FFF2E8DE at row 11: Total CCPs Generated (Tons)\n",
      "process_categories 11: setting units from var: Tons\n",
      "process_categories: unknown color FFF2E8DE at row 12: Total CCPs Diverted from Landfill (Tons)\n",
      "process_categories 12: setting units from var: Tons\n",
      "wb(3) dataframe\n",
      "Waste\n",
      "DPDHL-ESG-Statbook-2020-en.xlsx\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "90 x 13\n",
      "80 x 10\n",
      "find_header_row: color = FF00B050\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "['2016', '2017', '2018', '2019', '2020']\n",
      "worksheet Environmental Group Overview: unknown topic Environmental Data at Group levels\n",
      "ingest_file: processing 2\n",
      "worksheet Environmental Group Overview: unknown topic The Group's material topics: Carbon Efficiency & Climate Protection, Air Pollution\n",
      "ingest_file: processing 3\n",
      "worksheet Environmental Group Overview: unknown topic Key Performance Indicator : Carbon Efficiency Index \n",
      "ingest_file: processing 4\n",
      "worksheet Environmental Group Overview: unknown topic Relevant GRI-indicators: 103 and 305\n",
      "ingest_file: processing 5\n",
      "worksheet Environmental Group Overview: unknown topic Relevant SASB-codes: TR-AF-110a.1 - 3; 120a.1\n",
      "ingest_file: processing 6\n",
      "worksheet Environmental Group Overview: unknown topic Materiality analysis\n",
      "ingest_file: processing 7\n",
      "process_topic 7: no var text\n",
      "new topic set at row 9: Carbon emissions (color = FF00B050)\n",
      "process_topic 9: setting topic emissions\n",
      "process_categories: new category set at row 10: KPI: Carbon Efficiency Index (CEX) (color = E2F0D9)\n",
      "find units: nothing found for CEX\n",
      "process_var 10: unhandled ( CEX )\n",
      "process_categories 11: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 15: segmentation_stack now []\n",
      "process_categories: new category set at row 15: CO2e emissions by modes (color = E2F0D9)\n",
      "process_categories 15: segmenting CO2e emissions::modes\n",
      "pop at row 20: segmentation_stack now []\n",
      "process_categories: new category set at row 20: CO2e intensity total (color = E2F0D9)\n",
      "process_categories 20: segmenting CO2e intensity total::CO2e intensity\n",
      "pop at row 22: segmentation_stack now []\n",
      "process_categories: new topic set at row 22: Energy consumption total (Scopes 1 and 2) (color = FF00B050)\n",
      "process_topic 22: setting topic Energy\n",
      "process_topic 22: setting category Energy consumption total \n",
      "process_topic 22: setting units m kWh\n",
      "process_categories 22: segmenting Energy consumption::(Scopes 1 and 2)\n",
      "find units: nothing found for LPG\n",
      "process_var 30: unhandled ( LPG )\n",
      "pop at row 31: segmentation_stack now [<Cell 'Environmental Group Overview'.K22>, <Cell 'Environmental Group Overview'.A23>]\n",
      "find units: nothing found for CNG\n",
      "process_var 33: unhandled ( CNG )\n",
      "find units: nothing found for LNG\n",
      "process_var 34: unhandled ( LNG )\n",
      "pop at row 35: segmentation_stack now [<Cell 'Environmental Group Overview'.K22>, <Cell 'Environmental Group Overview'.A23>]\n",
      "pop at row 35: segmentation_stack now [<Cell 'Environmental Group Overview'.K22>]\n",
      "pop at row 40: segmentation_stack now [<Cell 'Environmental Group Overview'.K22>, <Cell 'Environmental Group Overview'.A35>]\n",
      "find units: nothing found for LPG\n",
      "process_var 45: unhandled ( LPG )\n",
      "pop at row 46: segmentation_stack now [<Cell 'Environmental Group Overview'.K22>]\n",
      "pop at row 46: segmentation_stack now []\n",
      "process_categories: new topic set at row 46: Local air pollutants (Scope 1) (color = FF00B050)\n",
      "process_topic 46: setting category Local air pollutants \n",
      "process_topic 46: setting units t\n",
      "process_categories: new category set at row 47: Mono-nitrogen oxides (NOx) total (color = E2F0D9)\n",
      "process_categories 47: setting units from var: NOx\n",
      "process_categories 47: segmenting Mono-nitrogen oxides (NOx) total::Mono-nitrogen oxides (NOx)\n",
      "pop at row 50: segmentation_stack now []\n",
      "process_categories: new category set at row 50: Sulfur dioxide (SO2) total (color = E2F0D9)\n",
      "process_categories 50: setting units from var: SO2\n",
      "process_categories 50: segmenting Sulfur dioxide (SO2) total::Sulfur dioxide (SO2)\n",
      "pop at row 53: segmentation_stack now []\n",
      "process_categories: new category set at row 53: Particulate matter (PM10) total (color = E2F0D9)\n",
      "process_categories 53: setting units from var: PM10\n",
      "process_categories 53: segmenting Particulate matter (PM10) total::Particulate matter (PM10)\n",
      "pop at row 56: segmentation_stack now []\n",
      "process_categories: new topic set at row 56: Sites with ISO certifications (color = FF00B050)\n",
      "process_topic 56: setting category Sites with ISO certifications\n",
      "process_topic 56: setting units Share\n",
      "process_categories: new category set at row 57: Sites total (color = E2F0D9)\n",
      "process_categories 57: segmenting Sites total::Sites\n",
      "pop at row 62: segmentation_stack now [<Cell 'Environmental Group Overview'.K57>]\n",
      "pop at row 62: segmentation_stack now []\n",
      "process_categories: new topic set at row 62: OTHER ENVIRONMENTAL DATA (color = D0CECE)\n",
      "process_topic 62: setting topic OTHER\n",
      "ingest_file: processing 66\n",
      "worksheet Environmental Group Overview: unknown topic Scope 2 total \n",
      "process_topic 66: setting category Scope 2 total \n",
      "process_topic 66: setting units m t CO2e\n",
      "process_categories: new category set at row 67: Scope 3 CO2e emissions by GHG categories  (color = E2F0D9)\n",
      "process_categories 67: segmenting Scope 3 CO2e emissions::GHG categories \n",
      "pop at row 75: segmentation_stack now [<Cell 'Environmental Group Overview'.K67>]\n",
      "process_segmentation: stack at end = [<Cell 'Environmental Group Overview'.K67>]\n",
      "NOTES\n",
      "['n/d = not disclosed', '1', 'Calculation of CO2e emissions and CEX based on GHG, GLEC, EN\\xa016258, ETS. Therefore, offsetting not included.', '2', 'Including emissions from rail, ferry and business cars that are not listed separatly (together < 1%).', '3', 'Including electric vehicle consumption.', '4', 'Also includes quantities of gasoline and diesel for auxiliary power generators', '5', 'Air transport: ecotransIT, ocean transport: Clean Cargo Working Group, road transport: Handbook Emission Factors for Road Transport.', '6', 'Air transport: kerosene; road transport: diesel; ocean transport: HFO.', '7', 'Adjusted;', '8', 'The decrease from 2018 to 2019 was mainly due to the reorientation of, and associated organizational changes in, our post and parcel business in the year under review. In 2020, data for eCommerce Solutions not updated.', '9', 'Water consumption is not considered a material issue for our business model. We therefore only record consumption at our German sites.']\n",
      "END NOTES\n",
      "wb(2) dataframe\n",
      "Environmental Group Overview\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "49 x 10\n",
      "49 x 9\n",
      "find_header_row: color = FF00B050\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "['2016', '2017', '2018', '2019', '2020']\n",
      "worksheet Environmental Data by Division: unknown topic Environmental Data by Divisions\n",
      "ingest_file: processing 2\n",
      "worksheet Environmental Data by Division: unknown topic The Group's material topics: Carbon Efficiency & Climate Protection, Air Pollution\n",
      "ingest_file: processing 3\n",
      "worksheet Environmental Data by Division: unknown topic Key Performance Indicator: Carbon Efficiency Index \n",
      "find units: nothing found for CEX\n",
      "ingest_file: processing 4\n",
      "process_topic 4: no var text\n",
      "new topic set at row 6: Post & Parcel  Germany (color = FF00B050)\n",
      "process_categories: new category set at row 7: CEX (color = E2F0D9)\n",
      "process_categories 8: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 12: segmentation_stack now []\n",
      "process_categories: new category set at row 12: Energy consumption (own operations) total (color = E2F0D9)\n",
      "find units: nothing found for own operations\n",
      "process_var 12: unhandled ( own operations )\n",
      "process_categories: new topic set at row 13: Express (color = FF00B050)\n",
      "process_categories: new category set at row 14: CEX (color = E2F0D9)\n",
      "process_categories 15: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 19: segmentation_stack now []\n",
      "process_categories: new category set at row 19: Energy consumption (own operations) total (color = E2F0D9)\n",
      "find units: nothing found for own operations\n",
      "process_var 19: unhandled ( own operations )\n",
      "process_categories: new topic set at row 20: Global Forwarding, Freight (color = FF00B050)\n",
      "process_categories: new category set at row 21: CEX (color = E2F0D9)\n",
      "process_categories 22: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 26: segmentation_stack now []\n",
      "process_categories: new category set at row 26: Energy consumption (own operations) total (color = E2F0D9)\n",
      "find units: nothing found for own operations\n",
      "process_var 26: unhandled ( own operations )\n",
      "process_categories: new topic set at row 27: Supply Chain (color = FF00B050)\n",
      "process_categories: new category set at row 28: CEX (color = E2F0D9)\n",
      "process_categories 29: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 33: segmentation_stack now []\n",
      "process_categories: new category set at row 33: Energy consumption (own operations) total (color = E2F0D9)\n",
      "find units: nothing found for own operations\n",
      "process_var 33: unhandled ( own operations )\n",
      "process_categories: new topic set at row 34: eCommerce Solutions (color = FF00B050)\n",
      "process_categories: new category set at row 35: CEX (color = E2F0D9)\n",
      "process_categories 36: segmenting CO2e emissions total::CO2e emissions\n",
      "pop at row 40: segmentation_stack now []\n",
      "process_categories: new category set at row 40: Energy consumption (own operations) total (color = E2F0D9)\n",
      "find units: nothing found for own operations\n",
      "process_var 40: unhandled ( own operations )\n",
      "process_categories: new topic set at row 41: OTHER ENVIRONMENTAL DATA (color = E7E6E6)\n",
      "process_topic 41: setting topic OTHER\n",
      "process_categories: new category set at row 42: Scope 2 CO2e emissions calculated by location-based method (GHG) (color = E2F0D9)\n",
      "find units: nothing found for GHG\n",
      "process_categories 42: segmenting Scope 2 CO2e emissions calculated::location-based method (GHG)\n",
      "find units: nothing found for GHG\n",
      "process_var 42: unhandled ( GHG )\n",
      "process_segmentation: stack at end = [<Cell 'Environmental Data by Division'.K42>]\n",
      "NOTES\n",
      "['1) Calculation of CO2e emissions and CEX based on GHG, GLEC, EN\\xa016258, ETS. Therefore, offsetting not included.', '2', 'Adjusted.']\n",
      "END NOTES\n",
      "wb(3) dataframe\n",
      "Environmental Data by Division\n",
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "42 x 10\n",
      "40 x 9\n",
      "find_header_row: color = FF00B050\n",
      "find_header_row: color = 00000000\n",
      "find_header_row: color = 00000000\n",
      "['2016', '2017', '2018', '2019', '2020']\n",
      "worksheet Group Fleet Data: unknown topic Fleet Data at Group Levels\n",
      "ingest_file: processing 2\n",
      "worksheet Group Fleet Data: unknown topic The Group's material topics: Carbon Efficiency & Climate Protection, Air Pollution\n",
      "ingest_file: processing 3\n",
      "process_topic 3: no var text\n",
      "new topic set at row 5: Air fleet (jets and feeders) (color = FF00B050)\n",
      "process_topic 5: setting category Air fleet \n",
      "process_topic 5: setting units Total no.\n",
      "find units: nothing found for jets and feeders\n",
      "process_var 5: unhandled ( jets and feeders )\n",
      "process_categories: new category set at row 6: Jets by NOx emission standards (color = E2F0D9)\n",
      "process_categories 6: segmenting Jets::NOx emission standards\n",
      "pop at row 12: segmentation_stack now []\n",
      "process_categories: new category set at row 12: Jets by noise standards (color = E2F0D9)\n",
      "process_categories 12: segmenting Jets::noise standards\n",
      "pop at row 16: segmentation_stack now []\n",
      "process_categories: new topic set at row 16: Vehicles  (color = FF00B050)\n",
      "process_topic 16: setting category Vehicles \n",
      "process_topic 16: setting units Total no.\n",
      "process_categories: new category set at row 20: Vehicles with alternative drive systems (color = E2F0D9)\n",
      "find units: nothing found for CNG and LNG\n",
      "find units: nothing found for CNG and LNG\n",
      "process_var 23: unhandled ( CNG and LNG )\n",
      "find units: nothing found for LPG\n",
      "find units: nothing found for LPG\n",
      "process_var 24: unhandled ( LPG )\n",
      "process_categories: new category set at row 27: Vehicles with Euronorm classifications (color = E2F0D9)\n",
      "find units: nothing found for zero emissions vehicles\n",
      "find units: nothing found for zero emissions vehicles\n",
      "process_var 28: unhandled ( zero emissions vehicles )\n",
      "process_categories: new category set at row 35: Bicycles (color = E2F0D9)\n",
      "find units: nothing found for Post & Parcel Germany\n",
      "find units: nothing found for Post & Parcel Germany\n",
      "process_var 36: unhandled ( Post & Parcel Germany )\n",
      "find units: nothing found for Post & Parcel Germany\n",
      "find units: nothing found for Post & Parcel Germany\n",
      "process_var 37: unhandled ( Post & Parcel Germany )\n",
      "find units: nothing found for Express\n",
      "process_var 38: unhandled ( Express )\n",
      "NOTES\n",
      "['n/a = not available', '1', 'Including no. of Streetscooters:  9,048 (FY 2018); 10,510 (FY 2019); 14,435 (FY 2020);', '2', 'Bioethanol trucks were replaced with other technologies by the leasing companies in 2019;', '3', 'EEV:  Enhanced environmentally friendly vehicles']\n",
      "END NOTES\n",
      "wb(4) dataframe\n",
      "Group Fleet Data\n",
      "Unilever sustainability performance data_Climate FINAL.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/openpyxl/reader/drawings.py:29: UserWarning: DrawingML support is incomplete and limited to charts and images only. Shapes and drawings will be lost.\n",
      "  warn(\"DrawingML support is incomplete and limited to charts and images only. Shapes and drawings will be lost.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "48 x 1007\n",
      "47 x 12\n",
      "['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011', '2010']\n",
      "worksheet Sustainability performance data: unknown topic CLIMATE ACTION\n",
      "new topic set at row 11: Value chain GHG emissions (color = FFEBF1DE)\n",
      "process_topic 11: setting topic emissions\n",
      "ingest_file: processing 12\n",
      "process_topic 12: setting topic emissions\n",
      "process_topic 12: setting units from var: tonnes CO2\n",
      "process_topic 12: setting category Unilever operations: Scope 1 GHG emissions \n",
      "process_topic 12: setting units tonnes CO2\n",
      "process_categories: unknown color FFFFFF at row 13: Unilever operations: Scope 2 GHG emissions (tonnes CO2)\n",
      "process_categories 13: setting units from var: tonnes CO2\n",
      "process_categories: unknown color FFFFFF at row 14: Upstream and downstream of Unilever operations: Scope 3 GHG emissions (tonnes CO2e)\n",
      "process_categories 14: setting units from var: tonnes CO2e\n",
      "process_categories: new topic set at row 15: Ingredients and packaging:  (color = E2F0D9)\n",
      "ingest_file: processing 17\n",
      "worksheet Sustainability performance data: unknown topic       Primary packaging \n",
      "ingest_file: processing 18\n",
      "worksheet Sustainability performance data: unknown topic       Secondary packaging\n",
      "ingest_file: processing 19\n",
      "worksheet Sustainability performance data: unknown topic       Inbound transport\n",
      "new topic set at row 20: Distribution and retail:  (color = E2F0D9)\n",
      "ingest_file: processing 21\n",
      "worksheet Sustainability performance data: unknown topic       Distribution\n",
      "ingest_file: processing 22\n",
      "worksheet Sustainability performance data: unknown topic       Retail\n",
      "new topic set at row 23: Consumer use:  (color = E2F0D9)\n",
      "ingest_file: processing 24\n",
      "worksheet Sustainability performance data: unknown topic      Consumer use\n",
      "ingest_file: processing 25\n",
      "worksheet Sustainability performance data: unknown topic      Disposal\n",
      "ingest_file: processing 26\n",
      "process_topic 26: no var text\n",
      "new topic set at row 27: Manufacturing energy use and GHG emissions (color = E2F0D9)\n",
      "process_topic 27: setting topic energy\n",
      "process_topic 27: setting topic emissions\n",
      "ingest_file: processing 28\n",
      "process_topic 28: setting topic energy\n",
      "process_topic 28: setting units from var: GJ\n",
      "process_topic 28: setting category Total energy use  \n",
      "process_topic 28: setting units GJ\n",
      "process_categories 29: setting units from var: GJ/tonne of production\n",
      "process_categories: unknown color FFFFFF at row 30: Renewable energy use (GJ)\n",
      "process_categories 30: setting units from var: GJ\n",
      "process_categories: unknown color FFFFFF at row 31: Share of renewable energy use (%)\n",
      "process_categories 31: setting units from var: %\n",
      "process_categories 32: setting units from var: tonnes\n",
      "process_categories 33: setting units from var: kg/tonne of production\n",
      "finish_notes @ 36\n",
      "wb(0) dataframe\n",
      "Sustainability performance data\n",
      "esg-tables.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/openpyxl/reader/drawings.py:55: UserWarning: The image xl/media/image2.tiff will be removed because it cannot be read\n",
      "  warn(msg)\n",
      "/opt/app-root/lib64/python3.8/site-packages/openpyxl/reader/drawings.py:55: UserWarning: The image xl/media/image3.tiff will be removed because it cannot be read\n",
      "  warn(msg)\n",
      "/opt/app-root/lib64/python3.8/site-packages/openpyxl/reader/drawings.py:55: UserWarning: The image xl/media/image4.tiff will be removed because it cannot be read\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "crop_sheet\n",
      "132 x 28\n",
      "122 x 7\n",
      "['2017', '2020', '2019', '2018']\n",
      "process_topic 3: setting topic from title environmental\n",
      "process_topic 3: setting category Scope 1 (direct) emissions\n",
      "process_topic 3: setting units tonnes CO2e\n",
      "find units: nothing found for direct\n",
      "process_var 3: unhandled ( direct )\n",
      "process_categories: unknown color FFFFFFFF at row 4: Normalized Scope 1 emissions\n",
      "process_categories: new category set at row 5: Scope 1 emissions by business division (color = FF92D050)\n",
      "process_categories 5: segmenting Scope 1 emissions::business division\n",
      "pop at row 15: segmentation_stack now []\n",
      "process_categories: new category set at row 15: Scope 2 (indirect) emissions-market-based (color = FF92D050)\n",
      "find units: nothing found for indirect\n",
      "process_var 15: unhandled ( indirect )\n",
      "process_categories: unknown color FFFFFFFF at row 16: Normalized Scope 2 emissions-market-based\n",
      "process_categories: new category set at row 17: Scope 2 emissions by business division-market-based (color = FF92D050)\n",
      "process_categories 17: segmenting Scope 2 emissions::business division-market-based\n",
      "pop at row 27: segmentation_stack now []\n",
      "process_categories: new category set at row 27: Scope 3 (indirect) emissions (color = FF92D050)\n",
      "find units: nothing found for indirect\n",
      "find units: nothing found for indirect\n",
      "process_var 27: unhandled ( indirect )\n",
      "process_categories: unknown color FFE2F0D9 at row 28: Purchased goods and services\n",
      "process_categories: unknown color FFFFFFFF at row 29: Capital goods\n",
      "process_categories: unknown color FFE2F0D9 at row 30: Fuel and energy related activities\n",
      "process_categories: unknown color FFFFFFFF at row 31: Upstream transportation/distribution\n",
      "process_categories: unknown color FFE2F0D9 at row 32: Waste generated in operations\n",
      "process_categories: unknown color FFFFFFFF at row 33: Business travel\n",
      "process_categories: unknown color FFE2F0D9 at row 34: Employee commuting\n",
      "process_categories: unknown color FFFFFFFF at row 35: Downstream transportation/distribution\n",
      "process_categories: unknown color FFE2F0D9 at row 36: End-of-life treatment of sold products\n",
      "process_categories: unknown color FFFFFFFF at row 37: Investments\n",
      "process_categories: new category set at row 38: Water Use (color = FF92D050)\n",
      "process_categories: unknown color FFFFFFFF at row 39: Normalized Water Use\n",
      "process_categories: new category set at row 40: Water Use by Business Division (color = FF92D050)\n",
      "process_categories 40: segmenting Water Use::Business Division\n",
      "pop at row 52: segmentation_stack now []\n",
      "process_categories: new topic set at row 52: Wastewater discharge (color = FF9BDA44)\n",
      "process_topic 52: setting category Wastewater discharge\n",
      "process_topic 52: setting units millions of gallons\n",
      "process_categories: unknown color FFFFFFFF at row 53: Normalized wastewater discharge\n",
      "process_categories: new category set at row 54: Wastewater discharge by business division (color = FF92D050)\n",
      "process_categories 54: segmenting Wastewater discharge::business division\n",
      "pop at row 64: segmentation_stack now []\n",
      "process_categories: new topic set at row 64: Energy Use (color = FF9BDA44)\n",
      "process_topic 64: setting topic Energy\n",
      "process_topic 64: setting category Energy Use\n",
      "process_topic 64: setting units billions of btus\n",
      "process_categories: unknown color FFFFFFFF at row 65: Normalized energy use\n",
      "process_categories: new category set at row 66: Energy Use by business division (color = FF92D050)\n",
      "process_categories 66: segmenting Energy Use::business division\n",
      "pop at row 76: segmentation_stack now []\n",
      "process_categories: unknown color FFFFFFFF at row 76: Energy from renewables\n",
      "process_categories: unknown color FFE2F0D9 at row 77: Energy from renewables\n",
      "process_categories: unknown color FFFFFFFF at row 78: Non-renewable energy\n",
      "process_categories: unknown color FFE2F0D9 at row 79: Non-renewable energy\n",
      "process_categories: unknown color FFFFFFFF at row 80: Electrical power use\n",
      "process_categories: unknown color FFE2F0D9 at row 81: Normalized electrical power use\n",
      "process_categories: unknown color FFFFFFFF at row 82: Electrical power use from non-renewables\n",
      "process_categories: unknown color FFE2F0D9 at row 83: Electrical power use from renewables\n",
      "process_categories: unknown color FFFFFFFF at row 84: Electrical power use from non-renewables\n",
      "process_categories: unknown color FFE2F0D9 at row 85: Electrical power use from renewables\n",
      "process_categories: new category set at row 86: Renewable electrical power use – by source (color = FF92D050)\n",
      "process_categories 86: segmenting Renewable electrical power use –::source\n",
      "pop at row 88: segmentation_stack now []\n",
      "process_categories: new topic set at row 88: Waste generated (color = FF9BDA44)\n",
      "process_topic 88: setting topic Waste\n",
      "process_topic 88: setting category Waste generated\n",
      "process_topic 88: setting units millions of lbs.\n",
      "process_categories: unknown color FFE2F0D9 at row 89: Non-hazardous waste generated\n",
      "process_categories: unknown color FFFFFFFF at row 90: Waste recycled\n",
      "process_categories: unknown color FFE2F0D9 at row 91: Waste incinerated\n",
      "process_categories: unknown color FFFFFFFF at row 92: Waste landfilled\n",
      "process_categories: unknown color FFE2F0D9 at row 93: Hazardous waste generated\n",
      "process_categories: unknown color FFFFFFFF at row 94: Normalized hazardous waste\n",
      "find units: nothing found for short_tons/mm$_revenue\n",
      "process_categories: new category set at row 95: Hazardous waste by business division (color = FF92D050)\n",
      "process_categories 95: segmenting Hazardous waste::business division\n",
      "process_segmentation: stack at end = [<Cell 'Environmental'.J95>]\n",
      "finish_notes @ 106\n",
      "wb(1) dataframe\n",
      "Environmental\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for filename in filename_magic:\n",
    "    print(filename)\n",
    "    crm = filename_magic[filename]\n",
    "    ingest_filename(filename)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b859ab-7d24-428b-8cc9-96e1ab09f2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3dff508-d294-4b92-94b6-8ea2c546df92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thin'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.cell(11,1).border.top.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b389459-bd1c-4bf0-b584-183e23c409f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Category</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Variable, Notes, Topic, Category, Segmentation, Unit, Year, Value]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Variable', 'Unit']\n",
    "# We need these columns to reshape our data\n",
    "for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "    if melted_df[extra_col].notna().any():\n",
    "        columns.append(extra_col)\n",
    "melted_df[melted_df['Segmentation']=='(anon)::Road transport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "460ecd19-48a4-4781-b96e-6dac13a6b6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.0000000000000002 gigaliter/year"
      ],
      "text/latex": [
       "$1.0000000000000002\\ \\frac{\\mathrm{gigaliter}}{\\mathrm{year}}$"
      ],
      "text/plain": [
       "1.0000000000000002 <Unit('gigaliter / year')>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = 1e6 * ureg('kl/year')\n",
    "units.to_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a72a405a-8d21-42f7-ac61-9b6c5f9e2011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-96b5f9fede15>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-96b5f9fede15>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    bletch!\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bletch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11503d-24dd-4d87-a57c-ca0355715485",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm = filename_magic['DPDHL-ESG-Statbook-2020-en.xlsx']\n",
    "wb = load_workbook(crm.input_filename, data_only=True)\n",
    "ws = wb.worksheets[2]\n",
    "preprocess(wb, ws)\n",
    "\n",
    "row = process_topic(pc, crm.topic_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d4733-e25f-4746-9fcf-270483363f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess2(wb, ws):\n",
    "    global crm\n",
    "    \n",
    "    scope1_gases = ['CO2', 'CH4', 'N2O', 'HFC', 'SF6', 'PFC', 'NF3', 'CO2e', 'NOx', 'SO2', 'PM10']\n",
    "    scope1_regex = re.compile('(' + ')|('.join(scope1_gases) + ')', flags=re.I)\n",
    "    \n",
    "    scope3_dict = { 'Purchased Goods and Services':1,\n",
    "                    'Capital Goods':2,\n",
    "                    'Fuel and Energy Related Activities':3,\n",
    "                    'Fuel and Energy Related Activities (Market-Based)':3,\n",
    "                    'Fuel and Energy Related Activities (Location-Based)':3,\n",
    "                    'Upstream Transportation and Distribution':4,\n",
    "                    'Transportation services':4,                # DPDHL\n",
    "                    'Fuel- and energy-related activities':4,    # DPDHL\n",
    "                    'Waste Generated in Operations (Large office campuses)':5,\n",
    "                    'Business Travel':6,\n",
    "                    'Employee Commuting':7,\n",
    "                    'Upstream Leased Assets':8,\n",
    "                    'Downstream Transportation and Distribution':9,\n",
    "                    'Processing of Sold Products':10,\n",
    "                    'Use of Sold Products':11,\n",
    "                    'End of Life Treatment of Sold Products':12,\n",
    "                    'Downstream Leads Assets':13,\n",
    "                    'Franchises':14,\n",
    "                    'Investments':15 }\n",
    "\n",
    "    # Convert reported units to things standard in `pint`\n",
    "    unit_dict = { 'trillion (10^12) MJ':'PJ', 'million MWh':'TWh', 'MW':'MW', \n",
    "                  'million tonnes CO2e':'Mt CO2e', 'tonnes CO2e':'t CO2e', 'm t CO2e':'Mt CO2e', 'MT CO2e':'Mt CO2e',\n",
    "                  'million tonnes':'Mt', 'thousand tonnes':'kilot', 'tonnes':'t', 'kg':'kg', 'MT':'Mt', 'Lbs':'lbs', 'Metric Tons':'t', \n",
    "                  'tBtu':'TBtu',\n",
    "                  'm liter':'M liter', 'Grams per € revenue':'Grams / EUR',\n",
    "                  'Millions of m3':'1000 dam', 'm3':'m3', 'Gallons':'gal',\n",
    "                  'Million Gallons':'M gallons', 'Billions of Liters':'10^9 l', 'billions of Liters':'10^9 l',}\n",
    "    u2u_dict = { '%':'pct', 'Grams per € revenue':'Grams / EUR', 'revenue':'EUR', 'MM$ revenue':'1000000 revenue',\n",
    "                 'short ton':'short_ton', 'No.':'[]', 'Nb':'[]', }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d95790-9edb-48ff-b5c3-fe4b2e37ca03",
   "metadata": {},
   "source": [
    "### Time for a Pint!\n",
    "\n",
    "See https://github.com/IAMconsortium/units/issues/9\n",
    "and https://github.com/openscm/openscm-units/issues/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9369e4-b14f-42cd-9b28-2f906092b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pint_pandas\n",
    "from openscm_units import unit_registry\n",
    "\n",
    "pint_pandas.PintType.ureg = u = unit_registry\n",
    "\n",
    "one_co2 = unit_registry(\"CO2\")\n",
    "print(one_co2)\n",
    "\n",
    "x = pd.DataFrame([[2.0,'Mt CO2']], columns=['Value', 'Unit'])\n",
    "print(x)\n",
    "x = x.astype({'Value': 'pint[Mt CO2]'})\n",
    "print(x.Value.pint.to('t CO2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39945448-35db-4179-ba8c-0d2cddb0e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u('Mt/1000000').to_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524e262-0708-41c9-9348-8815dbe0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = unit_registry\n",
    "Q_ = ureg.Quantity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31d04f51-689a-4022-aaa7-77206d273458",
   "metadata": {},
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = pint.UnitRegistry()\n",
    "Q_ = ureg.Quantity\n",
    "\n",
    "pint_pandas.PintType.ureg = ureg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bf3a7-bfca-4329-b503-0961841a52a8",
   "metadata": {},
   "source": [
    "Note that pint[unit] must be used for the Series constuctor, whereas the PintArray constructor allows the unit string or object.\n",
    "\n",
    "```\n",
    "    df = pd.DataFrame({\n",
    "        \"length\" : pd.Series([1.,2.], dtype=\"pint[m]\"),\n",
    "        \"width\" : PA_([2.,3.], dtype=\"pint[m]\"),\n",
    "        \"distance\" : PA_([2.,3.], dtype=\"m\"),\n",
    "        \"height\" : PA_([2.,3.], dtype=ureg.m),\n",
    "        \"depth\" : PA_.from_1darray_quantity(Q_([2,3],ureg.m)),\n",
    "    })\n",
    "```\n",
    "\n",
    "See https://pint.readthedocs.io/en/0.18/pint-pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07843c-4fb7-4fd1-b124-43acffea39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook(long_fmt_filename, data_only=True)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def long_ws_to_df(ws):\n",
    "    data = ws.values\n",
    "    cols = next(data)\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 1, None) for r in data)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "    # The original data has topic we construct.  It is removed when writing LONG data but can be restored from SHEET_NAME\n",
    "    if 'Topic' not in df.columns:\n",
    "        print('Restoring Topic ' + ws.title)\n",
    "        df.insert(crm.topic_col-1, 'Topic', ws.title)\n",
    "    \n",
    "    return df\n",
    "\n",
    "trino_df = pd.concat([long_ws_to_df(ws) for ws in wb.worksheets])\n",
    "    \n",
    "len(trino_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2879-1246-4faf-b229-b102dd971acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trino_df['Unit'].value_counts())\n",
    "trino_df.Unit.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075088-fa8f-446c-a377-1dc65cf34290",
   "metadata": {},
   "source": [
    "Now create data in Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8bb0c-a4b4-4028-9d27-8008582bbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937708e8-adcf-459e-8aec-d855c53a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956592b-0ce9-4716-963e-4e3823b17fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.now()\n",
    "# For now we used a fixed date so we don't fill things up needlessly\n",
    "timestamp = \"2008-09-03T20:56:35.450686Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1c7de-7caf-4772-a497-610394df13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'osc_corp_data'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447cdbd-4b5b-42b5-9386-bac8f52c8981",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75766c69-167c-4100-8811-ce8c49870759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(enforce_sql_column_names(df))\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}/{timestamp}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid,timestamp=timestamp)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5e83d-618d-43c0-aeb1-d0bc0706476d",
   "metadata": {},
   "source": [
    "### Write out Report with metadata\n",
    "\n",
    "Create the actual metadata for the source.  In this case, it is osc_corp_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a9c61-9180-446e-80b3-0cc95d34c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {}\n",
    "metadata_text = \"\"\"Title: AEP GHG and Energy Report, 2020\n",
    "Description: \n",
    "Version: 2020\n",
    "Release Date: \n",
    "URI: https://reports.shell.com/sustainability-report/2020/our-performance-data/greenhouse-gas-and-energy-data.html\n",
    "Copyright: \n",
    "License: \n",
    "Contact: \n",
    "Citation: \"\"\"\n",
    "\n",
    "for line in metadata_text.split('\\n'):\n",
    "    k, v = line.split(':', 1)\n",
    "    k = sql_compliant_name(k)\n",
    "    custom_meta_content[k] = v\n",
    "\n",
    "custom_meta_content['abstract'] = \"\"\"Abstract text\"\"\"\n",
    "custom_meta_content['name'] = 'osc_corp_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a3fe-4149-45e4-88b9-9bd3a7bd009d",
   "metadata": {},
   "source": [
    "Create the metadata for all the fields in all the tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd5046d-5237-42b9-9d40-508bfd177bcf",
   "metadata": {},
   "source": [
    "field_text = \"\"\"`country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [https://www.iso.org/iso-3166-country-codes.html]\n",
    "`country_long` (text): longer form of the country designation\n",
    "`name` (text): name or title of the power plant, generally in Romanized form\n",
    "`gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "`capacity_mw` (number): electrical generating capacity in megawatts\n",
    "`latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "`other_fuel1` (text): energy source used in electricity generation or export\n",
    "`other_fuel2` (text): energy source used in electricity generation or export\n",
    "`other_fuel3` (text): energy source used in electricity generation or export\n",
    "`commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "`owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "`source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "`url` (text): web document corresponding to the `source` field\n",
    "`geolocation_source` (text): attribution for geolocation information\n",
    "`wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "`year_of_capacity_data` (number): year the capacity information was reported\n",
    "`generation_data_source` (text): attribution for the reported generation information\"\"\"\n",
    "\n",
    "field_descs = [line.split(': ')[1] for line in field_text.split('\\n')]\n",
    "field_keys = [line.split(': ')[0].split(' ')[0][1:-1] for line in field_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566b9fd-7cfa-42da-a5f5-b0be69d0e85e",
   "metadata": {},
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a27bcc-cb92-481a-bf99-ba9ac9090a9c",
   "metadata": {},
   "source": [
    "custom_meta_fields = {}\n",
    "for k, v in zip(field_keys, field_descs):\n",
    "    custom_meta_fields[k] = { 'description': v }\n",
    "\n",
    "custom_meta_fields['capacity_mw']['dimension'] = 'MW'\n",
    "custom_meta_fields['latitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['longitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['commissioning_year']['dimension'] = 'year'\n",
    "custom_meta_fields['year_of_capacity_data']['dimension'] = 'year'\n",
    "custom_meta_fields['year'] = { 'description': 'year of report', 'dimension': 'year'}\n",
    "custom_meta_fields['gppd_idnr'] = { 'description': 'unique index into plants table', 'dimension': None}\n",
    "custom_meta_fields['generation_gwh'] = { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_gwh'] = { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_note'] = { 'description': 'label of the model/method used to estimate generation for the year', 'dimension': None }\n",
    "custom_meta_key_fields = 'metafields'\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'Global Power Plant Database',\n",
    "    'description': 'A comprehensive, global, open source database of power plants',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "}\n",
    "custom_meta_key = 'metaset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e1321-bfa5-49a8-975e-d88042985316",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722f0bd-eca3-4bbc-9406-26d6b09d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'aep_2020'\n",
    "custom_meta_fields = {}\n",
    "create_trino_pipeline (s3, schemaname, tablename, timestamp, shell_df, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b29ea-3636-4af6-8108-49f4d3c80cf4",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26c30d50-88f0-4f41-b1f8-1dffc87c21b1",
   "metadata": {},
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd177b-4035-43ee-a8bf-6b8500be0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6e28-686b-4540-bc9f-83b44cba3d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c7bbc-c00f-41e8-85b0-6b553e73a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'aep_2020',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b86eb-aebb-445d-b125-197b7f73a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(iam_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e01667-a1a2-4752-b756-dad449a8853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb585901-7a49-4bf2-aa6e-732ba4a8e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
