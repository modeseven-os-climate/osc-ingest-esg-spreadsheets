{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0ebfc5-d93a-4731-a1a5-319b73accd2a",
   "metadata": {},
   "source": [
    "## Load 2020 WIDE-formatted ESG data (Shell)\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Initially developed using the Royal Dutch Shell plc Sustainability Report 2020 report (Many Sheets)\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691f183-a756-45e1-9348-1156044dee52",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dff22-a393-47e4-bcaa-3d6c992ab083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee67533-80c2-4594-8e78-c864adddf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from itertools import islice\n",
    "\n",
    "import pint\n",
    "import pint_pandas\n",
    "\n",
    "from osc_ingest_trino import *\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import io\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59df89e-d31d-4f2e-a5f2-e0d696f9e7c3",
   "metadata": {},
   "source": [
    "For spreadsheets in WIDE format, pre-process the spreadsheet as a workbook, cascading label data into 3rd-normal form row and column metadata\n",
    "\n",
    "* var_col is the label of the variable being measured (whose specificity (like CO2, CH4, NOx, etc) often affects units)\n",
    "* units_col is the column where units are stated\n",
    "* val_col is the column where the first value is quantitatively reported\n",
    "\n",
    "We add:\n",
    "* notes_col (source worksheet-specific; could act as a kind of source table metadata)\n",
    "* topic_col (sheet-level category; if we wanted large tables, they could be named by topic)\n",
    "* category_col (to which row-level data rolls up; if we wanted small tables, they could be named by topic:category)\n",
    "* segment_col (the dimension by which row-level data is segmented)\n",
    "\n",
    "Some spreadsheets use color to express a multi-level category hierarchy (such as Energy Consumption>>Business Use>>Fuel Type).  We concatenate the categories from left to right as the category for our purposes, except we split off the rightmost subcategory as the segmentation.\n",
    "\n",
    "Based on all of the above, we don't really have table-level metadata other than notes attached to sheets and generic column information.  An argument could be made that we need to allocate specifier columns for additional data we want to split out from our variables.  That could look like:\n",
    "\n",
    "* spec1_col\n",
    "* spec2_col\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29d491-2424-40f8-af42-a5fea5648dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Magic knowledge\n",
    "var_col = 1\n",
    "\n",
    "ingest_columns = [ 'Variable', 'Notes', 'Topic', 'Category', 'Segmentation' ]\n",
    "ingest_dict = dict((j,i) for i,j in enumerate(ingest_columns[1:], start=var_col+1))\n",
    "notes_col = ingest_dict['Notes']\n",
    "topic_col = ingest_dict['Topic']\n",
    "category_col = ingest_dict['Category']\n",
    "segmentation_col = ingest_dict['Segmentation']\n",
    "\n",
    "# Magic knowledge\n",
    "units_col = segmentation_col+1   # units_col starts as var_col+1\n",
    "val_col = segmentation_col+2     # val_col starts as var_col+2\n",
    "topic_row = 3\n",
    "header_row = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1766f7-ded7-46e3-863d-b3ac358865f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all this complexity to find fill colors, if any, for discerning additional category hierarchies (which we flatten by concatenation).\n",
    "\n",
    "from colorsys import rgb_to_hls, hls_to_rgb\n",
    "# From: https://stackoverflow.com/questions/58429823/getting-excel-cell-background-themed-color-as-hex-with-openpyxl/58443509#58443509\n",
    "#   which refers to: https://pastebin.com/B2nGEGX2 (October 2020)\n",
    "#       Updated to use list(elem) instead of the deprecated elem.getchildren() method\n",
    "#       which has now been removed completely from Python 3.9 onwards.\n",
    "#\n",
    "\n",
    "#https://bitbucket.org/openpyxl/openpyxl/issues/987/add-utility-functions-for-colors-to-help\n",
    "\n",
    "RGBMAX = 0xff  # Corresponds to 255\n",
    "HLSMAX = 240  # MS excel's tint function expects that HLS is base 240. see:\n",
    "# https://social.msdn.microsoft.com/Forums/en-US/e9d8c136-6d62-4098-9b1b-dac786149f43/excel-color-tint-algorithm-incorrect?forum=os_binaryfile#d3c2ac95-52e0-476b-86f1-e2a697f24969\n",
    "\n",
    "def rgb_to_ms_hls(red, green=None, blue=None):\n",
    "    \"\"\"Converts rgb values in range (0,1) or a hex string of the form '[#aa]rrggbb' to HLSMAX based HLS, (alpha values are ignored)\"\"\"\n",
    "    if green is None:\n",
    "        if isinstance(red, str):\n",
    "            if len(red) > 6:\n",
    "                red = red[-6:]  # Ignore preceding '#' and alpha values\n",
    "            blue = int(red[4:], 16) / RGBMAX\n",
    "            green = int(red[2:4], 16) / RGBMAX\n",
    "            red = int(red[0:2], 16) / RGBMAX\n",
    "        else:\n",
    "            red, green, blue = red\n",
    "    h, l, s = rgb_to_hls(red, green, blue)\n",
    "    return (int(round(h * HLSMAX)), int(round(l * HLSMAX)), int(round(s * HLSMAX)))\n",
    "\n",
    "def ms_hls_to_rgb(hue, lightness=None, saturation=None):\n",
    "    \"\"\"Converts HLSMAX based HLS values to rgb values in the range (0,1)\"\"\"\n",
    "    if lightness is None:\n",
    "        hue, lightness, saturation = hue\n",
    "    return hls_to_rgb(hue / HLSMAX, lightness / HLSMAX, saturation / HLSMAX)\n",
    "\n",
    "def rgb_to_hex(red, green=None, blue=None):\n",
    "    \"\"\"Converts (0,1) based RGB values to a hex string 'rrggbb'\"\"\"\n",
    "    if green is None:\n",
    "        red, green, blue = red\n",
    "    return ('%02x%02x%02x' % (int(round(red * RGBMAX)), int(round(green * RGBMAX)), int(round(blue * RGBMAX)))).upper()\n",
    "\n",
    "\n",
    "def get_theme_colors(wb):\n",
    "    \"\"\"Gets theme colors from the workbook\"\"\"\n",
    "    # see: https://groups.google.com/forum/#!topic/openpyxl-users/I0k3TfqNLrc\n",
    "    from openpyxl.xml.functions import QName, fromstring\n",
    "    xlmns = 'http://schemas.openxmlformats.org/drawingml/2006/main'\n",
    "    root = fromstring(wb.loaded_theme)\n",
    "    themeEl = root.find(QName(xlmns, 'themeElements').text)\n",
    "    colorSchemes = themeEl.findall(QName(xlmns, 'clrScheme').text)\n",
    "    firstColorScheme = colorSchemes[0]\n",
    "\n",
    "    colors = []\n",
    "\n",
    "    for c in ['lt1', 'dk1', 'lt2', 'dk2', 'accent1', 'accent2', 'accent3', 'accent4', 'accent5', 'accent6']:\n",
    "        accent = firstColorScheme.find(QName(xlmns, c).text)\n",
    "        for i in list(accent): # walk all child nodes, rather than assuming [0]\n",
    "            if 'window' in i.attrib['val']:\n",
    "                colors.append(i.attrib['lastClr'])\n",
    "            else:\n",
    "                colors.append(i.attrib['val'])\n",
    "\n",
    "    return colors\n",
    "\n",
    "def tint_luminance(tint, lum):\n",
    "    \"\"\"Tints a HLSMAX based luminance\"\"\"\n",
    "    # See: http://ciintelligence.blogspot.co.uk/2012/02/converting-excel-theme-color-and-tint.html\n",
    "    if tint < 0:\n",
    "        return int(round(lum * (1.0 + tint)))\n",
    "    else:\n",
    "        return int(round(lum * (1.0 - tint) + (HLSMAX - HLSMAX * (1.0 - tint))))\n",
    "\n",
    "def theme_and_tint_to_rgb(wb, theme, tint):\n",
    "    \"\"\"Given a workbook, a theme number and a tint return a hex based rgb\"\"\"\n",
    "    rgb = get_theme_colors(wb)[theme]\n",
    "    h, l, s = rgb_to_ms_hls(rgb)\n",
    "    return rgb_to_hex(ms_hls_to_rgb(h, tint_luminance(tint, l), s))\n",
    "\n",
    "# ??? The header row color is going to be spreadsheet-specific.  This is what DPDHL gives us.\n",
    "\n",
    "def find_header_row(wb, ws):\n",
    "    # If we haven't found the header by max_row-1, we'll never find it...\n",
    "    for row in range(1, ws.max_row):\n",
    "        cell = ws.cell(row,1)\n",
    "        if cell.fill.fgColor.type == 'rgb':\n",
    "            if cell.fill.fgColor.rgb == 'FFBF00':\n",
    "                return row\n",
    "            continue\n",
    "        theme = cell.fill.start_color.theme\n",
    "        tint = cell.fill.start_color.tint\n",
    "        color = theme_and_tint_to_rgb(wb, theme, tint)\n",
    "        if color=='FFBF00':\n",
    "            return row\n",
    "    print('No header found')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f389bc8-f1f7-421c-9d3f-b35afab04605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pre-process the structure of the worksheet so that it can be trivially loaded into a dataframe for further reshaping.\n",
    "\n",
    "# Stash notes for each worksheet here.  These are *per worksheet*\n",
    "# ??? In the case of DPDHL, there's a Comment field we don't track, which means we miss a stated target\n",
    "ws_notes = {}\n",
    "    \n",
    "\n",
    "def preprocess(wb, ws):\n",
    "    \n",
    "    # Intended for Shell notes\n",
    "    def save_ws_notes(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        note_label, note_text = note.split(' ', 1)\n",
    "        ws_notes[ws.title][note_label] = note_text.strip()\n",
    "    \n",
    "    # Intended for DPDHL notes\n",
    "    def save_ws_notes2(ws, note):\n",
    "        global ws_notes\n",
    "        \n",
    "        if ws.title not in ws_notes:\n",
    "            ws_notes[ws.title] = {}\n",
    "        notes = re.split(r' (\\d+)\\)\\s+')\n",
    "        print('NOTES')\n",
    "        print(notes)\n",
    "        print('END NOTES')\n",
    "        for i in range(len(notes)/2):\n",
    "            ws_notes[ws.title][notes[2*i]] = notes[2*i+1].strip()\n",
    "    \n",
    "    scope1_gases = ['CO2', 'CH4', 'N2O', 'HFC', 'SF6', 'PFC', 'NF3', 'CO2e', 'NOx', 'SO2', 'PM10']\n",
    "    scope1_regex = re.compile('(' + ')|('.join(scope1_gases) + ')', flags=re.I)\n",
    "    \n",
    "    scope3_dict = { 'Purchased Goods and Services':1,\n",
    "                    'Capital Goods':2,\n",
    "                    'Fuel and Energy Related Activities':3,\n",
    "                    'Fuel and Energy Related Activities (Market-Based)':3,\n",
    "                    'Fuel and Energy Related Activities (Location-Based)':3,\n",
    "                    'Upstream Transportation and Distribution':4,\n",
    "                    'Transportation services':4,                # DPDHL\n",
    "                    'Fuel- and energy-related activities':4,    # DPDHL\n",
    "                    'Waste Generated in Operations (Large office campuses)':5,\n",
    "                    'Business Travel':6,\n",
    "                    'Employee Commuting':7,\n",
    "                    'Upstream Leased Assets':8,\n",
    "                    'Downstream Transportation and Distribution':9,\n",
    "                    'Processing of Sold Products':10,\n",
    "                    'Use of Sold Products':11,\n",
    "                    'End of Life Treatment of Sold Products':12,\n",
    "                    'Downstream Leads Assets':13,\n",
    "                    'Franchises':14,\n",
    "                    'Investments':15 }\n",
    "\n",
    "    def normalize_scope3(s3):\n",
    "        # Later we should normalize against the scope3_dict\n",
    "        return s3\n",
    "    \n",
    "    # The role of this function is to capture and distrbute data attributes that can be inferred/applied to subsequent rows\n",
    "    def split_header(c):\n",
    "        notes = ''\n",
    "        \n",
    "        # Deal with None\n",
    "        if c.value:\n",
    "            h = str(c.value)\n",
    "        else:\n",
    "            h = ''\n",
    "        \n",
    "        # Look for superscripts that openpxyl has parsed and disposed of before we can see them\n",
    "        m = re.search(r'^20\\d\\d', h)\n",
    "        if m:\n",
    "            notes = h[4:]\n",
    "            return notes, m.group(0), ''\n",
    "        \n",
    "        # Don't let 'Scope 1' look like a note\n",
    "        m = re.search(r'[^ ](\\d+)$', h)\n",
    "        if m:\n",
    "            notes = m.group(1)\n",
    "            h = h[0:m.start(1)]\n",
    "        else:\n",
    "            m = re.search(r'\\[.*\\]', h)\n",
    "            if m:\n",
    "                notes = m.group(0)\n",
    "                h = h.replace(notes,'').strip()\n",
    "        \n",
    "        # If the variable expresses a segmentation, pass that back accordingly\n",
    "        for x in [ ' per ', ' by ', ' of ' ]:\n",
    "            sub_h_arr = h.split(x, 1)\n",
    "            if len(sub_h_arr)>1:\n",
    "                return notes, sub_h_arr[0], sub_h_arr[1]\n",
    "        \n",
    "        # Treat X (Y) as 'Category X Segmentation Y'\n",
    "        m = re.search(r'^(.*) \\((.*)\\)', h)\n",
    "        if m:\n",
    "            return notes, m.group(1), m.group(2)\n",
    "        return notes, h, ''\n",
    "    \n",
    "    # Convert reported units to things standard in `pint`\n",
    "    unit_dict = { 'trillion (10^12) MJ':'PJ', 'million MWh':'TWh', 'million tonnes CO2e': 'Mt CO2e',\n",
    "                 'million tonnes': 'Mt', 'thousand tonnes': 'kilot', 'tonnes': 't', 'tBtu':'TBtu',\n",
    "                  'm t CO2e':'Mt CO2e', 'm liter':'M liter', 'Grams per € revenue': 'Grams / EUR' }\n",
    "    def normalize_units(u, g):\n",
    "        if g in u:\n",
    "            g = ''\n",
    "        if '/' in u:\n",
    "            u1, u2 = u.split('/', 1)\n",
    "            if g in u2:\n",
    "                g1 = ''\n",
    "                g2 = g\n",
    "            else:\n",
    "                g1 = g\n",
    "                g2 = ''\n",
    "            return ' / '.join([normalize_units(u1, g1), normalize_units(u2, g2)])\n",
    "        u = u.strip()\n",
    "        if u in unit_dict:\n",
    "            return normalize_units(unit_dict[u], g)\n",
    "        if g:\n",
    "            return ' '.join([u, g])\n",
    "        return u\n",
    "    \n",
    "    def crop_sheet(ws):\n",
    "        # Frist, set max_row/max_column based on actually active cells, not cells with random spaces or empty strings\n",
    "        this_max_row = 1\n",
    "        this_max_col = 1\n",
    "        for row in range(1,ws.max_row+1):\n",
    "            for col in range(1,ws.max_column+1):\n",
    "                cell = ws.cell(row,col)\n",
    "                if cell.value==None:\n",
    "                    continue\n",
    "                if type(cell.value)==str and cell.value.strip()=='':\n",
    "                    cell.value = None\n",
    "                    continue\n",
    "                if row > this_max_row:\n",
    "                    this_max_row = row\n",
    "                if col > this_max_col:\n",
    "                    this_max_col = col\n",
    "        ws.delete_rows(this_max_row+1,ws.max_row)\n",
    "        ws.delete_cols(this_max_col+1,ws.max_column)\n",
    "    \n",
    "    crop_sheet(ws)\n",
    "\n",
    "    # Make space for TOPIC : CATEGORY : SEGMENTATION triple.\n",
    "    # This triple could very well become an index into a data framework (such as SASB, TCFD, etc)\n",
    "    ws.insert_cols(var_col+1,amount=segmentation_col-var_col)\n",
    "    if True:\n",
    "        # All Shell headers start in the smae place\n",
    "        header_row = 5\n",
    "    else:\n",
    "        header_row = find_header_row (wb, ws)\n",
    "\n",
    "    for col in range(var_col, units_col):\n",
    "        ws.cell(header_row, col).value = ingest_columns[col-var_col]\n",
    "    # Find notes in header line (such as a callout that measurement systems changed in a particular year)\n",
    "    for col in range(units_col, ws.max_column+1):\n",
    "        notes, main_text, segmentation = split_header(ws.cell(header_row, col))\n",
    "        if notes:\n",
    "            # main_text has the note removed from it\n",
    "            ws.cell(header_row, col).value = main_text\n",
    "            if segmentation:\n",
    "                print('found note, but lost this: ' + segmentation)\n",
    "    \n",
    "    topic = ws.cell(topic_row,var_col).value\n",
    "    \n",
    "    notes = ''\n",
    "    category = ''\n",
    "    # DPDHL expresses a 3-level cat hierarchy via bold plus dark green, light green, and white backgrounds.  Also Grays for \"Other Env. Data\"\n",
    "    cat_color_dict = { None:0, 'FF00B050':0, 'E2F0D9':1, '00000000':2, 'D0CECE':0, 'E7E6E6':0 }\n",
    "    categories = ['', '', '']\n",
    "    segmentation = ''\n",
    "    scope1_gas = ''\n",
    "    \n",
    "    # Make the inferences, filling out TOPIC : CATEGORY : SEGMENTATION, as well as inferring/adjusting UNITS\n",
    "    for row in range(header_row+1, ws.max_row+1):\n",
    "        cell = ws.cell(row, var_col)\n",
    "        ws.cell(row, topic_col).value = topic\n",
    "        if cell.value==None:\n",
    "            continue\n",
    "        # Find either bracketed note or note that begins with possible superscript\n",
    "        if cell.value[0]=='[':\n",
    "            save_ws_notes(ws, cell.value)\n",
    "            continue\n",
    "        elif re.search(r'^[^(]*\\d[)]', str(cell.value)):\n",
    "            save_ws_notes2(ws, cell.value)\n",
    "        \n",
    "        # *BOLD* text indicates we have a header to parse\n",
    "        if cell.font.b:\n",
    "            if True:\n",
    "                # Shell doesn't use colors\n",
    "                cat_color = None\n",
    "            elif cell.fill.fgColor.type == 'rgb':\n",
    "                cat_color = format(cell.fill.fgColor.rgb)\n",
    "            else:\n",
    "                theme = cell.fill.start_color.theme\n",
    "                tint = cell.fill.start_color.tint\n",
    "                cat_color = theme_and_tint_to_rgb(wb, theme, tint)\n",
    "\n",
    "            notes, category, segmentation = split_header(cell)\n",
    "            categories[cat_color_dict[cat_color]] = re.sub(r' total\\s?', '', category)\n",
    "            for i in range(cat_color_dict[cat_color]+1, len(categories)):\n",
    "                categories[i] = ''\n",
    "            category = ':'.join([c for c in categories[0:2] if c])\n",
    "            if re.search(r'Scope\\s*3', category, flags=re.I):\n",
    "                m = re.search(r'Scope\\s+3 (emissions )(by.*categor((y)|(ies)))?', category, flags=re.I)\n",
    "                category = 'Scope 3 emissions'\n",
    "                segmentation = 'GHG Categories'\n",
    "            else:\n",
    "                if categories[2]:\n",
    "                    if segmentation:\n",
    "                        print('cat[2] = {}; segmentation = {}'.format(categories[2], segmentation))\n",
    "                    segmentation = categories[2]\n",
    "            \n",
    "            # If we have no units, borrow from following row\n",
    "            if ws.cell(row, units_col).value==None:\n",
    "                ws.cell(row, units_col).value = ws.cell(row+1, units_col).value\n",
    "            # If there is no disclosure here, move on with the notes/category/segmentation we've captured\n",
    "            if ws.cell(row, val_col).value==None:\n",
    "                continue\n",
    "            \n",
    "            if category == 'Scope 3 emissions' and segmentation == 'GHG Categories':\n",
    "                ws.cell(row, var_col).value = normalize_scope3 (ws.cell(row, var_col).value)\n",
    "            \n",
    "        # Try to get units and category from variable description.  In the Shell case, which mostly fills in units,\n",
    "        # we try vary hard to fill in empty units with parenthetical expressions.  In DPDHL case, which mostly does not,\n",
    "        # we want to infer more from previous rows or cascade a defined unit from the current row to subsequent rows\n",
    "        if ws.cell(row, units_col).value==None:\n",
    "            maybe_notes, maybe_category, maybe_var_units = split_header(cell)\n",
    "            if type(ws.cell(row, units_col)) == openpyxl.cell.cell.Cell:\n",
    "                # Don't try to update a MergedCell\n",
    "                maybe_var_units = re.sub(r'\\((.*)\\)', r'\\1', maybe_var_units)\n",
    "                if maybe_var_units in unit_dict:\n",
    "                    units = maybe_var_units\n",
    "                ws.cell(row, units_col).value = units\n",
    "            else:\n",
    "                notes, category = maybe_notes, maybe_category\n",
    "                if maybe_var_units:\n",
    "                    units = maybe_var_units\n",
    "                print('Merged cell: {} {}'.format(row, units_col))\n",
    "        # Now fill the empty columns we created with the metadata we have inferred\n",
    "        ws.cell(row, notes_col).value = notes\n",
    "        ws.cell(row, category_col).value = category\n",
    "        ws.cell(row, segmentation_col).value = segmentation\n",
    "        if 'emissions' in category.lower() and not re.search('CO2e', ws.cell(row, units_col).value, re.I):\n",
    "            m = re.search(scope1_regex, str(ws.cell(row, var_col).value))\n",
    "            if m:\n",
    "                scope1_gas = m.group(0)\n",
    "            # else it carries forward\n",
    "        else:\n",
    "            scope1_gas = ''\n",
    "        if ws.cell(row, units_col).value!=None:\n",
    "            units = normalize_units(ws.cell(row, units_col).value, scope1_gas)\n",
    "            ws.cell(row, units_col).value = units"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8638a459-01d5-439f-8172-5eb922554f81",
   "metadata": {},
   "source": [
    "filename = os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/raw/greenhouse-gas-and-energy-data-shell-sr20.xlsx'\n",
    "wb = load_workbook(filename, data_only=True)\n",
    "ws = wb.worksheets[8]\n",
    "\n",
    "if True:\n",
    "    preprocess(wb,ws)\n",
    "    header_row_list = [ -1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ]\n",
    "    data = islice(ws.values, header_row_list[8]-1, None)\n",
    "    cols = list(next(data))\n",
    "    data = list(data)\n",
    "    cols[units_col-1] = 'Unit'            # Already set by Shell\n",
    "    df = pd.DataFrame(data, columns=cols) # we don't pass in an index here\n",
    "\n",
    "    df.replace(to_replace='^n/[acd]$', value='', regex=True, inplace=True)\n",
    "\n",
    "    df[[c for c in df.columns if c!=None]].columns\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    df.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa4190-7ea4-4f5b-afab-cc055c0d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a nicely formatted workbook, do the rest of our work (including writing to Trino) using dataframes\n",
    "\n",
    "# IPIECA, SASB, and GRI columns all feed metadata\n",
    "\n",
    "header_row_list = [ -1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ]\n",
    "\n",
    "def shell_ws_to_df(wb, i):\n",
    "    data = islice(wb.worksheets[i].values, header_row_list[i]-1, None)\n",
    "    cols = list(next(data))\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 0, None) for r in data)\n",
    "    cols[units_col-1] = 'Unit'            # Already set by Shell\n",
    "    df = pd.DataFrame(data, columns=cols) # we don't pass in an index here\n",
    "\n",
    "    # df.dropna(subset=['Unit'], inplace=True)\n",
    "    df.replace(to_replace='^n/[acd]$', value='', regex=True, inplace=True)\n",
    "    \n",
    "    # Remove null columns\n",
    "    df = df[[c for c in df.columns if c!= None]]\n",
    "    # Change numerical years to strings to make pandas indexing behave\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    # Drop completely empty rows\n",
    "    df.dropna(how='all', axis=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63095f7-55e2-4eef-b1d4-379fcf9ebaba",
   "metadata": {},
   "source": [
    "Write out polymorphic dataframe in LONG format.  This follows tidy data model, with one variable observation per row.  \n",
    "Polymorphic means that Units/dimensions of each row are specified, but not necessarily the same row to row.  \n",
    "Aggregation functions must be careful that selection criteria does not mix up incompatible unit types and/or observation variables."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a289eea2-0acb-4ed1-aa07-c1fa3f0fce1e",
   "metadata": {},
   "source": [
    "filename = os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/raw/greenhouse-gas-and-energy-data-shell-sr20.xlsx'\n",
    "wb = load_workbook(filename)\n",
    "ws = wb.worksheets[8]\n",
    "ws_notes = {}\n",
    "preprocess(wb,ws)\n",
    "# What to do with ws_notes???\n",
    "df = shell_ws_to_df(wb, 8)\n",
    "df.replace('',pd.NA,inplace=True)\n",
    "melted_df = pd.melt(df, id_vars=ingest_columns + ['Unit'], var_name='Year', value_name='Value', value_vars=['2016', '2017', '2018', '2019', '2020'])\n",
    "# melted_df.loc[:, melted_df.columns != 'Topic'].to_excel(writer_long, sheet_name=df.iloc[0]['Topic'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffed37-1d28-4038-8390-d2f5ec91f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/raw/greenhouse-gas-and-energy-data-shell-sr20.xlsx'\n",
    "wb = load_workbook(filename)\n",
    "\n",
    "# For a label like \"Scope 1 emissions by country\" return ['', 'Scope 1 emissions', 'country']\n",
    "# For a label like \"Direct GHG emissions (Scope 1) [A] [B] [C] [D]\" return ['[A] [B] [C] [D]', 'Direct GHG emissions (Scope 1) ', '']\n",
    "\n",
    "with pd.ExcelWriter(os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/processed/Shell_2020_LONG.xlsx') as writer_long:\n",
    "    with pd.ExcelWriter(os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/processed/Shell_2020_WIDE.xlsx') as writer_wide:\n",
    "        # We skip the first sheet as it's just a table of contents\n",
    "        for i in range(1, len(wb.worksheets)):\n",
    "            ws = wb.worksheets[i]\n",
    "            ws_notes = {}\n",
    "            preprocess(wb, ws)\n",
    "            # What to do with ws_notes???\n",
    "            df = shell_ws_to_df(wb, i)\n",
    "            df.replace('',pd.NA,inplace=True)\n",
    "            melted_df = pd.melt(df, id_vars=ingest_columns+['Unit'], var_name='Year', value_name='Value', value_vars=['2016', '2017', '2018', '2019', '2020'])\n",
    "            melted_df.dropna(subset=['Value'],inplace=True)\n",
    "            melted_df = melted_df.astype({'Year': 'int'})\n",
    "            # This writes out LONG data with TOPIC as SHEET_NAME.  Later we'll create a truly long table with TOPIC restored as a column\n",
    "            melted_df.loc[:, melted_df.columns != 'Topic'].to_excel(writer_long, index=False, sheet_name=df.iloc[0]['Topic'][0:30])\n",
    "\n",
    "            print(ws.title)\n",
    "            columns = ['Variable', 'Unit']\n",
    "            # We need these columns to reshape our data\n",
    "            for extra_col in ['Notes', 'Category', 'Segmentation']:\n",
    "                if df[extra_col].notna().any():\n",
    "                    columns.append(extra_col)\n",
    "            # In the case of Shell, we have only one topic per sheet, so can transform melted_df directly\n",
    "            pf = melted_df.pivot(index=['Year', 'Topic'], columns=columns, values=['Value'])\n",
    "            pf = pf.droplevel('Topic')\n",
    "            # Once reshaped, the extra columns actually appear as multi-level indexes.  Drop them from also behaving like values\n",
    "            pf[[c for c in columns if c not in ['Variable', 'Unit']]] = pd.NA\n",
    "            pf.dropna(how='all', axis=1, inplace=True)\n",
    "            pf.to_excel(writer_wide, sheet_name=df.iloc[0]['Topic'][0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f22a99-537c-408a-928a-4886b2e7453d",
   "metadata": {},
   "source": [
    "Make the workbook more legible to those reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d17e9-f79e-4c30-bff8-ad46c95dd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\n",
    "from openpyxl.utils import get_column_letter\n",
    "long_filename = os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/processed/Shell_2020_LONG.xlsx'\n",
    "\n",
    "wb = load_workbook(long_filename)\n",
    "\n",
    "# We are now working with our own workbook, which doesn't have a zero-index sheet to ignore\n",
    "for ws in wb.worksheets:\n",
    "    dim_holder = DimensionHolder(worksheet=ws)\n",
    "    for col in range(ws.min_column, ws.max_column + 1):\n",
    "        if get_column_letter(col)=='A':\n",
    "            width = 40\n",
    "        elif get_column_letter(col) in ['B', 'E']:\n",
    "            width = 15\n",
    "        elif get_column_letter(col) in ['C', 'D']:\n",
    "            width = 25\n",
    "        else:\n",
    "            width = 10\n",
    "        dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=width)\n",
    "    ws.column_dimensions = dim_holder\n",
    "\n",
    "wb.save(long_filename)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393047aa-32c1-4d1b-94ce-2b1b35122873",
   "metadata": {},
   "source": [
    "Write out dataframe in WIDE format.  This data is technically tiday, with one multi-dimensional observation per row.  Units/dimensions are consistent on a per-column basis, making it easy to aggregate column-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dcc5a-1818-4b3c-8dc8-b52674ee60a5",
   "metadata": {},
   "source": [
    "Make the workbook more legible to those reading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d481a-4fa4-4abf-8278-2ed2aef5d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment, Font\n",
    "\n",
    "def as_text(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value)\n",
    "\n",
    "wide_filename = os.environ.get('PWD', '/opt/app-root/src') + '/osc-ingest-shell/data/processed/Shell_2020_WIDE.xlsx'\n",
    "\n",
    "wb = load_workbook(wide_filename)\n",
    "\n",
    "for ws in wb.worksheets:\n",
    "    dim_holder = DimensionHolder(worksheet=ws)\n",
    "    for col in range(ws.min_column, ws.max_column + 1):\n",
    "        cell = ws.cell(2, col)\n",
    "        cell.alignment = Alignment(wrap_text=True,vertical='top') \n",
    "        dim_holder[get_column_letter(col)] = ColumnDimension(ws, min=col, max=col, width=max(10,1+len(as_text(cell.value))/3))\n",
    "    ws.column_dimensions = dim_holder\n",
    "\n",
    "wb.save(wide_filename)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d95790-9edb-48ff-b5c3-fe4b2e37ca03",
   "metadata": {},
   "source": [
    "### Time for a Pint!\n",
    "\n",
    "See https://github.com/IAMconsortium/units/issues/9https://github.com/IAMconsortium/units/issues/9\n",
    "and https://github.com/openscm/openscm-units/issues/31https://github.com/openscm/openscm-units/issues/31\n",
    "and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9369e4-b14f-42cd-9b28-2f906092b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pint_pandas\n",
    "from openscm_units import unit_registry\n",
    "\n",
    "pint_pandas.PintType.ureg = unit_registry\n",
    "\n",
    "one_co2 = unit_registry(\"CO2\")\n",
    "print(one_co2)\n",
    "\n",
    "x = pd.DataFrame([[2.0,'Mt CO2']], columns=['Value', 'Unit'])\n",
    "print(x)\n",
    "x = x.astype({'Value': 'pint[Mt CO2]'})\n",
    "print(x.Value.pint.to('t CO2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524e262-0708-41c9-9348-8815dbe0a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = unit_registry\n",
    "Q_ = ureg.Quantity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31d04f51-689a-4022-aaa7-77206d273458",
   "metadata": {},
   "source": [
    "PA_ = pint_pandas.PintArray\n",
    "\n",
    "ureg = pint.UnitRegistry()\n",
    "Q_ = ureg.Quantity\n",
    "\n",
    "pint_pandas.PintType.ureg = ureg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bf3a7-bfca-4329-b503-0961841a52a8",
   "metadata": {},
   "source": [
    "Note that pint[unit] must be used for the Series constuctor, whereas the PintArray constructor allows the unit string or object.\n",
    "\n",
    "```\n",
    "    df = pd.DataFrame({\n",
    "        \"length\" : pd.Series([1.,2.], dtype=\"pint[m]\"),\n",
    "        \"width\" : PA_([2.,3.], dtype=\"pint[m]\"),\n",
    "        \"distance\" : PA_([2.,3.], dtype=\"m\"),\n",
    "        \"height\" : PA_([2.,3.], dtype=ureg.m),\n",
    "        \"depth\" : PA_.from_1darray_quantity(Q_([2,3],ureg.m)),\n",
    "    })\n",
    "```\n",
    "\n",
    "See https://pint.readthedocs.io/en/0.18/pint-pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07843c-4fb7-4fd1-b124-43acffea39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook(long_filename)\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def long_ws_to_df(ws):\n",
    "    data = ws.values\n",
    "    cols = next(data)\n",
    "    data = list(data)\n",
    "    # idx = [r[0] for r in data]\n",
    "    # data = (islice(r, 1, None) for r in data)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "    # The original data has topic we construct.  It is removed when writing LONG data but can be restored from SHEET_NAME\n",
    "    if 'Topic' not in df.columns:\n",
    "        print('Restoring Topic ' + ws.title)\n",
    "        df.insert(topic_col-1, 'Topic', ws.title)\n",
    "    \n",
    "    return df\n",
    "\n",
    "shell_df = pd.concat([long_ws_to_df(ws) for ws in wb.worksheets])\n",
    "    \n",
    "len(shell_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2879-1246-4faf-b229-b102dd971acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(shell_df['Unit'].value_counts())\n",
    "shell_df.Unit.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68075088-fa8f-446c-a377-1dc65cf34290",
   "metadata": {},
   "source": [
    "Now create data in Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8bb0c-a4b4-4028-9d27-8008582bbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client.  We will user later when we write out data and metadata\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937708e8-adcf-459e-8aec-d855c53a07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956592b-0ce9-4716-963e-4e3823b17fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.now()\n",
    "# For now we used a fixed date so we don't fill things up needlessly\n",
    "timestamp = \"2008-09-03T20:56:35.450686Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1c7de-7caf-4772-a497-610394df13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_uuid = str(uuid.uuid4())\n",
    "\n",
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "schemaname = 'osc_corp_data'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447cdbd-4b5b-42b5-9386-bac8f52c8981",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75766c69-167c-4100-8811-ce8c49870759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(enforce_sql_column_names(df))\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, '/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp))\n",
    "    # df.to_parquet('/tmp/{sname}.{tname}.{uuid}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, index=False))\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp),\n",
    "        Filename='/tmp/{sname}.{tname}.{uuid}.{timestamp}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    print('dropping table: ' + tablename)\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/{uuid}/{timestamp}'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename,uuid=ingest_uuid,timestamp=timestamp)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5e83d-618d-43c0-aeb1-d0bc0706476d",
   "metadata": {},
   "source": [
    "### Write out Report with metadata\n",
    "\n",
    "Create the actual metadata for the source.  In this case, it is osc_corp_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a9c61-9180-446e-80b3-0cc95d34c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_meta_content = {}\n",
    "metadata_text = \"\"\"Title: Shell GHG and Energy Report, 2020\n",
    "Description: \n",
    "Version: 2020\n",
    "Release Date: \n",
    "URI: https://reports.shell.com/sustainability-report/2020/our-performance-data/greenhouse-gas-and-energy-data.html\n",
    "Copyright: \n",
    "License: \n",
    "Contact: \n",
    "Citation: \"\"\"\n",
    "\n",
    "for line in metadata_text.split('\\n'):\n",
    "    k, v = line.split(':', 1)\n",
    "    k = sql_compliant_name(k)\n",
    "    custom_meta_content[k] = v\n",
    "\n",
    "custom_meta_content['abstract'] = \"\"\"Abstract text\"\"\"\n",
    "custom_meta_content['name'] = 'osc_corp_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7a3fe-4149-45e4-88b9-9bd3a7bd009d",
   "metadata": {},
   "source": [
    "Create the metadata for all the fields in all the tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd5046d-5237-42b9-9d40-508bfd177bcf",
   "metadata": {},
   "source": [
    "field_text = \"\"\"`country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [https://www.iso.org/iso-3166-country-codes.html]\n",
    "`country_long` (text): longer form of the country designation\n",
    "`name` (text): name or title of the power plant, generally in Romanized form\n",
    "`gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "`capacity_mw` (number): electrical generating capacity in megawatts\n",
    "`latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "`primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "`other_fuel1` (text): energy source used in electricity generation or export\n",
    "`other_fuel2` (text): energy source used in electricity generation or export\n",
    "`other_fuel3` (text): energy source used in electricity generation or export\n",
    "`commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "`owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "`source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "`url` (text): web document corresponding to the `source` field\n",
    "`geolocation_source` (text): attribution for geolocation information\n",
    "`wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "`year_of_capacity_data` (number): year the capacity information was reported\n",
    "`generation_data_source` (text): attribution for the reported generation information\"\"\"\n",
    "\n",
    "field_descs = [line.split(': ')[1] for line in field_text.split('\\n')]\n",
    "field_keys = [line.split(': ')[0].split(' ')[0][1:-1] for line in field_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566b9fd-7cfa-42da-a5f5-b0be69d0e85e",
   "metadata": {},
   "source": [
    "Create custom meta data and key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a27bcc-cb92-481a-bf99-ba9ac9090a9c",
   "metadata": {},
   "source": [
    "custom_meta_fields = {}\n",
    "for k, v in zip(field_keys, field_descs):\n",
    "    custom_meta_fields[k] = { 'description': v }\n",
    "\n",
    "custom_meta_fields['capacity_mw']['dimension'] = 'MW'\n",
    "custom_meta_fields['latitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['longitude']['dimension'] = 'degrees'\n",
    "custom_meta_fields['commissioning_year']['dimension'] = 'year'\n",
    "custom_meta_fields['year_of_capacity_data']['dimension'] = 'year'\n",
    "custom_meta_fields['year'] = { 'description': 'year of report', 'dimension': 'year'}\n",
    "custom_meta_fields['gppd_idnr'] = { 'description': 'unique index into plants table', 'dimension': None}\n",
    "custom_meta_fields['generation_gwh'] = { 'description': 'electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_gwh'] = { 'description': 'estimated electricity generation in gigawatt-hours reported for the year', 'dimension': 'GWh'}\n",
    "custom_meta_fields['estimated_generation_note'] = { 'description': 'label of the model/method used to estimate generation for the year', 'dimension': None }\n",
    "custom_meta_key_fields = 'metafields'\n",
    "\n",
    "custom_meta_content = {\n",
    "    'title': 'Global Power Plant Database',\n",
    "    'description': 'A comprehensive, global, open source database of power plants',\n",
    "    'version': '1.3.0',\n",
    "    'release_date': '20210602'\n",
    "}\n",
    "custom_meta_key = 'metaset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e1321-bfa5-49a8-975e-d88042985316",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722f0bd-eca3-4bbc-9406-26d6b09d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'shell_2020'\n",
    "custom_meta_fields = {}\n",
    "create_trino_pipeline (s3, schemaname, tablename, timestamp, shell_df, custom_meta_fields, custom_meta_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b29ea-3636-4af6-8108-49f4d3c80cf4",
   "metadata": {},
   "source": [
    "Restore data and metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26c30d50-88f0-4f41-b1f8-1dffc87c21b1",
   "metadata": {},
   "source": [
    "# Read the Parquet file into an Arrow table\n",
    "obj = s3.get_object(\n",
    "    Bucket=os.environ['S3_DEV_BUCKET'], \n",
    "    Key='trino/{sname}/{tname}/{uuid}/{timestamp}/{tname}.parquet'.format(sname=schemaname, tname=tablename, uuid=ingest_uuid, timestamp=timestamp)\n",
    ")\n",
    "restored_table = pq.read_table(io.BytesIO(obj['Body'].read()))\n",
    "# Call the table’s to_pandas conversion method to restore the dataframe\n",
    "# This operation uses the Pandas metadata to reconstruct the dataFrame under the hood\n",
    "restored_df = restored_table.to_pandas()\n",
    "# The custom metadata is accessible via the Arrow table’s metadata object\n",
    "# Use the custom metadata key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta = json.loads(restored_meta_json)\n",
    "# Use the custom metadata fields key used earlier (taking care to once again encode the key as bytes)\n",
    "restored_meta_json_fields = restored_table.schema.metadata[custom_meta_key_fields.encode()]\n",
    "# Deserialize the json string to get back metadata\n",
    "restored_meta_fields = json.loads(restored_meta_json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd177b-4035-43ee-a8bf-6b8500be0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac6e28-686b-4540-bc9f-83b44cba3d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c7bbc-c00f-41e8-85b0-6b553e73a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'shell_2020',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b86eb-aebb-445d-b125-197b7f73a83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
